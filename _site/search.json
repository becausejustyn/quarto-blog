[
  {
    "objectID": "posts/my_first_post/index.html",
    "href": "posts/my_first_post/index.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 4 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the contents of the about page for my blog."
  },
  {
    "objectID": "posts/my_first_post/computations.html",
    "href": "posts/my_first_post/computations.html",
    "title": "Quarto Computations",
    "section": "",
    "text": "Code\nimport numpy as np\na = np.arange(15).reshape(3, 5)\na\n\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])"
  },
  {
    "objectID": "posts/my_first_post/computations.html#matplotlib",
    "href": "posts/my_first_post/computations.html#matplotlib",
    "title": "Quarto Computations",
    "section": "Matplotlib",
    "text": "Matplotlib\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)"
  },
  {
    "objectID": "posts/my_first_post/computations.html#plotly",
    "href": "posts/my_first_post/computations.html#plotly",
    "title": "Quarto Computations",
    "section": "Plotly",
    "text": "Plotly\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\ngapminder = px.data.gapminder()\ngapminder2007 = gapminder.query(\"year == 2007\")\nfig = px.scatter(gapminder2007, \n                 x=\"gdpPercap\", y=\"lifeExp\", color=\"continent\", \n                 size=\"pop\", size_max=60,\n                 hover_name=\"country\")\nfig.show()"
  },
  {
    "objectID": "posts/my_first_post/computations.html#multiple-figures",
    "href": "posts/my_first_post/computations.html#multiple-figures",
    "title": "Quarto Computations",
    "section": "Multiple Figures",
    "text": "Multiple Figures\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\ngapminder = px.data.gapminder()\ndef gapminder_plot(year):\n    gapminderYear = gapminder.query(\"year == \" + \n                                    str(year))\n    fig = px.scatter(gapminderYear, \n                     x=\"gdpPercap\", y=\"lifeExp\",\n                     size=\"pop\", size_max=60,\n                     hover_name=\"country\")\n    fig.show()\n    \ngapminder_plot(1957)\ngapminder_plot(2007)\n\n\n\n\n\n\n                                                \n\n\n(a) Gapminder: 1957\n\n\n\n\n\n                                                \n\n\n(b) Gapminder: 2007\n\n\nFigure 1: Life Expectancy and GDP"
  },
  {
    "objectID": "posts/my_first_post/computations.html#equations",
    "href": "posts/my_first_post/computations.html#equations",
    "title": "Quarto Computations",
    "section": "Equations",
    "text": "Equations\nEinstein’s theory of special relatively that expresses the equivalence of mass and energy:\n\\[E = mc^{2}\\]\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important."
  },
  {
    "objectID": "posts/using_r/index.html",
    "href": "posts/using_r/index.html",
    "title": "Trying R",
    "section": "",
    "text": "As tempting as it might be to do all my data wrangling via tidyverse, I have been practising using pandas. This post was entirely written in RStudio, however, the python code will run in a notebook alternative such as Jupyter or VS Code.\n\nInstalling Packages\n\nRPython\n\n\n\n\nCode\ninstall.packages(\"ggplot2\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"purrr\")\n\n#or simply\ninstall.packages(\"tidyverse\")\n\n\n\n\n\n\nCode\n!pip3 install seaborn\n!pip3 install numpy\n!pip3 install pandas\n\n#You can also install these packages in the terminal \npip3 install seaborn\npip3 install numpy\npip3 install pandas\n\n\n\n\n\nLearning python was overwhelming because you use the terminal much more often than you need to when using R. Over time, I have began to appreciate using a virtual environment because you can easily run multiple versions of python, which makes it more practical when you are using a package that requires an older version.\n\n\nImporting Data, Loading Libraries\nI have hidden the output of the code, however, you can view it by clicking the dropdown menu. I did this primarily for myself so it was easier to scroll down, but I think it is also more practical since the output is not necessarily the focus of this post.\n\nRPython\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\ndf <- diamonds\n\ndf %>% head()\n\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nsns.set_style('white')\n\ndf = sns.load_dataset('diamonds')\n\ndf.head()\n\n\n   carat      cut color clarity  depth  table  price     x     y     z\n0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n\n\n\n\n\n\n\nExample of Functions\n\nVerbsExample\n\n\n\n\n\ndplyr\npandas\n\n\n\n\nfilter() and slice()\nquery() and loc[], iloc[]\n\n\narrange()\nsort_values and sort_index()\n\n\nselect() and rename()\n__getitem__ and rename()\n\n\nselect()\nfilter()\n\n\ndistinct()\ndrop_duplicates()\n\n\nmutate()\nassign\n\n\nsummarise()\nagg\n\n\ngroup_by()\ngroupby()\n\n\nsample_n() and sample_frac()\nsample\n\n\n%>%\npipe[^1]\n\n\n\n\n\n\n\n\n\n\n\n\ndplyr\npandas\n\n\n\n\nfilter(df, col == 'val')\ndf.query('col == \"val\"')\n\n\narrange(df, col)\ndf.sort_values('val')\n\n\nrename(df, new_name = old_name)\ndf.rename(columns = {old_name = new_name})\n\n\nselect(df, col)\ndf.loc['val']\n\n\ndistinct(df, col, .keep_all = TRUE)\ndf[['val']].drop_duplicates()\n\n\nmutate(new_var = col - col2)\ndf.assign(new_var = df.col - df.col2)\n\n\nsummarise(mean = mean(col2), n = count(col1))\ndf.agg({\"col1\": \"count\", \"col2\", \"mean\"})\n\n\ngroup_by(df, col)\ndf.groupby('col')\n\n\n%>%\npipe[^1]\n\n\n\n\n\n\nOne of the confusing things are first is that there are many similar functions under different names. I personally find it easier to remember them by the way I write my code. For example, by only using <- as an assignment operator in R, I find it easier to treat the two languages differently.\n\n\nSelecting Columns\n\nRPython\n\n\n\n\nCode\n#select(df, color, cut)\n\ndf %>%\n  select(color, cut)\n\n\n# A tibble: 53,940 × 2\n   color cut      \n   <ord> <ord>    \n 1 E     Ideal    \n 2 E     Premium  \n 3 E     Good     \n 4 I     Premium  \n 5 J     Good     \n 6 J     Very Good\n 7 I     Very Good\n 8 H     Very Good\n 9 E     Fair     \n10 H     Very Good\n# … with 53,930 more rows\n\n\n\n\n\n\nCode\ndf.filter(['color', 'cut'])\n\n#or\n#df[['color', 'cut']]\n\n\n      color        cut\n0         E      Ideal\n1         E    Premium\n2         E       Good\n3         I    Premium\n4         J       Good\n...     ...        ...\n53935     D      Ideal\n53936     D       Good\n53937     D  Very Good\n53938     H    Premium\n53939     D      Ideal\n\n[53940 rows x 2 columns]\n\n\n\n\n\n\n\nIf we want to select a range of columns\n\nRPython\n\n\n\n\nCode\nselect(df, x:z)\n\n\n# A tibble: 53,940 × 3\n       x     y     z\n   <dbl> <dbl> <dbl>\n 1  3.95  3.98  2.43\n 2  3.89  3.84  2.31\n 3  4.05  4.07  2.31\n 4  4.2   4.23  2.63\n 5  4.34  4.35  2.75\n 6  3.94  3.96  2.48\n 7  3.95  3.98  2.47\n 8  4.07  4.11  2.53\n 9  3.87  3.78  2.49\n10  4     4.05  2.39\n# … with 53,930 more rows\n\n\n\n\n\n\nCode\ndf.loc[:, 'x':'z']\n\n\n          x     y     z\n0      3.95  3.98  2.43\n1      3.89  3.84  2.31\n2      4.05  4.07  2.31\n3      4.20  4.23  2.63\n4      4.34  4.35  2.75\n...     ...   ...   ...\n53935  5.75  5.76  3.50\n53936  5.69  5.75  3.61\n53937  5.66  5.68  3.56\n53938  6.15  6.12  3.74\n53939  5.83  5.87  3.64\n\n[53940 rows x 3 columns]\n\n\n\n\n\n\n\nIf we want to pipe it\n\nRPython\n\n\n\n\nCode\ndf %>% \n  select(color, cut)\n\n\n# A tibble: 53,940 × 2\n   color cut      \n   <ord> <ord>    \n 1 E     Ideal    \n 2 E     Premium  \n 3 E     Good     \n 4 I     Premium  \n 5 J     Good     \n 6 J     Very Good\n 7 I     Very Good\n 8 H     Very Good\n 9 E     Fair     \n10 H     Very Good\n# … with 53,930 more rows\n\n\n\n\n\n\nCode\n(df\n.filter(['color', 'cut'])\n)\n\n\n      color        cut\n0         E      Ideal\n1         E    Premium\n2         E       Good\n3         I    Premium\n4         J       Good\n...     ...        ...\n53935     D      Ideal\n53936     D       Good\n53937     D  Very Good\n53938     H    Premium\n53939     D      Ideal\n\n[53940 rows x 2 columns]\n\n\n\n\n\n\n\nIf we want to drop a certain column\n\nRPython\n\n\n\n\nCode\nselect(df, -(x:z))\n\n\n# A tibble: 53,940 × 7\n   carat cut       color clarity depth table price\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int>\n 1  0.23 Ideal     E     SI2      61.5    55   326\n 2  0.21 Premium   E     SI1      59.8    61   326\n 3  0.23 Good      E     VS1      56.9    65   327\n 4  0.29 Premium   I     VS2      62.4    58   334\n 5  0.31 Good      J     SI2      63.3    58   335\n 6  0.24 Very Good J     VVS2     62.8    57   336\n 7  0.24 Very Good I     VVS1     62.3    57   336\n 8  0.26 Very Good H     SI1      61.9    55   337\n 9  0.22 Fair      E     VS2      65.1    61   337\n10  0.23 Very Good H     VS1      59.4    61   338\n# … with 53,930 more rows\n\n\n\n\n\n\nCode\n(df\n.drop(['x', 'y', 'z'], axis = 1)\n)\n\n\n       carat        cut color clarity  depth  table  price\n0       0.23      Ideal     E     SI2   61.5   55.0    326\n1       0.21    Premium     E     SI1   59.8   61.0    326\n2       0.23       Good     E     VS1   56.9   65.0    327\n3       0.29    Premium     I     VS2   62.4   58.0    334\n4       0.31       Good     J     SI2   63.3   58.0    335\n...      ...        ...   ...     ...    ...    ...    ...\n53935   0.72      Ideal     D     SI1   60.8   57.0   2757\n53936   0.72       Good     D     SI1   63.1   55.0   2757\n53937   0.70  Very Good     D     SI1   62.8   60.0   2757\n53938   0.86    Premium     H     SI2   61.0   58.0   2757\n53939   0.75      Ideal     D     SI2   62.2   55.0   2757\n\n[53940 rows x 7 columns]\n\n\n\n\n\n\n\nfiltering on one condition\n\nRPython\n\n\n\n\nCode\nfilter(df, color == 'E')\n\n\n# A tibble: 9,797 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n 5  0.2  Premium   E     SI2      60.2    62   345  3.79  3.75  2.27\n 6  0.32 Premium   E     I1       60.9    58   345  4.38  4.42  2.68\n 7  0.23 Very Good E     VS2      63.8    55   352  3.85  3.92  2.48\n 8  0.23 Very Good E     VS1      60.7    59   402  3.97  4.01  2.42\n 9  0.23 Very Good E     VS1      59.5    58   402  4.01  4.06  2.4 \n10  0.23 Good      E     VS1      64.1    59   402  3.83  3.85  2.46\n# … with 9,787 more rows\n\n\n\n\n\n\nCode\n(df\n.query(\"color == 'E'\")\n)\n\n\n       carat        cut color clarity  depth  table  price     x     y     z\n0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n8       0.22       Fair     E     VS2   65.1   61.0    337  3.87  3.78  2.49\n14      0.20    Premium     E     SI2   60.2   62.0    345  3.79  3.75  2.27\n...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n53926   0.71      Ideal     E     SI1   61.9   56.0   2756  5.71  5.73  3.54\n53928   0.79    Premium     E     SI2   61.4   58.0   2756  6.03  5.96  3.68\n53930   0.71    Premium     E     SI1   60.5   55.0   2756  5.79  5.74  3.49\n53932   0.70  Very Good     E     VS2   60.5   59.0   2757  5.71  5.76  3.47\n53933   0.70  Very Good     E     VS2   61.2   59.0   2757  5.69  5.72  3.49\n\n[9797 rows x 10 columns]\n\n\n\n\n\n\n\nIf we want multiple conditions\n\nRPython\n\n\n\n\nCode\nfilter(df, color == 'E', cut == 'Good')\n\n\n# A tibble: 933 × 10\n   carat cut   color clarity depth table price     x     y     z\n   <dbl> <ord> <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Good  E     VS1      56.9    65   327  4.05  4.07  2.31\n 2  0.23 Good  E     VS1      64.1    59   402  3.83  3.85  2.46\n 3  0.26 Good  E     VVS1     57.9    60   554  4.22  4.25  2.45\n 4  0.7  Good  E     VS2      57.5    58  2759  5.85  5.9   3.38\n 5  0.71 Good  E     VS2      59.2    61  2772  5.8   5.88  3.46\n 6  0.7  Good  E     VS2      64.1    59  2777  5.64  5.59  3.6 \n 7  0.7  Good  E     VS1      57.2    62  2782  5.81  5.77  3.31\n 8  0.76 Good  E     SI1      63.7    54  2789  5.76  5.85  3.7 \n 9  0.7  Good  E     VS2      64.1    55  2793  5.6   5.66  3.61\n10  0.73 Good  E     SI1      63.2    58  2796  5.7   5.76  3.62\n# … with 923 more rows\n\n\nCode\n#or\n#filter(df, color == 'E' & cut == 'Good')\n\n\n\n\n\n\nCode\n(df\n.query('color == \"E\" & cut == \"Good\"')\n)\n\n\n       carat   cut color clarity  depth  table  price     x     y     z\n2       0.23  Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n36      0.23  Good     E     VS1   64.1   59.0    402  3.83  3.85  2.46\n84      0.26  Good     E    VVS1   57.9   60.0    554  4.22  4.25  2.45\n95      0.70  Good     E     VS2   57.5   58.0   2759  5.85  5.90  3.38\n169     0.71  Good     E     VS2   59.2   61.0   2772  5.80  5.88  3.46\n...      ...   ...   ...     ...    ...    ...    ...   ...   ...   ...\n53695   0.75  Good     E     VS2   59.7   65.0   2717  5.85  5.80  3.48\n53739   0.73  Good     E     VS2   63.3   60.0   2723  5.67  5.73  3.61\n53741   0.78  Good     E     SI1   57.9   62.0   2723  6.06  6.03  3.50\n53785   0.89  Good     E     SI2   64.3   65.0   2728  6.00  5.95  3.84\n53890   0.73  Good     E     SI1   57.9   55.0   2749  6.00  5.96  3.46\n\n[933 rows x 10 columns]\n\n\n\n\n\n\n\nIf we want multiple conditions in one column\n\nRPython\n\n\n\n\nCode\ndf %>% \n    filter(color %in% c('E', 'J'))\n\n\n# A tibble: 12,605 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 5  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 6  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n 7  0.3  Good      J     SI1      64      55   339  4.25  4.28  2.73\n 8  0.23 Ideal     J     VS1      62.8    56   340  3.93  3.9   2.46\n 9  0.31 Ideal     J     SI2      62.2    54   344  4.35  4.37  2.71\n10  0.2  Premium   E     SI2      60.2    62   345  3.79  3.75  2.27\n# … with 12,595 more rows\n\n\n\n\n\n\nCode\n(df\n.query('color in [\"E\", \"J\"]')\n)\n\n\n       carat        cut color clarity  depth  table  price     x     y     z\n0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n4       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n5       0.24  Very Good     J    VVS2   62.8   57.0    336  3.94  3.96  2.48\n...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n53926   0.71      Ideal     E     SI1   61.9   56.0   2756  5.71  5.73  3.54\n53928   0.79    Premium     E     SI2   61.4   58.0   2756  6.03  5.96  3.68\n53930   0.71    Premium     E     SI1   60.5   55.0   2756  5.79  5.74  3.49\n53932   0.70  Very Good     E     VS2   60.5   59.0   2757  5.71  5.76  3.47\n53933   0.70  Very Good     E     VS2   61.2   59.0   2757  5.69  5.72  3.49\n\n[12605 rows x 10 columns]\n\n\n\n\n\n\n\nCount Missing Values\n\nRPython\n\n\n\n\nCode\n# sum of missing values in each column\ndf %>% \n  summarise(across(everything(), ~sum(is.na(.))))\n\n\n# A tibble: 1 × 10\n  carat   cut color clarity depth table price     x     y     z\n  <int> <int> <int>   <int> <int> <int> <int> <int> <int> <int>\n1     0     0     0       0     0     0     0     0     0     0\n\n\nCode\n#purrr::map_df(df, ~sum(is.na(.)))\n\n\n\n\n\n\nCode\ndf.isna().sum()\n\n\ncarat      0\ncut        0\ncolor      0\nclarity    0\ndepth      0\ntable      0\nprice      0\nx          0\ny          0\nz          0\ndtype: int64\n\n\n\n\n\n\n\nCount Unique Values in Each Column\n\nRPython\n\n\n\n\nCode\n# getting the count of unique values in each column \ndf %>% \n  summarise(across(everything(), n_distinct))\n\n#can also map across for the same result\npurrr::map_df(df, ~sum(n_distinct(.)))\n\n# if you just want numerical columns\ndf %>% \n  summarise(across(where(is.numeric), n_distinct))\n\n\n\n\n\n\nCode\ndf.nunique()\n\n# If you want unique values in numeric columns\ndf.select_dtypes(include = np.number).nunique()\n#or\ndf.select_dtypes('number').nunique()\n\n# If you just want the column names of numeric type\ndf.select_dtypes('number').columns\n# If you want them as a list\ndf.select_dtypes(include = np.number).columns.tolist()\n\n# count and unique values\ndf.agg(['count', 'size', 'nunique'])\n\n#for the proportions\ndf.select_dtypes(include = np.number).value_counts(normalize = True)\n#or\ndf.select_dtypes('number').value_counts(normalize = True)\n\n\n\n\n\n\n\nComplex Pipings\n\nRPython\n\n\n\n\nCode\ndf %>%\n  select(starts_with('c')) %>%\n  filter(cut %in% c('Ideal', 'Premium')) %>%\n  group_by(cut, color, clarity) %>%\n  summarise(\n    avgcarat = mean(carat, na.rm=TRUE),\n    n = n()\n    ) %>%\n  arrange(-avgcarat) %>% #desc(avgcarat) also works\n  head()\n\n\n`summarise()` has grouped output by 'cut', 'color'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 5\n# Groups:   cut, color [4]\n  cut     color clarity avgcarat     n\n  <ord>   <ord> <ord>      <dbl> <int>\n1 Ideal   J     I1          1.99     2\n2 Premium I     I1          1.61    24\n3 Premium J     I1          1.58    13\n4 Premium J     SI2         1.55   161\n5 Ideal   H     I1          1.48    38\n6 Premium I     SI2         1.42   312\n\n\n\n\n\n\nCode\n(df\n .filter(regex = '^c')\n .query('cut in [\"Ideal\", \"Premium\"]')\n .groupby(['cut', 'color', 'clarity'])\n .agg(['mean', 'size'])\n .sort_values(by = ('carat', 'mean'), ascending = False)\n .head())\n\n\n                          carat     \n                           mean size\ncut     color clarity               \nIdeal   J     I1       1.990000    2\nPremium I     I1       1.605833   24\n        J     I1       1.578462   13\n              SI2      1.554534  161\nIdeal   H     I1       1.475526   38\n\n\n\n\n\n\n\nMore Examples\n\nTransforming\n\n\n\n\n\n\n\nR\npandas\n\n\n\n\nselect(df, col_one = col1)\ndf.rename(columns = {'col1': 'col_one'})['col_one']\n\n\nrename(df, col_one = col1)[^2]\ndf.rename(columns = {'col1': 'col_one'})\n\n\nmutate(df, c = a - b)\ndf.assign(c = df['a'] - df['b'])\n\n\n\n\n\nSorting\n\n\n\n\n\n\n\nR\npandas\n\n\n\n\narrange(df, col1, col2)\ndf.sort_values(['col1', 'col2'])\n\n\narrange(df, desc(col1))[^3]\ndf.sort_values('col1', ascending = False)\n\n\n\n\n\nGrouping and Summarising\n\n\n\n\n\n\n\nR\npandas\n\n\n\n\nsummary(df)\ndf.describe()\n\n\ngroup_by(df, col1)\ndf.groupby('col1')\n\n\ngroup_by(df, col1) %>% summarise(avg = mean(col1, na.rm = TRUE))\ndf.groupby('col1').agg({'col1' : 'mean'})\n\n\ngroup_by(df, col1) %>% summarise(total = sum(col1))\ndf.groupby('col1').sum()"
  },
  {
    "objectID": "posts/nifty_features/index.html",
    "href": "posts/nifty_features/index.html",
    "title": "Untitled",
    "section": "",
    "text": "Note\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution.\n\n\n\n\n\n\n\n\nTip With Caption\n\n\n\nThis is an example of a callout with a caption.\n\n\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default.\n\n\n\n\n\n\n\n\n\n\n\nPay Attention\n\n\n\nUsing callouts is an effective way to highlight content that your reader give special consideration or attention. You can set this globally callout-appearance: simple.\n\n\n\n\n\n\n\n\n\n\n\nPay Attention\n\n\n\nUsing callouts is an effective way to highlight content that your reader give special consideration or attention callout-icon: false;.\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nmtcars2 <- mtcars\nmtcars2$am <- factor(\n  mtcars$am, labels = c('automatic', 'manual')\n)\nggplot(mtcars2, aes(hp, mpg, color = am)) +\n  geom_point() +  geom_smooth(formula = y ~ x, method = \"loess\") +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\nFigure 1: MPG vs horsepower, colored by transmission.\n\n\n\n\n\n\n\nCode\nknitr::kable(\n  mtcars[1:3, 1:3]\n)\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\n\n\n\n\nMazda RX4\n21.0\n6\n160\n\n\nMazda RX4 Wag\n21.0\n6\n160\n\n\nDatsun 710\n22.8\n4\n108\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know from the first fundamental theorem of calculus that for \\(x\\) in \\([a, b]\\):\n\\[\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).\\]"
  },
  {
    "objectID": "posts/barplots/index.html",
    "href": "posts/barplots/index.html",
    "title": "Untitled",
    "section": "",
    "text": "I have also been doing a lot of NFL data viz, which I found a lot harder than I thought it would be. I’ll share them at a later date, but I thought I would do a quick post at 11pm at some tricks I learnt with using text labels. I thought I would use the penguins dataset from palmerpenguins since it does not appear to be as common as others such as mtcars.\n\n\nCode\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(gt)\nlibrary(becausejustynfun) #personal package with plotting theme\n\npenguins <- palmerpenguins::penguins\n\n\nEven though I am using the cleaned version, I find it good practice to use glimpse() just to be safe.\n\n\nCode\npenguins %>% glimpse()\n\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nNext, I am checking for missing values.\n\n\nCode\n#penguins %>%\n#  summarise(across(everything(), ~sum(is.na(.))))\n\npenguins %>% map_dbl(~sum(is.na(.)))\n\n\n          species            island    bill_length_mm     bill_depth_mm \n                0                 0                 2                 2 \nflipper_length_mm       body_mass_g               sex              year \n                2                 2                11                 0 \n\n\nTo see how I will deal with them I like to see the unique values across each column.\n\n\nCode\n#penguins %>%\n#  summarise(across(everything(), n_distinct))\n\npenguins %>% \n  map_dbl(~n_distinct(.))\n\n\n          species            island    bill_length_mm     bill_depth_mm \n                3                 3               165                81 \nflipper_length_mm       body_mass_g               sex              year \n               56                95                 3                 3 \n\n\nI’ll have a glance at the missing values to see if they are important.\n\n\nCode\npenguins %>% \n  filter(if_any(everything(), is.na)) \n\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           NA            NA                  NA          NA\n 2 Adelie  Torgersen           34.1          18.1               193        3475\n 3 Adelie  Torgersen           42            20.2               190        4250\n 4 Adelie  Torgersen           37.8          17.1               186        3300\n 5 Adelie  Torgersen           37.8          17.3               180        3700\n 6 Adelie  Dream               37.5          18.9               179        2975\n 7 Gentoo  Biscoe              44.5          14.3               216        4100\n 8 Gentoo  Biscoe              46.2          14.4               214        4650\n 9 Gentoo  Biscoe              47.3          13.8               216        4725\n10 Gentoo  Biscoe              44.5          15.7               217        4875\n11 Gentoo  Biscoe              NA            NA                  NA          NA\n# … with 2 more variables: sex <fct>, year <int>\n\n\nGiven the NA values were low, I will just drop them.\n\n\nCode\npenguins <- penguins %>%\n  na.exclude()\n\n#to check\nmap_dfc(penguins, sum(is.na(penguins)))\n\n\n# A tibble: 0 × 0\n\n\nCode\n#map_df(penguins, .f = sum(is.na(penguins)))\n\n\nI am going to create a seperate df with the count values. You can easily just use add_count(), or just rely on ggplot2 to do that for you, but later on we are going to make more adjustments to the df that will make it easier if there is a mistake.\n\n\nCode\n#df of counts\npenguins_sum <- penguins %>%\n  count(species, sort = TRUE) %>% \n  mutate(\n    species = fct_rev(fct_inorder(species)),\n    perc = paste0(sprintf(\"%4.1f\", n / sum(n) * 100), \"%\") #percentage label\n    )\n\n#scales alternative\n#mutate(perc = scales::percent(n / sum(n), accuracy = .1, trim = FALSE))\n\n\n\n\nCode\npenguins_sum %>%\nggplot(aes(x = n, y = species)) +\n  geom_col(fill = \"gray70\") +\n  ## add percentage labels\n  geom_text(aes(label = perc)) +\n  white_theme()\n\n\n\n\n\n\n\nCode\n## prepare non-aggregated data set with lumped and ordered factors\npenguins_fct <- penguins %>% \n  dplyr::mutate(\n    total = dplyr::n(),\n    species = stringr::str_to_title(species),\n    species = forcats::fct_rev(forcats::fct_infreq(species))\n  )\n\npenguins_sum <- penguins_sum %>% \n  mutate(\n    colour = case_when(\n      row_number() == 1 ~ \"#468499\",\n      row_number() == 2 ~ \"#E697AC\",\n      row_number() == 3 ~ \"#81CDE6\",\n      ## all others should be gray\n      TRUE ~ \"gray70\"\n    )\n  )\n\n\nThis is one approach were we conditionally colour each variable of choice.\n\n\nCode\npenguins_sum %>%\n  ggplot(aes(\n    x = n, \n    y = species, \n    fill = colour)) +\n  geom_col() +\n  geom_text(\n    aes(label = perc),\n    hjust = 1, nudge_x = -.5\n  ) +\n  ## add custom colors\n  scale_fill_identity(guide = \"none\") +\n  white_theme()\n\n\n\n\n\nWe also have a lot of control over the font used. There can be challenges when installing a font onto your system for the first time, but I might go through that another time since I struggled with it for quite a while. Maybe I am just a silly billy.\n\n\nCode\npenguins_sum %>%\nggplot(aes(\n  x = n, \n  y = species, \n  fill = colour)) +\n  geom_col() +\n  geom_text(\n    aes(label = perc), \n    hjust = 1, nudge_x = -.5,\n    size = 4, fontface = \"bold\", family = \"Fira Sans\"\n  ) +\n  ## reduce spacing between labels and bars\n  scale_x_continuous(expand = c(.01, .01)) +\n  scale_fill_identity(guide = \"none\") +\n  white_theme() \n\n\n\n\n\nSometimes the colour of the font does not match well with the plot. This can be challenging when you have more than a few colours, so you might not want to manually adjust every single one. One option is to add white to the label with fill = \"white\".\n\n\nCode\npenguins_sum %>%\n  ggplot(aes(\n    x = n, \n    y = species, \n    fill = colour)) +\n  geom_col() +\n  geom_label(\n    aes(label = perc), \n    hjust = 0.95, nudge_x = -.5,\n    size = 4, fontface = \"bold\",\n    ## turn into white box without outline\n    fill = \"white\", label.size = 0\n  ) +\n  scale_x_continuous(expand = c(.01, .01)) +\n  scale_fill_identity(guide = \"none\") +\n  white_theme() +\n  theme(\n    axis.text.y = element_text(size = 14, hjust = 1, family = \"Fira Sans\"),\n    plot.margin = margin(rep(15, 4))\n  )\n\n\n\n\n\nLikewise, it is possible to adjust the position of the text labels conditionally. I think ggplot2 biggest challenge is learning what you can do.\n\n\nCode\npenguins_sum %>% \n  mutate(\n    ## set justification based on data \n    ## so that only the first label is placed inside\n    place = if_else(row_number() == 1, 1, 0),\n    ## add some spacing to labels since we cant use nudge_x anymore\n    perc = paste(\" \", perc, \" \")\n  ) %>% \n  ggplot(aes(\n    x = n, \n    y = species, \n    fill = colour)) +\n  geom_col() +\n  geom_text(\n    aes(label = perc, hjust = place), \n    fontface = \"bold\"\n  ) +\n  scale_x_continuous(expand = c(.01, .01)) +\n  scale_fill_identity(guide = \"none\") +\n  white_theme() +\n  theme(\n    plot.margin = margin(rep(15, 4))\n  )\n\n\n\n\n\nYou can use different positions of text labels to highlight things of interest, or sometimes adjusting it makes it easier to work around other features of the plot such as the background or legend.\nThat is all for today. I’ll try to post next week about more data viz.\n\n\nCode\nsessionInfo()\n\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] becausejustynfun_0.0.0.9000 gt_0.3.1                   \n [3] janitor_2.1.0               forcats_0.5.1              \n [5] stringr_1.4.0               dplyr_1.0.9                \n [7] purrr_0.3.4                 readr_2.1.2                \n [9] tidyr_1.2.0                 tibble_3.1.7               \n[11] ggplot2_3.3.6               tidyverse_1.3.1            \n[13] palmerpenguins_0.1.0       \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3           lubridate_1.8.0        assertthat_0.2.1      \n [4] digest_0.6.29          utf8_1.2.2             R6_2.5.1              \n [7] cellranger_1.1.0       backports_1.4.1        reprex_2.0.1          \n[10] evaluate_0.15          httr_1.4.3             pillar_1.7.0          \n[13] rlang_1.0.2            readxl_1.3.1           rstudioapi_0.13.0-9000\n[16] rmarkdown_2.14         labeling_0.4.2         htmlwidgets_1.5.4     \n[19] munsell_0.5.0          broom_0.8.0            compiler_4.1.2        \n[22] modelr_0.1.8           xfun_0.31.2            pkgconfig_2.0.3       \n[25] htmltools_0.5.2        tidyselect_1.1.2       fansi_1.0.3           \n[28] crayon_1.5.1           tzdb_0.2.0             dbplyr_2.1.1          \n[31] withr_2.5.0            grid_4.1.2             jsonlite_1.8.0        \n[34] gtable_0.3.0           lifecycle_1.0.1        DBI_1.1.2             \n[37] magrittr_2.0.3         scales_1.2.0           cli_3.3.0             \n[40] stringi_1.7.6          farver_2.1.0           renv_0.15.5           \n[43] fs_1.5.2               ggthemes_4.2.4         snakecase_0.11.0      \n[46] xml2_1.3.3             ellipsis_0.3.2         generics_0.1.2        \n[49] vctrs_0.4.1            tools_4.1.2            glue_1.6.2            \n[52] hms_1.1.1              fastmap_1.1.0          yaml_2.3.5            \n[55] colorspace_2.0-3       rvest_1.0.2            knitr_1.39            \n[58] haven_2.4.3"
  },
  {
    "objectID": "posts/nfl_hfa/index.html",
    "href": "posts/nfl_hfa/index.html",
    "title": "Untitled",
    "section": "",
    "text": "Code\n# if you do not have the data on your local device you can either \n# swap the path with \"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\"\n# or get it via nflfastR::load_pbp(1999:2021)\n# note that it will take a while since there is over 1 million rows by 366-ish columns \n# that is why I filter and drop rows straight away\n\npbp <- purrr::map_df(c(1999:2021), function(x) {\n  readRDS(\n    glue::glue(\"~/Documents/nfl/data/pbp/play_by_play_{x}.rds\")\n  )}) %>%\n  filter(\n    season_type == 'REG',\n    !is.na(posteam_type)\n    ) %>%\n  select(game_id, season, week, posteam_type, season_type, home_team, away_team, home_score, away_score) %>%\n  group_by(game_id) %>%\n  # this will give two rows for each game, one for the home team, one for the away team\n  #slice(n()) or distinct(game_id, posteam_type, .keep_all = TRUE) if you only want one observation per row\n  distinct(.keep_all = TRUE)\n\n\n\n\nCode\n# team\nleague_wide_win_team <- pbp %>%\n  distinct(game_id, .keep_all = TRUE) %>%\n  mutate(score_diff = home_score - away_score) %>%\n  group_by(season, home_team) %>%\n  summarise(\n    n = n(),\n    home_win = mean(home_score - away_score > 0),\n    home_points_diff = sum(home_score - away_score)\n  ) %>% ungroup() %>%\n  mutate(\n    mean_point_diff = home_points_diff / n,\n    # so you can plot is sequentially\n    week_n = row_number()\n  ) \n\n\n\n\nCode\n#add team colours for plotting\nleague_wide_win_team <- league_wide_win_team %>%\n  left_join(\n    select(nflfastR::teams_colors_logos, home_team = team_abbr, team_name, team_color, team_color2)\n  )\n\n\n\n\nCode\nleague_wide_win_team %>%\n  mutate(\n    #LV colour is grey so I am using their secondary colour here\n    team_color = ifelse(team_color == \"#a5acaf\", \"#000000\", team_color),\n    season_label = paste0(\"'\", substr(season, 3, 4)),\n    season_label = as_factor(season_label)\n    ) %>%\n  ggplot(aes(\n    # the label is a character, so get 99 at the start\n    x = fct_reorder(season_label, season),\n    y = mean_point_diff,\n    colour = team_color,\n    group = home_team,\n  )) +\n  geom_line(size = 1.15) +\n  scale_x_discrete(breaks = c(\"'99\", \"'01\", \"'03\", \"'05\", \"'07\", \"'09\", \"'11\", \"'13\", \"'15\", \"'17\", \"'19\", \"'21\")) +\n  gghighlight(use_direct_label = FALSE, keep_scales = TRUE) +\n  scale_color_identity(aesthetics = c(\"color\", \"fill\")) +\n  facet_wrap(~home_team, scales = \"free\", nrow = 4L) +\n  labs(\n    x = \"Season\",\n    y = \"HFA\",\n    title = \"HFA By Team\",\n    subtitle = \"Regular Season, 1999:2021\",\n    caption = \"data: nflfastR\"\n  ) +\n  becausejustynfun::white_theme() +\n  theme(axis.text = element_text(size = 8))\n\n\n\n\nCode\nleague_wide_win_team %>%\n  ggplot(aes(\n    x = factor(season), \n    y = mean_point_diff, \n    group = 1)) +\n  geom_line(aes(group = factor(home_team)), alpha = 0.5) + \n  geom_line(stat = 'summary', fun = 'mean', colour = 'blue', size = 1) +\n  scale_y_continuous(breaks = seq(-22, 24, 4)) +\n  scale_x_discrete(labels = paste0(\"'\", substr(c(1999:2021), 3, 4))) +\n  labs(\n    x = \"Season\",\n    y = \"HFA\",\n    title = \"Home Field Advantage\",\n    subtitle = \"Regular Season, 1999:2021\",\n    caption = \"data: nflfastR \\nBlue line is the mean HFA by season.\"\n  ) +\n  theme(axis.text.x = element_text(angle = 15, hjust = 1)) +\n  hrbrthemes::theme_ft_rc()\n\n\n\n\nCode\nmod <- lm(mean_point_diff ~ factor(season), data = league_wide_win_team)\nsummary(mod)\n\n\n\n\nCode\nleague_wide_win_team %>%\n  ggplot(aes(\n    x = factor(season), \n    y = mean_point_diff, group = 1)) +\n  geom_line(aes(group = factor(home_team)), alpha = 1/2) + \n  geom_line(stat = 'summary', fun = 'mean', colour = 'blue', size = 1) +\n  labs(\n    x = \"Season\",\n    y = \"HFA\",\n    subtitle = \"Regular Season, 1999:2021\",\n    caption = \"data: nflfastR \\n Pink line is the linear trend over time\"\n  ) +\n  stat_smooth(method = 'lm', col = '#984ea3', se = FALSE, size = 1) +\n  scale_x_discrete(labels = paste0(\"'\", substr(c(1999:2021), 3, 4))) +\n  theme(axis.text.x = element_text(angle = 15, hjust = 1)) +\n  hrbrthemes::theme_ft_rc()\n\n\n\n\nCode\n# Removing this linear trend and focus on residuals will examine teams' performance after controlling for this decrease in hfa \n\nleague_wide_win_team <- league_wide_win_team %>%\n  mutate(rel_hfa = resid(mod))\n\n# Average the residuals for each team then, perform a one-sample t-test to see teams that were above\n# or below the average HFA despite the decrease HFA over this time \n\nbest_hfa <- league_wide_win_team %>% \n  group_by(home_team) %>%\n  summarise(\n    mean = mean(rel_hfa),\n    sd = sd(rel_hfa),\n    sem = sd(rel_hfa)/sqrt(n()),\n    tpval = t.test(rel_hfa)[['p.value']]\n    #tpval = t.test(rel_hfa)$p.value\n  ) %>%\n  mutate(sig = tpval < .05)\n\nprint(best_hfa)\n\n\n\n\nCode\n# Plot the average residuals, their SEM, colouring if they are significantly different form 0 \n\n# y.axis bold conditional \nbest_hfa_sort <- arrange(best_hfa, mean)                       \naxisFace <- if_else(best_hfa_sort[['sig']] == TRUE, 'bold', 'plain')\n\n# shading conditional \nabove <- best_hfa %>% \n  filter(mean > 0, sig == TRUE) %>% \n  arrange(mean)\n\naverage <- best_hfa %>% \n  filter(sig == FALSE)\n\nbelow <- best_hfa %>% \n  filter(mean < 0, sig == TRUE) %>% \n  arrange(mean)\n\n\n\n\nCode\nbest_hfa %>%\n  ggplot(aes(\n    x = mean,\n    y = fct_reorder(home_team, mean),\n    colour = sig\n  )) +\n  geom_errorbarh(aes(\n    xmin = mean - sem,\n    xmax = mean + sem\n  )) +\n  geom_point(size = 2) +\n  scale_color_manual(\n    values = c(\"grey\", \"black\"),\n    guide = guide_legend(\n      reverse = TRUE,\n      title = \"Above/Below Average\",\n      title.position = \"top\"\n    )\n  ) +\n  scale_x_continuous(breaks = seq(-7.5, 9, 1.5)) +\n  labs(\n    x = \"Mean HFA (Residuals)\",\n    y = \"NFL Team\",\n    title = \"NFL Home Field Advantage 1999-2021\",\n    caption = \"Error bars are SEM. \\n TRUE is statistical signifance.\"\n  ) +\n  hrbrthemes::theme_ft_rc() +\n  theme(\n    axis.text.y = element_text(face = axisFace),\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"bottom\",\n    legend.key = element_rect(colour = \"transparent\", fill = \"transparent\"),\n    legend.key.width = unit(1, \"in\"),\n    legend.key.height = unit(.5, \"in\"),\n    legend.title.align = .5,\n    axis.title.x = element_text(vjust = -1)\n  ) +\n  annotate(\"rect\",\n           xmin = -Inf,\n           xmax = Inf,\n           ymax = above[['home_team']][nrow(above)],\n           ymin = above[['home_team']][1],\n           fill = \"#486e48\",\n           colour = NA,\n           alpha = 1 / 3\n  ) +\n  annotate(\"rect\",\n           xmin = -Inf,\n           xmax = Inf,\n           ymax = below[['home_team']][nrow(below)],\n           ymin = below[['home_team']][1],\n           fill = \"#346991\",\n           colour = NA,\n           alpha = 1 / 3\n  ) +\n  annotate(\"text\", x = -.1, y = above[['home_team']][3], label = \"Best Teams\") +\n  annotate(\"text\", x = .1, y = below[['home_team']][3], label = \"Worst Teams\")\n\n\n\n\nCode\n# league wide\nleague_win <- pbp %>%\n  distinct(game_id, .keep_all = TRUE) %>%\n  mutate(score_diff = home_score - away_score) %>%\n  group_by(season) %>%\n  summarise(\n    n = n(),\n    home_win = mean(home_score - away_score > 0),\n    home_points_diff = sum(home_score - away_score)\n    ) %>% \n  mutate(\n    mean_point_diff = home_points_diff / n\n    )\n\n#season, week wide\nweekly_team_win <- pbp %>%\n  distinct(game_id, .keep_all = TRUE) %>%\n  mutate(score_diff = home_score - away_score) %>%\n  group_by(season, week, home_team) %>%\n  summarise(\n    away_team,\n    home_win = mean(home_score - away_score > 0),\n    home_result = sum(home_score - away_score),\n    .groups = \"drop\"\n  ) \n\nweekly_team_win1 <- pbp %>%\n  distinct(game_id, .keep_all = TRUE) %>%\n  mutate(score_diff = home_score - away_score) %>%\n  group_by(season, week) %>%\n  summarise(\n    team = home_team,\n    n = n(),\n    home_win = mean(home_score - away_score > 0),\n    home_points_diff = sum(home_score - away_score),\n    .groups = \"drop\"\n  ) %>% \n  mutate(\n    mean_point_diff = home_points_diff / n,\n    week_n = row_number()\n  ) \n\n\ndata{\n  int<lower=1> N_games; // number of games\n  int<lower=1> N_teams; // number of teams\n  int<lower=1,upper=N_teams> home_team[N_games]; // home team ID\n  int<lower=1,upper=N_teams> away_team[N_games]; // away team ID\n  \n  vector[N_games] score; // home point differential\n}\n\nparameters{\n  // hierarchical parameters\n  real alpha_mean;\n  real<lower=0> alpha_sigma;\n  \n  // team skill variance\n  real<lower=0> mu_sigma;\n  \n  // observation variance\n  real<lower=0> score_sigma;\n  \n  // HFA and skill vectors\n  vector[N_teams] alpha_raw;\n  vector[N_teams] mu;\n}\n\ntransformed parameters{\n  vector[N_teams] alpha;\n  // centered parameterization helps mixing of alpha_sigma a lot\n  alpha = alpha_mean + alpha_raw*alpha_sigma;\n\n}\n\nmodel{\n  vector[N_games] score_mean;\n  score_mean = alpha[home_team] + mu[home_team] - mu[away_team]; //mean for each game\n  \n  //currently using STAN default priors, uncomment to change\n  //alpha_mean ~ normal(0,10);\n  //alpha_sigma ~ normal(0,10);\n  alpha_raw ~ std_normal();\n  \n\n  //mu_sigma ~ normal(0,10);\n  mu ~ normal(0,mu_sigma);\n  \n  score ~ normal(score_mean,score_sigma);\n}\n\n\nCode\noptions(mc.cores = parallel::detectCores())\n\nweekly_team_win <- pbp %>%\n  distinct(game_id, .keep_all = TRUE) %>%\n  mutate(score_diff = home_score - away_score) %>%\n  group_by(season, week, home_team) %>%\n  summarise(\n    away_team,\n    home_win = mean(home_score - away_score > 0),\n    home_result = sum(home_score - away_score),\n    .groups = \"drop\"\n  ) \n\nteam_vars <- distinct(weekly_team_win, home_team) %>% pull()\n\nfits <- lapply(1999:2021,function(y){\n  year <- y\n  stan_df <- weekly_team_win %>% \n    filter(\n      season == year #, location == \"Home\"\n      )\n  stan_list <- list(\n      N_games = stan_df %>% nrow,\n      N_teams = 32,\n      home_team = stan_df %>% \n        pull(home_team) %>% match(team_vars),\n      away_team = stan_df %>% \n        pull(away_team) %>% match(team_vars),\n      score = stan_df %>% \n        pull(home_result)\n    )\n  \n    fit <- stan(\"~/Downloads/nfl_new/NFL_HFA-master/scripts/season_model.stan\",\n                data = stan_list,\n                iter = 2000,\n                chains = 3,\n                control = list(adapt_delta = 0.99),\n                pars = c(\"score_mean\"),\n                include = FALSE)\n    fit\n})\n\nif(!dir.exists(\"~/Downloads/nfl_new/NFL_HFA-master/output\")) dir.create(\"~/Downloads/nfl_new/NFL_HFA-master/output\")\nsave(team_vars, fits, file = \"~/Downloads/nfl_new/NFL_HFA-master/output/team_effects_default_priors.Rdata\")\n\n\n\n\nCode\n#all teams HFA by season and team\n\n\nstan_home_results %>%\n  left_join(nflfastR::teams_colors_logos, by = c(\"team\" = \"team_abbr\")) %>%\n  mutate(\n    # LV colour is grey so I am using their secondary colour here\n    team_color = ifelse(team_color == \"#a5acaf\", \"#000000\", team_color),\n    season_label = paste0(\"'\", substr(season, 3, 4)),\n    season_label = as_factor(season_label)\n  ) %>%\n  filter(!(team %in% \"ALL\")) %>%\n  ggplot(aes(\n    x = fct_reorder(season_label, season),\n    # x = season,\n    y = mean,\n    group = team,\n    colour = team_color\n  )) +\n  geom_line(size = 1.15) +\n  scale_x_discrete(breaks = c(\"'99\", \"'01\", \"'03\", \"'05\", \"'07\", \"'09\", \"'11\", \"'13\", \"'15\", \"'17\", \"'19\", \"'21\")) +\n  gghighlight(use_direct_label = FALSE, keep_scales = TRUE) +\n  scale_color_identity(aesthetics = c(\"color\", \"fill\")) +\n  facet_wrap(vars(team), scales = \"free\", ncol = 8L) +\n  labs(\n    x = \"Season\",\n    y = \"HFA\",\n    title = \"HFA By Team\",\n    subtitle = \"Regular Season, 1999:2021\",\n    caption = \"data: nflfastR\"\n  ) +\n  becausejustynfun::white_theme() +\n  theme(axis.text = element_text(size = 8))\n\n\n\n\nCode\n#best and worst\nbest_worst <- stan_home_results %>%\n  left_join(nflfastR::teams_colors_logos, by = c(\"team\" = \"team_abbr\")) %>%\n  filter(team != \"ALL\") %>%\n  group_by(season) %>%\n  mutate(mean_rank = rank(mean), n = n()) %>%\n  filter(mean_rank %in% c(1, 32)) %>%\n  mutate(\n    hfa_bw = case_when(\n      mean_rank == 1 ~ \"Worst\",\n      mean_rank == 32 ~ \"Best\"\n      )) %>%\n  ggplot(aes(x = season, y = mean, color = hfa_bw)) +\n  # scale_color_manual(breaks = nflfastR::teams_colors_logos$team_abbr,\n  #                    values = nflfastR::teams_colors_logos$team_color) +\n  geom_line() +\n  ggimage::geom_image(aes(image = team_logo_wikipedia, color = NULL), size = 0.05, by = \"width\") +\n  theme_bw() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\") +\n  labs(\n    colour = \"HFA Rank\", \n    x = \"Season\", \n    y = \"HFA (Points)\",\n    title = \"Best and Worst Home-Field Advantage by Season\"\n    ) \n\n\n\n\nCode\nstan_home_results %>%\n  filter(\n    parameter != \"alpha_sigma\", \n    parameter != \"alpha_mean\"\n    ) %>%\n  group_by(team) %>%\n  summarise(\n    sd = sd(mean), \n    mean = mean(mean)\n    ) %>%\n  arrange(-mean) %>%\n  left_join(select(nflfastR::teams_colors_logos, team = team_abbr, team_logo_espn), by = \"team\") %>%\n  mutate(\n    team_logo_espn = str_c(\"![](\", team_logo_espn, \"){width=30px}\")) %>%\n  select(team_logo_espn, team, mean, sd) %>%\n  knitr::kable(digits = 2, col.names = c(\"\", \"Team\", \"HFA (Mean)\", \"HFA (SD)\"), align = \"clcc\") %>%\n  kableExtra::kable_styling(bootstrap_options = \"striped\", full_width = F)"
  },
  {
    "objectID": "posts/reticulate/index.html",
    "href": "posts/reticulate/index.html",
    "title": "Untitled",
    "section": "",
    "text": "Code\ndata = pd.concat(\n    [nfl.load_pbp_data(season).assign(season = season) for season in range(1999, 2021)]\n)\n\n#if you do not want to install nflfastpy\n\n# Read in data\nYEAR = 2021\n\ndata = pd.read_csv(\n  'https://github.com/guga31bb/nflfastR-data/blob/master/data/play_by_play_' + \n  str(YEAR) + '.csv.gz?raw=True',\n  compression = 'gzip', low_memory = False)\n\n\nHowever, I am using python via reticulate for this, so I could easily load the data through R and then send it over to python.\n\n\nCode\npbp <- purrr::map_df(c(2010:2021), function(x) {\n  readRDS(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\")\n    )\n})\n\n\n\n\nCode\ndata <- reticulate::r_to_py(pbp)\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n\nIf you get an error such as\n\nModuleNotFoundError: No module named ‘matplotlib’\n\nThen you can install matplotlib in the terminal via pip install matplotlib\nI added the data via R, so now I need to move it over to python.\n\n\nCode\ndata = r.data\ndata_df = data.copy()\nprint(data.head())\n\n\n\n\nCode\n#View a random sample of our df to ensure everything is correct          \ndata.sample(3)\n\n\n\n\nCode\n#The last step in preprocessing for this particular analysis is dropping null values to avoid jumps in our WP chart. To clean things up, we can filter the columns to show only those that are of importance to us.\n\ncols = ['home_wp', 'away_wp', 'game_seconds_remaining']\ndata_df = data_df[cols].dropna()\n\n#View new df to again ensure everything is correct\ndata_df\n\n\n\nGame Excitement Index\n\\[\n\\frac{2400}{\\text{Length of Game}} \\sum_{i = 2}^{\\text{n plays}} \\mid \\text{WinProb(i) - WinProb(i - 1)} \\mid\n\\]\n\nthe formula sums the absolute value of the win probability change from each play\n\nhttps://sports.sites.yale.edu/game-excitement-index-part-ii\n\n\nCode\n#Calculate average length of 2019 games for use in our function\navg_length = data.groupby(by = ['game_id'])['epa'].count().mean()\n\ndef calc_gei(game_id):\n  game = data[(data['game_id']==game_id)]\n  #Length of game\n  length = len(game)\n  #Adjusting for game length\n  normalize = avg_length / length\n  #Get win probability differences for each play\n  win_prob_change = game['home_wp'].diff().abs()\n  #Normalization\n  gei = normalize * win_prob_change.sum()\n  return(gei)\n\nprint(f\"Buffalo @ Chiefs GEI: {calc_gei('2021_20_BUF_KC')}\")\n\n\n\n\nCode\n#Set style\nplt.style.use('dark_background')\n\n#Create a figure\nfig, ax = plt.subplots(figsize=(16,8))\n\n\n\n\nCode\n#Generate lineplots\nsns.lineplot('game_seconds_remaining', 'away_wp', \n             data=data_df, color='#3B47CC',linewidth=2)\n\nsns.lineplot('game_seconds_remaining', 'home_wp', \n             data=data_df, color='#E31837',linewidth=2)\n\n#Generate fills for the favored team at any given time\n\nax.fill_between(data_df['game_seconds_remaining'], 0.5, data_df['away_wp'], \n                where=data_df['away_wp']>.5, color = '#3B47CC',alpha=0.3)\n\nax.fill_between(data_df['game_seconds_remaining'], 0.5, data_df['home_wp'], \n                where=data_df['home_wp']>.5, color = '#E31837',alpha=0.3)\n                \n#Labels\nplt.ylabel('Win Probability %', fontsize=16)\nplt.xlabel('', fontsize=16)\n\n#Divider lines for aesthetics\nplt.axvline(x=900, color='white', alpha=0.7)\nplt.axvline(x=1800, color='white', alpha=0.7)\nplt.axvline(x=2700, color='white', alpha=0.7)\nplt.axhline(y=.50, color='white', alpha=0.7)\n\n#Format and rename xticks\nax.set_xticks(np.arange(0, 3601,900))\n\nplt.gca().invert_xaxis()\nx_ticks_labels = ['End','End Q3','Half','End Q1','Kickoff']\nax.set_xticklabels(x_ticks_labels, fontsize=12)\n\n#Titles\n\n#[Text(0, 0, 'End'), Text(900, 0, 'End Q3'), Text(1800, 0, 'Half'), Text(2700, 0, 'End Q1'), Text(3600, 0, 'Kickoff')]\n\nplt.suptitle('Buffalo Bills @ Kansas City Chiefs', \n             fontsize=20, style='italic',weight='bold')\n\nplt.title('KC 42, BUF 36 - Div Round ', fontsize=16, \n          style='italic', weight='semibold')\n\n#Creating a textbox with GEI score\nprops = dict(boxstyle='round', facecolor='black', alpha=0.6)\nplt.figtext(.133,.85,'Game Excitement Index (GEI): 7.29',style='italic',bbox=props)\n\n#Citations\nplt.figtext(0.131,0.137,'Data: @nflfastR')\n\n\n\n\nCode\nplt.show()"
  },
  {
    "objectID": "posts/data_viz/abstract_viz.html",
    "href": "posts/data_viz/abstract_viz.html",
    "title": "Untitled",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggforce)\n\n\n\n\nCode\nset.seed(23479)\nn <- 25\ntibble(\n  x = runif(n),\n  y = runif(n)\n) %>%\n  ggplot(aes(x, y)) +\n  geom_voronoi_tile(aes(fill = y)) +\n  scale_fill_viridis_c(option = \"A\") +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nCode\nset.seed(23479)\nn <- 1000\ntibble(\n  x = runif(n), \n  y = runif(n)) %>%\n  ggplot(aes(x, y)) +\n  geom_voronoi_tile(aes(fill = y)) +\n  scale_fill_viridis_c(option = \"A\") +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nCode\nset.seed(23479)\nN <- 1000\ntibble(\n  x = runif(n, -1, 1), \n  y = sqrt(abs(x) + runif(n))) %>%\n  ggplot(aes(x, y)) +\n  geom_voronoi_tile(aes(fill = y)) +\n  scale_fill_viridis_c(option = \"E\") +\n  theme_void() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/data_viz/colouring_text.html",
    "href": "posts/data_viz/colouring_text.html",
    "title": "Untitled",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggtext) \nlibrary(grid)\n\n\n\n\nCode\nmtcars %>%\n  ggplot(aes(\n    x = wt, \n    y = mpg, \n    colour = factor(cyl)\n  )) +\n  geom_point() +\n  labs(\n    x = \"<span style = 'color:#93C1DE'>**wt**</span>\",\n    y = \"<span style = 'color:#2ca25f'>**mpg**</span>\",\n    title = \"Lower <span style='color:#93C1DE'>**wt**</span> tends to increase the amount of <span style='color:#2ca25f'>**mpg**</span>\",\n    colour = \"\"\n  ) +\n  theme(\n    plot.title = element_markdown(),\n    axis.title.x = element_markdown(),\n    axis.title.y = element_markdown()\n  )\n\n\nColouring one of interest\n\n\nCode\niris %>%\n  group_by(Species) %>%\n  summarise(mean_petal_width = mean(Petal.Width), .groups = \"drop\") %>%\n  mutate(\n    colour = c(\"lightgray\", \"lightgray\", \"#0072B2\"), #009E73\n    name = glue(\"<i style='color:{colour}'>{Species}</i>\"),\n    name = fct_reorder(name, mean_petal_width)\n  ) %>%\n  ggplot(aes(\n    x = name,\n    y = mean_petal_width,\n    fill = colour\n  )) +\n  geom_col() +\n  hrbrthemes::theme_ipsum() +\n  scale_fill_identity() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"<span style = 'color: #43a2ca;'>Virginica irises</span> have the largest average sepal width\"\n  ) +\n  theme(\n    plot.title = element_markdown(),\n    axis.text.x = element_markdown(),\n    legend.position = \"none\"\n  )\n\n\nHaving them all coloured\n\n\nCode\niris %>%\n  group_by(Species) %>%\n  summarise(mean_petal_width = mean(Petal.Width), .groups = \"drop\") %>%\n  mutate(\n    colour = c(\"#91529e\", \"#009E73\", \"#0072B2\"), #009E73\n    name = glue(\"<i style='color:{colour}'>{Species}</i>\"),\n    name = fct_reorder(name, mean_petal_width)\n  ) %>%\n  ggplot(aes(\n    x = name,\n    y = mean_petal_width,\n    fill = colour\n  )) +\n  geom_col() +\n  hrbrthemes::theme_ipsum() +\n  scale_fill_identity() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"<span style = 'color: #43a2ca;'>Virginica irises</span> have the largest average sepal width\"\n  ) +\n  theme(\n    plot.title = element_markdown(),\n    axis.text.x = element_markdown(),\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/nfl_elo/elo_python.html",
    "href": "posts/nfl_elo/elo_python.html",
    "title": "Untitled",
    "section": "",
    "text": "https://andr3w321.com/elo-ratings-part-1/\nFor a simple overview of Elo everyone with no record is assigned a default rating of 1500.\nIf two teams rated at 1500 play each other with a k value of 20:\n\nthe winner would end up with a 1510 rating,\nthe loser ending on a 1490 rating.\n\nThe trick of Elo comes when two teams of drastically different ratings play each other.\nFor example when 1600 beats 1400 with a K value of 20, the new ratings are only 1602 and 1398. Not much changed – only a difference of 2 because the favorite won and was expected to win. However, if 1400 beats 1600, the new ratings would be 1418 and 1582. A much bigger change of 18, because a huge upset occurred its likely that the original 1400 and 1600 rating were incorrect.\n\nCode\ndef rate_1vs1(p1, p2, k=20, drawn=False):\n    rp1 = 10 ** (p1/400)\n    rp2 = 10 ** (p2/400)\n    exp_p1 = rp1 / float(rp1 + rp2)\n    exp_p2 = rp2 / float(rp1 + rp2)\n    if drawn == True:\n        s1 = 0.5\n        s2 = 0.5\n    else:\n        s1 = 1\n        s2 = 0\n    new_p1 = p1 + k * (s1 - exp_p1)\n    new_p2 = p2 + k * (s2 - exp_p2)\n    return(new_p1, new_p2)\n    \n# if you need a win probability function\n\ndef win_probability(p1, p2):\n    diff = p1 - p2\n    p = 1 - 1 / (1 + 10 ** (diff / 400.0))\n    return(p)\n\nExample of the code\n\nCode\nrate_1vs1(1600,1400)\n#(1601.8181818181818, 1398.1818181818182)\nrate_1vs1(1400,1600)\n#(1418.1818181818182, 1581.8181818181818)\nwin_probability(1600,1400)\n#0.759746926647958\n\nhttps://andr3w321.com/elo-ratings-part-2-margin-of-victory-adjustments/\n\n\nFor mov, fivethirtyeight uses the function ln(abs(mov) + 1)\n\nCode\ndef rate_1vs1(p1, p2, mov = 1, k = 20, drawn = False):\n    k_multiplier = 1.0\n    if mov >= 7:\n        k_multiplier = 2.0\n    rp1 = 10 ** (p1/400)\n    rp2 = 10 ** (p2/400)\n    exp_p1 = rp1 / float(rp1 + rp2)\n    exp_p2 = rp2 / float(rp1 + rp2)\n    if drawn == True:\n        s1 = 0.5\n        s2 = 0.5\n    else:\n        s1 = 1\n        s2 = 0\n    new_p1 = p1 + k_multiplier * k * (s1 - exp_p1)\n    new_p2 = p2 + k_multiplier * k * (s2 - exp_p2)\n    return(new_p1, new_p2)\n\nAutocorrelation Lets say instead of two neutral teams playing each other on a neutral field we have two mismatched teams playing each other. We would have four possible outcomes\n1)favorite wins small 2)favorite wins big 3)underdog wins small 4)underdog wins big\n\nCode\nrate_1vs1(1550,1450,1,20)\n#(1560.0, 1440.0)\nrate_1vs1(1550,1450,14,20)\n#(1570.0, 1430.0)\nrate_1vs1(1450,1550,1,20)\n#(1460.0, 1540.0)\nrate_1vs1(1450,1550,14,20)\n#(1470.0, 1530.0)\n\nTo have a variable k multiplyer to reward underdog wins and punish good teams for losing to bad teams, we can use he equation (2.2/((ELOW-ELOL)*.001+2.2))\n\\[\n\\frac{2.2}{((ELOW - ELOL) \\times 0.001 + 2.2)}\n\\]\nFor example, if the favorite wins we have \\(2.2/(100 * 0.001 + 2.2) = 0.956\\) and when the underdog wins we have \\(2.2/(-100 * 0.001 + 2.2) = 1.048\\). Our rewritten elo rating function will be\n\nCode\ndef rate_1vs1(p1, p2, mov=1, k=20, drawn=False):\n    k_multiplier = 1.0\n    corr_m = 1.0\n    if mov >= 7:\n        k_multiplier = 2.0\n        corr_m = 2.2 / ((p1 - p2)*.001 + 2.2)\n    rp1 = 10 ** (p1/400)\n    rp2 = 10 ** (p2/400)\n    exp_p1 = rp1 / float(rp1 + rp2)\n    exp_p2 = rp2 / float(rp1 + rp2)\n    if drawn == True:\n        s1 = 0.5\n        s2 = 0.5\n    else:\n        s1 = 1\n        s2 = 0\n    new_p1 = p1 + k_multiplier * corr_m * k * (s1 - exp_p1)\n    new_p2 = p2 + k_multiplier * corr_m * k * (s2 - exp_p2)\n    return(new_p1, new_p2)\n    \n#Our new ratings\nrate_1vs1(1550,1450,1,20)\n#(1560.0, 1440.0)\nrate_1vs1(1550,1450,14,20)\n#(1569.1304347826087, 1430.8695652173913)\nrate_1vs1(1450,1550,1,20)\n#(1460.0, 1540.0)\nrate_1vs1(1450,1550,14,20)\n#(1470.952380952381, 1529.047619047619)"
  },
  {
    "objectID": "posts/nfl_elo/elo_adjustments.html",
    "href": "posts/nfl_elo/elo_adjustments.html",
    "title": "Untitled",
    "section": "",
    "text": "\\[\n\\underbrace{Elo}_{\\text{current year}} = \\underbrace{Elo}_{\\text{previous year}} \\times \\frac{2}{3} + 1500 \\times \\frac{1}{3}\n\\]\nYou could also use a 4 year rolling mean since the average career is 3.5 years. However, the average player would not make as much difference as key players.\nAdditionally, if you want to value margin of victory that discounts blowouts, e.g. winning by 21 points is not much better than winning by 28\n\\[\n\\text{Margin of Victory Multiplier} = ln(|\\text{PointDiff}| +1) \\times \\frac{2.2}{|\\underbrace{Elo}_{\\text{A}} - \\underbrace{Elo}_{\\text{B}}| \\times 0.001 + 2.2}\n\\]\nIf you want to include turnover and yard differential you could do\nMargin of Victory, Turnover, Yard, Differential Multiplier = MM\n\\[\n\\text{MM} = e^{(\\frac{1}{5}(\\text{Turnover Diff}))} \\times \\text{ ln } (| \\text{PointDiff} | +1) \\times \\frac{2.2}{(|\\underbrace{Elo}_{\\text{A}} - \\underbrace{Elo}_{\\text{B}}| \\times 0.001 + 2.2)}\n\\]\nhttp://schw4rzr0tg0ld.s3-website.eu-central-1.amazonaws.com/blog/2018/08/elo-boosting.html\nThe ELO model uses a scaled logistic function to map differences in strength to probabilities of won and loss. Let \\(Team_{A}\\) be the elo rating for \\(Team_{A}\\), and \\(Team_{B}\\) for \\(Team_{B}\\), and \\(y_{i} = 1\\) if \\(Team_{A}\\) wins, 0.5 if they draw, and 0 if they lose.\n\n\nCode\ncalculate_predictor <- function(X, beta) {\n  # calculate the linear predictor Xbeta\n  colSums(t(X) * beta)\n}\n\nlogistic_function <- function(predictor, scaling_factor = log(10)/400) {\n  # ELO uses a scaling factor compared of log(10) / 400 compared to the\n  # usual specification\n  1 / (1 + exp(-predictor * scaling_factor))\n}\n\nlogistic_loss <- function(X, y, beta) {\n  # number of observations\n  N <- length(y)\n  # expected score, i.e. predicted probability for win/loss\n  e_score <- logistic_function(calculate_predictor(X, beta))\n\n  loss <- (1 / N) * sum(-y * log(e_score) - (1 - y) * log(1 - e_score))\n  return(loss)\n}\n\nupdate_beta <- function(X, y, beta, k) {\n  # number of observations\n  n <- length(y)\n  # expected score, i.e. predicted probability for win/loss\n  e_score <- logistic_function(calculate_predictor(X, beta))\n  # gradient and the current position\n  gradient <- colSums(X * (e_score - y))\n  # update step\n  beta <- beta - k * gradient\n  # print logistic loss\n  #print(logistic_loss(X, y, beta))\n  return(beta)\n}\n\ngradient_descent <- function(batches, beta_init, k, iterations) {\n  # set beta to initial value\n  beta <- beta_init\n  # initalize matrix to store updates\n  beta_history <- matrix(nrow = length(batches),\n                         ncol = length(beta_init))\n  # loop over iterations, aka epochs\n  for (i in 1:iterations) {\n    # loop over mini-batches\n    for (b in 1:length(batches)) {\n      # run update procedure\n      batch <- batches[[b]]\n      beta <- update_beta(batch$X, batch$y, beta, k)\n      beta_history[b, ] <- beta\n    }\n  }\n  return(beta_history)\n}\n\n\n\n\n\n\nCode\ndata <- data.frame(\n  t = factor(c(0, 0, 0, 0, 1, 1, 1)),  # period\n  f = factor(c(1, 2, 3, 1, 2, 3, 3)),  # first team\n  s = factor(c(2, 3, 1, 3, 3, 1, 1)),  # second team\n  y = c(1, 1, 0, 1, 1, 0, 1) # win/loss\n  )  \n  \nget_input_variables <- function(data) {\n  design_matrix <-\n    model.matrix(~ as.factor(f) - 1, data) -\n    model.matrix(~ as.factor(s) - 1, data)\n  colnames(design_matrix) <- paste0(\"player_\", seq_len(ncol(design_matrix)))\n  return(list(X = design_matrix, y = data$y))\n}\n\nbatches <- lapply(split(data, data$t), get_input_variables)\n\n\n\n\nCode\n# determine numer of players from data\nP <- max(sapply(batches, function(x) {ncol(x$X)}))\n# run batch gradient descent\nbgd_beta <- gradient_descent(batches, \n                             beta = numeric(P), \n                             k = 1,\n                             iterations = 1)\n# formatting\nbgd_beta_df <- data.frame(bgd_beta)\ncolnames(bgd_beta_df) <- paste0(\"beta_\", seq_len(P))\n\n\n\n\nCode\nelo_ratings <- PlayerRatings::elo(data.frame(apply(data, 2, as.numeric)),\n                                  kfac = 1, init = 0, history = TRUE)\n# formatting\nelo_ratings_df <- data.frame(t(elo_ratings$history[, , \"Rating\"]))\ncolnames(elo_ratings_df) <- paste0(\"beta_\", seq_len(P))\n\n\n\n\nCode\ndata$t <- factor(0)\n# via glm\nsingle_batch <- get_input_variables(data)\nX <- single_batch$X[, -1]  # drop first player for identification\ny <- single_batch$y\nglm_beta <- c(0, coef(glm(y~ -1 + X, family = binomial)))\nglm_beta <- glm_beta * 400 / log(10)  # normalizing factor from ELO\n\n# formatting\nglm_beta_df <- data.frame(t(glm_beta))\ncolnames(glm_beta_df) <- paste0(\"beta_\", seq_len(P))\n\n\n\n\nCode\nbgd_beta_opt <- gradient_descent(list(single_batch), \n                             beta = numeric(P), \n                             k = 1,\n                             iterations = 1000)\n\n# normalize btea_1 to zero\nbgd_beta_opt <- bgd_beta_opt - bgd_beta_opt[1, 1]\n# formatting\nbgd_beta_opt_df <- data.frame(bgd_beta_opt)\ncolnames(bgd_beta_opt_df) <- paste0(\"beta_\", seq_len(P))\n\n\n\n\nCode\n# for ELO\nELO_beta <- gradient_descent(list(single_batch), \n                             beta = numeric(P), \n                             k = 1,\n                             iterations = 1)\nlogistic_loss(single_batch$X, single_batch$y, as.numeric(ELO_beta))\n\n\n\n\nCode\n# for GLM\nGLM_beta <- gradient_descent(list(single_batch), \n                             beta = numeric(P), \n                             k = 1,\n                             iterations = 1000)\nlogistic_loss(single_batch$X, single_batch$y, as.numeric(GLM_beta))"
  },
  {
    "objectID": "posts/nfl_elo/elo_notes.html",
    "href": "posts/nfl_elo/elo_notes.html",
    "title": "Untitled",
    "section": "",
    "text": "https://andr3w321.com/a-note-on-autocorrelation/ https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/"
  },
  {
    "objectID": "posts/nfl_elo/elo_notes.html#elo-functions",
    "href": "posts/nfl_elo/elo_notes.html#elo-functions",
    "title": "Untitled",
    "section": "Elo functions",
    "text": "Elo functions\n#yearly_adjustment\n\nelo_current_year = (elo_previous_year * 2/3) + (1500 * 1/3)\n\n#margin of victory\n\\[\\text{Margin of Victory Multiplier} = ln(\\text{WinnerPointDiff} + 1) \\times \\frac{2.2}{\\text{WinnerEloDiff} \\times 0.001 + 2.2}\\]\n\\(\\text{Margin of Victory Multiplier} = LN(ABS(\\text{Point Diff})+1) \\times \\left( \\frac{2.2}{((ELOW-ELOL)*.001+2.2)} \\right)\\)\nWhere PD is the point differential in the game, ELOW is the winning team’s Elo Rating before the game, and ELOL is the losing team’s Elo Rating before the game.\nmargin_of_victory = log(winner_point_diff + 1) * (2.2 / winner_elo_diff * 0.001 + 2.2)"
  },
  {
    "objectID": "posts/nfl_elo/elo_autocorr.html",
    "href": "posts/nfl_elo/elo_autocorr.html",
    "title": "Untitled",
    "section": "",
    "text": "FiveThirtyEight uses the following formula for their NFL Elo ratings:\n\\[\nR_{Team_{A}}^{k+1} = R_{Team_{A}}^{k} + K \\cdot M(mov) \\cdot A(x) \\cdot (S_{Team_{A} \\; Team_{B}} - \\sigma(x))\n\\]\nwhere \\(mov\\) is the game’s margin of victory, \\(x = R_{Team_{A}}^{k} - R_{Team_{B}}^{k}\\), and\n\\[\n\\Large\n\\begin{align*}\nM(mov) &= \\ln (|mov|+1) \\\\\nA(x) &= \\frac{2.2}{2.2-0.001(-1)^{S_{Team_{A} \\; Team_{B}}} x} \\\\\n& = \\frac{1}{1-(-1)^{S_{Team_{A} \\; Team_{B}}}\\frac{x}{2200}} \\\\\nS_{Team_{A} \\; Team_{B}} &= \\begin{cases} 1 & Team_{A} \\text{ wins} \\\\ 0.5 & Team_{A} \\text{ ties} \\\\ 0 & Team_{A} \\text{ loses}\\end{cases} \\\\\n\\sigma(x) &= \\frac{1}{1+10^{-x/400}}\n\\end{align*}\n\\]\nMy question is the specific justification for the \\[A(x)\\] term:\n\nwhy this form, and\nwhy those numbers.\n\nI’m looking for more than a layman’s explanation, which 538 already offers and is the typical answer elsewhere. (Although this post goes a bit deeper.)"
  },
  {
    "objectID": "posts/nfl_elo/elo_autocorr.html#quick-intuition",
    "href": "posts/nfl_elo/elo_autocorr.html#quick-intuition",
    "title": "Untitled",
    "section": "Quick intuition",
    "text": "Quick intuition\nSo first you should buy off on the idea that given Team \\[i\\]’s current rating is \\[R_i\\], we should expect its rating after the current game to still be \\[R_i\\]. For example, we shouldn’t ever expect Team \\[i\\]’s rating to increase, because if we did, “we should have rated them higher to begin with”!\nPut in statistical language, this is the statement that we want \\[\\mathbb{E}[R_i^{k+1}\\vert\\text{all prev ratings}] = R_i^k\\], but more on that in a second."
  },
  {
    "objectID": "posts/nfl_elo/elo_autocorr.html#the-typical-explanation",
    "href": "posts/nfl_elo/elo_autocorr.html#the-typical-explanation",
    "title": "Untitled",
    "section": "The typical explanation",
    "text": "The typical explanation\nSo \\(A(x)\\), which again is\n\\[\n\\begin{equation}\nA(x) = \\frac{1}{1-(-1)^{S_{ij}}\\frac{x}{2200}}\n\\label{eq:autocorr}\n\\end{equation}\n\\]\nis intended to correct for over- or under-inflation of ratings in the model. We see the function is designed so that\n\nIf Team \\[i\\] is the favorite (\\[x>0\\]), a loss is upweighted (\\[A(x)>0\\]) and a win is downweighted (\\[A(x)<0\\]).\nIf Team \\[i\\] is the underdog (\\[x<0\\]), the opposite.\n\nSo this seems like it would correct for over-inflation of rating due to a heavy favorite, and vice-versa for a big underdog.\nHowever, we are left with the questions: why achieve it in this way? And why use the denominator \\[d=2200\\]?"
  },
  {
    "objectID": "posts/nfl_elo/elo_autocorr.html#gettin-stats-y",
    "href": "posts/nfl_elo/elo_autocorr.html#gettin-stats-y",
    "title": "Untitled",
    "section": "Gettin stats-y",
    "text": "Gettin stats-y\nWe may interpret Elo ratings as a time series where each new rating depends only on the previous rating, plus some “noise.” More specifically, under this interpretation Elo assumes each team has some true rating, its mean, about which it is constantly fluctuating. This is called an autoregressive model, in our case AR(1). We are also assuming the team’s ratings are stationary, meaning (loosely) the mean stays the same over time.\n(By the way, this is a different interpretation than the connection to SGD I wrote about before, but SGD and AR(1) are, in some sense, the same thing.)\nSkipping some details, (I think) this all amounts to needing the next observation in the time series to, in expectation, equal our current observation. That is, \\[\\mathbb{E}[R_i^{k+1}\\vert\\text{prev ratings}] = R_i^k\\].\nThe fact that we’re fretting about a correction term at all arises because of the margin-of-victory \\[M(z)\\] term we are including. This isn’t part of the “classical” Elo rating scheme, and it’s messing everything up! Without it, we have\n\\[\n\\begin{align*}\n\\mathbb{E}[R_k^{k+1}] &= \\mathbb{E}[R_i^k] + \\mathbb{E}[k(S_{ij}-\\sigma(x))] \\\\\n&= R_i^k + k(\\mathbb{E}[S_{ij}] - \\sigma(x)) \\\\\n&= R_i^k\n\\end{align*}\n\\]\nwhich is just fine.\nNow, including a margin-of-victory term \\[M(z)\\], we need\n\\[\n\\mathbb{E}[M(z)\\cdot A(x) \\cdot (S_{ij} - \\sigma(x))] = 0\n\\]\nwhich, computing expectation over all possible game outcomes as encoded in \\[z\\], given \\[R_i^k\\] and \\[R_j^k\\], implies\n\\[\n\\int_{-\\infty}^0 M(z) A(x) (-\\sigma(x)) \\text{Pr}(z) \\ dz + \\int_0^{\\infty} M(z) A(x) (1-\\sigma(x)) \\text{Pr}(z) \\ dz = 0\n\\]\nover some distribution for the margin \\[z\\]. Rearranging, we get\n\\[\n\\begin{equation}\n\\frac{A(x; i \\text{ win})}{A(x; i \\text{ lose})} = \\frac{\\sigma(x)}{1-\\sigma(x)} \\cdot \\frac{\\mathbb{E}[M(z)|i \\text{ lose}]}{\\mathbb{E}[M(z)|i \\text{ wins}]}\n\\label{eq:goal}\n\\end{equation}\n\\]\nand we want some \\[A(x)\\] so this holds for any Elo delta \\[x\\].\nWe should be able to interpolate some function for the expected \\[M(z)\\]’s, and then if we are satisfied with our functional form for \\[A(x)\\], solve for the denominator \\[d\\].\nLet’s try it."
  },
  {
    "objectID": "posts/nfl_elo/elo_autocorr.html#in-search-of-d",
    "href": "posts/nfl_elo/elo_autocorr.html#in-search-of-d",
    "title": "Untitled",
    "section": "In search of d",
    "text": "In search of d\nWe need to (1) work out the empirical conditional expectations for \\[M(z)\\], then (2) approximate them with functions, and finally (3) solve for \\[d\\] in terms of those functions.\nWe can easily* pull boxscores, Elo ratings, and plot the mean \\[M(z)\\]’s. (*really not that easy, but see code at the bottom of this post.)\nHere’s the expected \\[M(z)\\] given various Elo rating deltas, based on the past 18 NFL seasons.\nNice! So the empirical (conditional) expectations we’re after are both reasonably approximated by linear functions:\n\\[\n\\begin{align*}\n\\mathbb{E}[M(z)|i \\text{ win}] &\\approx \\frac{x}{1000} + 2.2 \\\\\n\\mathbb{E}[M(z)|i \\text{ lose}] &\\approx -\\frac{x}{1000} + 2.2\n\\end{align*}\n\\]\n(Let the reader note: the slope and intercept here are what you might call “eyeball” estimates, although they are quite close to an OLS estimate.)\nReturning to Eq. \\(\\eqref{eq:goal}\\) and using our satisfying functional form for \\[A(x)\\] from Eq. \\(\\eqref{eq:autocorr}\\), we (almost) have\n\\[\n\\begin{align*}\n\\frac{A(x; i \\text{ wins})}{A(x; i \\text{ lose})} &=\n\\frac{\\mathbb{E}[M(z)|i \\text{ wins}]}{\\mathbb{E}[M(z)|i \\text{ lose}]} \\\\\n\\frac{1-x/d}{1+x/d} &\\approx \\frac{-x/1000 + 2.2}{x/1000 + 2.2}\n\\end{align*}\n\\]\nwhich gives \\[d=2200\\]. Voila!\nTo do. To get here, we had to ignore the \\[\\sigma/(1-\\sigma)=10^{x/400}\\] term in Eq. \\(\\eqref{eq:goal}\\). There might be a way to rewrite the expected \\[M(z)\\]’s in a way they still fit the data and cancel this other term out, but if not, I’m not sure."
  },
  {
    "objectID": "posts/nfl_elo/elo_autocorr.html#code",
    "href": "posts/nfl_elo/elo_autocorr.html#code",
    "title": "Untitled",
    "section": "Code",
    "text": "Code\nFortunately we can make heavy use of 538’s public NFL data.\nFirst some imports to get us running in a Jupyter notebook:\n\nCode\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nThen we load the data, extract just the last few seasons, and run the desired stats:\n\nCode\nbox = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/nfl-elo-game/master/data/nfl_games.csv')\n\n# convert date to datetime\nbox['date'] = pd.to_datetime(box['date'])\n\n# grab since ____ season\nbox = (box[box['date'] > '2003-08-01']\n       .reset_index()\n       .drop(['index'], axis=1)\n       .copy())\n\nn = box.shape[0]\nelodiffs = np.zeros(n)\npdiffs   = np.zeros(n)\nfor i, row in box.iterrows():\n    elodiffs[i] = row['elo1'] - row['elo2'] + (0 if row['neutral']==1 else 65)\n    pdiffs[i]   = row['score1'] - row['score2']\n\nWe could do something like a pdiffs vs. elodiffs plot with Seaborn at this point, perhaps a jointplot like this …\n\nCode\nimport seaborn as sns\nsns.jointplot(elodiffs / 25, pdiffs, ylim=(-50,50), kind='hex')\n\nwhich is quite exciting. But what we’re really interested in is the mean MOV function distributions, which is the much less pretty:\n\nCode\nxs = np.linspace(-300,400,10)\n\ndef g1(x):\n    return x/1000) + 2.2\ndef g2(x):\n    return -x/1000 + 2.2\n\nfig, ax = plt.subplots(1,1, figsize=(8,6))\n\nax.plot(ed, mw, 'ko-', label='Win')\nax.plot(xs, g1(xs), 'k--')\n\nax.plot(ed, ml, 'co-', label='Loss')\nax.plot(xs, g2(xs), 'c--')\n\nax.set_xlabel(r'$R_i - R_j$', fontsize=16)\nax.set_ylabel(r'Mean $M(z)$', fontsize=16)\nax.legend()\n\nplt.show()"
  },
  {
    "objectID": "posts/nfl_elo/college_elo_python.html",
    "href": "posts/nfl_elo/college_elo_python.html",
    "title": "Untitled",
    "section": "",
    "text": "Code\n# configure API key\nconfiguration = cfbd.Configuration()\nconfiguration.api_key['Authorization'] = '+TdVQcvhNBRAN948z/lsmZN4ETE9sQ60VyQtobVZ+ARur3G1brcO6FLqhoXAYZw1'\nconfiguration.api_key_prefix['Authorization'] = 'Bearer'\n\n# instantiate a games API instance\napi_config = cfbd.ApiClient(configuration)\ngames_api = cfbd.GamesApi(cfbd.ApiClient(configuration))\n\n\nCode\n#logistic curve function\n\ndef get_expected_score(team_a, team_b):\n    exp = (team_b - team_a) / 400\n    return(1 / (1 + 10**exp))\n\n#team_a_new_elo = team_a_old_elo + K (score for team A - expected score for team A)\n\n\nCode\nprint(get_expected_score(1500, 1500))\nprint(get_expected_score(1500, 1400))\nprint(get_expected_score(1750, 1500))\nprint(get_expected_score(2000, 1500))\n\n\nCode\n#Making adjustments to Elo score based on the outcome of a game. \n# takes home team Elo rating, away team Elo rating, and final scoring margin as parameters and returns the new Elo ratings for the participants. \n\ndef get_new_elos(home_rating, away_rating, margin):\n    k = 25\n\n    # score of 0.5 for a tie\n    home_score = 0.5\n    if margin > 0:\n        # score of 1 for a win\n        home_score = 1\n    elif margin < 0:\n        #score of 0 for a loss\n        home_score = 0\n\n    # get expected home score\n    expected_home_score = get_expected_score(home_rating, away_rating)\n    # multiply difference of actual and expected score by k value and adjust home rating\n    new_home_score = home_rating + k * (home_score - expected_home_score)\n\n    # repeat these steps for the away team\n    # away score is inverse of home score\n    away_score = 1 - home_score\n    expected_away_score = get_expected_score(away_rating, home_rating)\n    new_away_score = away_rating + k * (away_score - expected_away_score)\n\n    # return a tuple\n    return(round(new_home_score), round(new_away_score))\n\n\nCode\n# Again, we're merely finding the difference in actual and expected scores for each team, multiplying them by our predefined K factor of 25, and adding that adjustment to the pregame Elo scores. \n# Finally, we returned a tuple of two values, the first being the new Elo rating for the home team and the second being that of the away team. \n# I should probably also note that the margin value refers to the difference in home and away points (i.e. a positive value is a home win while a negative value is an away win)\n\n# takes a data string and converts it to a datetime object for sorting\ndef date_sort(game):\n    game_date = datetime.datetime.strptime(game['start_date'], \"%Y-%m-%dT%H:%M:%S.000Z\")\n    return(game_date)\n\n# returns an elo rating\ndef elo_sort(team):\n    return(team['elo'])\n\n\nCode\ngames = []\n\nfor year in range(2010, 2022):\n    response = games_api.get_games(year = year)\n    games = [*games, *response]\n\ngames = [dict(\n            start_date = g.start_date,\n            home_team = g.home_team,\n            home_conference = g.home_conference,\n            home_points = g.home_points,\n            away_team = g.away_team,\n            away_conference = g.away_conference,\n            away_points = g.away_points\n            ) for g in games if g.home_points is not None and g.away_points is not None]\n\ngames.sort(key = date_sort)\n\n\nCode\n# We can now loop through each game to calculate current Elo ratings for each team. \n# We will also be tracking pregame and postgame Elo ratings for each game for each team so that we can track team trends over time as well as look at how game outcomes affect team ratings.\n\n# dict object to hold current Elo rating for each team\nteams = dict()\n\n# loop through games in order\nfor game in games:\n\n    # get current rating for home team\n    if game['home_team'] in teams:\n        home_elo = teams[game['home_team']]\n    elif game['home_conference'] is not None:\n        # if no rating, set initial rating to 1500 for FBS teams\n        home_elo = 1500\n    else:\n        # otherwise, set initial rating to 1200 for non-FBS teams\n        home_elo = 1200\n\n    # get current rating for away team\n    if game['away_team'] in teams:\n        away_elo = teams[game['away_team']]\n    elif game['away_conference'] is not None:\n        # if no rating, set initial rating to 1500 for FBS teams\n        away_elo = 1500\n    else:\n        # otherwise, set initial rating to 1200 for non-FBS teams\n        away_elo = 1200\n\n    # calculate score margin from game\n    margin = game['home_points'] - game['away_points']\n\n    # get new elo ratings\n    new_elos = get_new_elos(home_elo, away_elo, margin)\n\n    # set pregame elos on game dict\n    game['pregame_home_elo'] = home_elo\n    game['pregame_away_elo'] = away_elo\n\n    # set postgame elos on game dict\n    game['postgame_home_elo'] = new_elos[0]\n    game['postgame_away_elo'] = new_elos[1]\n\n    # set current elo values in teams dict\n    teams[game['home_team']] = new_elos[0]\n    teams[game['away_team']] = new_elos[1]\n\n\nCode\n# For non-FBS teams, we'll set an initial rating of 1200.\n\n#convert our team dict (which holds the current rating for each team) into a list ordered by rating from highest to lowest\n\nend_elos = [dict(team = key, elo = teams[key]) for key in teams]\nend_elos.sort(key = elo_sort, reverse = True)\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# This is the styling I use. Check out other themes here: https://matplotlib.org/3.2.1/gallery/style_sheets/style_sheets_reference.html\nplt.style.use('fivethirtyeight')\n\n# Graph sizing\nplt.rcParams[\"figure.figsize\"] = [20,10]\n\n\nCode\ndef generate_chart(team):\n    team_games = []\n    for game in games:\n        if game['home_team'] == team:\n            team_games.append(dict(start_date = game['start_date'], elo = game['postgame_home_elo']))\n\n        if game['away_team'] == team:\n            team_games.append(dict(start_date = game['start_date'], elo = game['postgame_away_elo']))\n\n    df = pd.DataFrame.from_records(team_games)\n\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['elo'])\n\n    ax.set(xlabel = 'Game No.', ylabel = 'Elo Rating',\n           title = \"Historical Elo Rating - {0}\".format(team))\n\n    plt.show()\n\n\nCode\ngenerate_chart('Michigan')"
  },
  {
    "objectID": "posts/nfl_elo/elo_vs_spread.html",
    "href": "posts/nfl_elo/elo_vs_spread.html",
    "title": "Untitled",
    "section": "",
    "text": "Code\nurl = \"https://github.com/fivethirtyeight/nfl-elo-game/raw/master/data/nfl_games.csv\"\ndf = pd.read_csv(url)\nprint(df.shape)\ndf.head(5)\n\n\nCode\n# create the spread column. This is just point diff\n\ndf['spread'] = df.score1 - df.score2\nprint(\"Total teams in the dataset:\", len(df.team1.unique()))\nprint(\"Total teams after 1990:\", len(df[df.season > 1990].team1.unique()))\n\n\nCode\ndef color_coded_hist(x, **kwargs):\n    \"\"\"Color the negative and positive bins differently.\n\n    Color scheme from http://colorbrewer2.org/    \n    \"\"\"\n    hist_bins = [-75, -42, -35, -28, -21, -14, -7, 0, 7, 14, 21, 28, 35, 42, 75]\n    __, __, patches = plt.hist(x, density=True, bins=hist_bins, color=\"#f1a340\")\n    # Purple for positive spread:\n    i = next(i for i, val in enumerate(hist_bins) if val == 0)\n    for p in patches[i:]:\n        p.set_facecolor(\"#998ec3\")\n        \n\ndef best_norm(x, **kwargs):\n    \"\"\"Plot the best normal fit for the data.\"\"\"\n    mu, std = stats.norm.fit(x)\n    # Plot the PDF.\n    xmin, xmax = min(x), max(x)\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k--', alpha = 0.6)\n\n\nCode\n# Plot all of the teams' spreads since 1990\n\ng = sns.FacetGrid(\n    df[df.season > 1990].sort_values('team1'), \n    col = \"team1\", col_wrap = 5, height = 2)\n\ng = (g\n.map(color_coded_hist, \"spread\")\n.map(best_norm, \"spread\")\n.set_titles(\"{col_name}\")\n.set_axis_labels(\"spread\", \"density\"))\n\nmsg = \"Histograms of spread (at home) since 1990. Normal approximation is dashed line.\"\nplt.suptitle(msg, y = 1.025, fontsize = 14)\n\n\nCode\n# Show some of the values\nprint(\"~ Overall ~\")\n\nprint(\n    df[\n        df.neutral == 0][\n            ['spread', 'result1']].mean())\n\nprint(\"\\n~ Past few seasons ~\")\n\nprint(\n    df[\n        (df.neutral == 0) & \n        (df.season > 2011)\n        ]\n    .groupby(\"season\")[[\"spread\", \"result1\"]]\n    .mean()\n)\n\n\nCode\n# Create a data frame with lagged rolling means and combine with the existing data\nrolling_avg = (\n    df.groupby(\"season\")[[\"spread\", \"result1\"]]\n    .mean()\n    .rolling(3, min_periods = 1)\n    .mean()\n    .shift(1)\n)\n\nrolling_avg[\"win_pct_advantage\"] = rolling_avg.result1 - 0.5\nrolling_avg[\"spread_advantage\"] = rolling_avg.spread\nrolling_avg = rolling_avg.drop(columns = [\"result1\", \"spread\"])\n\n\nCode\n# Plot everything\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (14.0, 4.0)\nf, ax = plt.subplots(1, 2)\n\nsns.lineplot(x = \"season\", y = \"spread\", data = df[df.neutral == 0], ax = ax[0])\nax[0].plot(rolling_avg.index, rolling_avg.spread_advantage, 'k:')\nax[0].set_ylabel(\"home spread\")\n\nsns.lineplot(x = \"season\", y = \"result1\", data = df[df.neutral == 0], ax = ax[1])\nax[1].plot(rolling_avg.index, rolling_avg.win_pct_advantage + .5, 'k:')\nax[1].set_ylabel(\"home win percent\")\n\nplt.suptitle(\n    'Central tendency and confidence interval of spread (left), '\n    'win percent (right) by season — lagged moving average is dotted line',\n    fontsize = 14)\n\n\nCode\n#Convert from game to team format\n\nhome_games = df[\n    [\"date\", \"season\", \"neutral\", \"playoff\", \"team1\", \"team2\", \"result1\", \"spread\"]\n][df.season > 1990]\n\nhome_games[\"home\"] = 1\n\n# Now swap the teams for \"away\"\naway_games = home_games.rename(columns = {\"team1\": \"team2\", \"team2\": \"team1\"})\naway_games[\"home\"] = 1 - home_games.home\n\n# Remember to switch the meaning of the winning and spread columns too\naway_games[\"result1\"] = 1 - home_games.result1\naway_games[\"spread\"] = -home_games.spread\n\nby_team = pd.concat(\n    [home_games, away_games], ignore_index = True).sort_values(\n    by=[\"season\", \"team1\", \"date\"]\n)\n\n\nCode\nfrom abc import ABC, abstractmethod\nfrom collections import namedtuple\n\nclass Updater(ABC):\n    def __init__(self, *hyperparameters):\n        # Form of lookup:\n        # {season: {team: [{week1 data}, {week2 data}, ... {weekN data}]}}\n        self.lookup = {}\n        self.Params = namedtuple('Params', ['date'] + list(hyperparameters))\n        \n    def iterrows(self):\n        for season, teams in self.lookup.items():\n            for team, results in teams.items():\n                for row in results:\n                    yield dict(season=season, team1=team, **row._asdict())\n        \n    def get_rows(self):\n        return [r for r in self.iterrows()]\n    \n    @abstractmethod\n    def revert_to_mean(self, season, team, keep=.3, n_obs=8):\n        pass\n    \n    @abstractmethod\n    def update(self, row):\n        pass\n\n\nCode\nclass BernoulliUpdater(Updater):\n    def __init__(self):\n        super().__init__('alpha1', 'beta1')\n        \n    def revert_to_mean(self, season, team, keep=.8, n_obs=4):\n        # default\n        alpha = beta = 1 + n_obs * .5\n        # or use existing data        \n        if season in self.lookup and team in self.lookup[season]:\n            last_entry = self.lookup[season][team].pop()\n            date, alpha0, beta0 = last_entry\n            p = alpha0 / (alpha0 + beta0)\n            alpha = 1 + n_obs * (keep * p + (1 - keep) * .5)\n            beta = 1 + n_obs * (keep * (1 - p) + (1 - keep) * .5)\n            # push back the reverted value to the list\n            self.lookup[season][team].append(self.Params(date, alpha, beta))\n        return alpha, beta\n            \n    def update(self, row):\n        if row.season not in self.lookup:\n            self.lookup[row.season] = {}\n        if row.team1 not in self.lookup[row.season]:\n            self.lookup[row.season][row.team1] = []\n            alpha, beta = self.revert_to_mean(row.season - 1, row.team1)\n        else:\n            __, alpha, beta = self.lookup[row.season][row.team1][-1]\n        # THE UPDATE STEP:\n        # a' = a + 1 if win else 0\n        # b' = b + 1 if lose\n        if row.result1 == 1:  # Won\n            alpha_beta_next = self.Params(row.date, alpha + 1, beta)\n        elif row.result1 == 0.5:  # Tie\n            alpha_beta_next = self.Params(row.date, alpha + .5, beta + .5)\n        else:  # Lost\n            alpha_beta_next = self.Params(row.date, alpha, beta + 1)\n        self.lookup[row.season][row.team1].append(alpha_beta_next)\n        return alpha, beta\n\n\nbernoulli_updater = BernoulliUpdater()\nfor i, row in by_team.iterrows():\n    bernoulli_updater.update(row)\n\nab = pd.DataFrame(bernoulli_updater.get_rows()).sort_values(['team1','season'])\ng = ab.groupby('team1')\nab = ab.assign(alpha1 = g.alpha1.shift(), beta1=g.beta1.shift())\n\nbernoulli_dataset = (\n    by_team[[c for c in by_team.columns if c != 'spread']]\n    .merge(ab, on=['season', 'date', 'team1'])\n    .reindex(columns=[\n        'season', 'date', 'home', 'neutral', 'playoff',\n        'team1', 'team2', 'result1', 'alpha1', 'beta1'])\n)\n\n\nCode\nbernoulli_dataset[(bernoulli_dataset.season == 1993) & (bernoulli_dataset.team1 == 'PHI')]\n\n\nCode\n#Convert back from \"by-team\" to \"by-game\" format\n\nb = (\n    bernoulli_dataset[['season', 'date', 'team1', 'alpha1', 'beta1']]\n    .rename(columns=dict(team1='team2', alpha1='alpha2', beta1='beta2'))\n    .merge(bernoulli_dataset, on=['season', 'date', 'team2'])\n    .join(\n        rolling_avg[['win_pct_advantage']]\n        .rename(columns={'win_pct_advantage':'home_advantage'})\n        , on='season')\n)\n\nb = (\n    b.assign(\n        pwin = \n        (b.alpha1 + b.beta2 - 1) / (b.alpha1 + b.beta1 + b.alpha2 + b.beta2 - 2)\n        # if at home and not neutral add home advantage\n        + b.home * (1 - b.neutral) * b.home_advantage\n        # if away and not neutral subtract home advantage\n        - (1 - b.home) * (1 - b.neutral) * b.home_advantage\n        ,\n        success = lambda row:  row.pwin.round() == row.result1\n    )\n    .reindex(columns=(\n        list(bernoulli_dataset.columns)\n        + ['alpha2', 'beta2', 'home_advantage', 'pwin', 'success']\n    ))\n)\n\nprint(b.success.mean())\nb.tail()\n\n\nCode\ndef plot_roc(predicted, actual, resolution=100, ax=None):\n    \"\"\"'predicted' and 'actual' are pandas Series.\"\"\"\n    ax = ax or plt.gca()\n    cutoff = np.linspace(0, 1, resolution)\n    total_pos = (actual == 1).sum()\n    total_neg = (actual != 1).sum()\n    true_positive_rate = np.fromiter(\n        map(lambda c: (actual[predicted > c] == 1).sum() / total_pos, cutoff),\n        float)\n    false_positive_rate = np.fromiter(\n        map(lambda c: (actual[predicted > c] != 1).sum() / total_neg, cutoff),\n        float)\n    ax.plot(\n        false_positive_rate, true_positive_rate,\n        linestyle='-', color=sns.color_palette()[0], linewidth=3)\n    ax.set_xlim([0,1])\n    ax.set_ylim([0,1])\n    ax.plot([0,1], [0,1], 'k:')\n    # Area under the curve\n    auc = sum((true_positive_rate[:-1] + true_positive_rate[1:]) / 2\n              * (false_positive_rate[:-1] - false_positive_rate[1:]))\n    ax.set_title('ROC curve. AUC = {:0.3f}'.format(auc), fontsize=14);\n\n\n## Start the actual plot\nplt.rcParams['figure.figsize'] = (15.0, 3.0)\nf, ax = plt.subplots(1, 3)\n\nsummary = b.groupby(['team1', 'season'], as_index=False).success.mean()\n\n# Histogram\nsns.distplot(summary.success, ax=ax[0], bins=np.linspace(0, 1, 11))\nax[0].axvline(0.5, color='k', linestyle=':')\nax[0].set_ylabel(\"frequency count\")\nax[0].set_title('Model accuracy (grouped by team, season)', fontsize=14)\n\n# Time series\nsns.lineplot(x=\"season\", y=\"success\", data=summary, ax=ax[1])\nax[1].set_ylabel(\"Model success rate\")\nax[1].set_title('Accuracy year over year (mean {:0.0%})'.format(b.success.mean()), fontsize=14)\n\n# ROC\nplot_roc(b.pwin, b.result1, resolution=100, ax=ax[2])\n\n\nCode\nclass TUpdater(Updater):\n    def __init__(self):\n        super().__init__('nu1', 'mu1', 'alpha1', 'beta1')\n        \n    def get_mean_beta(self, season):\n        mean_beta = 16**2 / 2  # Default\n        if season in self.lookup:\n            team_sets = self.lookup[season].values()\n            mean_beta = (\n                sum(ts[-1].beta1 for ts in team_sets)\n                / sum(ts[-1].nu1 for ts in team_sets))\n        return mean_beta\n        \n    def revert_to_mean(self, season, team, keep=.5, n_obs=3):\n        mean_beta = self.get_mean_beta(season - 1)  # Default\n        nu, mu, alpha, beta = n_obs, 0, n_obs / 2, mean_beta * n_obs\n        # or use existing data\n        if season in self.lookup and team in self.lookup[season]:\n            last_entry = self.lookup[season][team].pop()\n            date, nu0, mu0, alpha0, beta0 = last_entry\n            mu = keep * mu0\n            beta = nu * (keep * beta0 / nu0 + (1 - keep) * mean_beta)\n            # push back the reverted value to the list\n            self.lookup[season][team].append(self.Params(date, nu, mu, alpha, beta))\n        return nu, mu, alpha, beta\n            \n    def update(self, row):\n        if row.season not in self.lookup:\n            self.lookup[row.season] = {}\n        if row.team1 not in self.lookup[row.season]:\n            self.lookup[row.season][row.team1] = []\n            nu, mu, alpha, beta = self.revert_to_mean(row.season - 1, row.team1)\n        else:\n            __, nu, mu, alpha, beta = self.lookup[row.season][row.team1][-1]\n        # THE UPDATE STEP:\n        delta = row.spread - mu\n        nu_mu_alpha_beta_next = self.Params(\n            row.date,\n            nu + 1,                       # nu' = nu + 1\n            mu + delta / (nu + 1),        # mu' = mu + delta / (nu + 1)\n            alpha + .5,                   # alpha' = alpha + 1/2\n            beta + delta * (mu + delta / (nu + 1)) / 2\n                                          # beta' = beta + delta * mu' / 2\n        )\n        self.lookup[row.season][row.team1].append(nu_mu_alpha_beta_next)\n        return nu, mu, alpha, beta\n\n\nt_updater = TUpdater()\nfor i, row in by_team.iterrows():\n    t_updater.update(row)\n\nnmab = pd.DataFrame(t_updater.get_rows()).sort_values(['team1','season'])\ng = nmab.groupby('team1')\nnmab = nmab.assign(\n    nu1 = g.nu1.shift(),\n    mu1 = g.mu1.shift(),\n    alpha1 = g.alpha1.shift(),\n    beta1=g.beta1.shift())\n\nt_dataset = (\n    by_team[[c for c in by_team.columns if c != 'result1']]\n    .merge(nmab, on=['season', 'date', 'team1'])\n    .reindex(columns=[\n        'season', 'date', 'home', 'neutral', 'playoff',\n        'team1', 'team2', 'spread', 'nu1', 'mu1', 'alpha1', 'beta1'])\n)\n\n\nCode\nt_dataset[(t_dataset.season == 2017) & (t_dataset.team1 == 'PHI')]\n\n\nCode\nt = (\n    t_dataset[['season', 'date', 'team1', 'nu1', 'mu1', 'alpha1', 'beta1']]\n    .rename(columns=dict(\n        team1='team2', nu1='nu2', mu1='mu2', alpha1='alpha2', beta1='beta2'))\n    .merge(t_dataset, on=['season', 'date', 'team2'])\n    .join(\n        rolling_avg[['spread_advantage']]\n        .rename(columns={'spread_advantage':'home_advantage'})\n        , on='season')\n)\n\nt = (\n    t.assign(\n        pspread =\n            (t.nu1 * t.mu1 - t.nu2 * t.mu2) / (t.nu1 + t.nu2)\n            # if at home and not neutral add home advantage\n            + t.home * (1 - t.neutral) * t.home_advantage\n            # if away and not neutral subtract home advantage\n            - (1 - t.home) * (1 - t.neutral) * t.home_advantage\n        ,\n        betaprime =\n            t.beta1 + t.beta2\n            + (t.nu1 * t.nu2) / (t.nu1 + t.nu2)\n            * (t.mu1 + t.mu2)**2 / 2\n        ,\n        pwin = (\n            lambda row: 1 - stats.t.cdf(\n                0,\n                row.nu1 + row.nu2,\n                loc=row.pspread,\n                scale=(\n                    row.betaprime\n                    * (row.nu1 + row.nu2 + 1)\n                    / (row.nu1 + row.nu2) / (row.alpha1 + row.alpha2)\n                )))\n        ,\n        success = lambda row: row.pwin.round() == (row.spread > 0)\n    )\n    .reindex(columns=(\n        list(t_dataset.columns)\n        + ['nu2', 'mu2', 'alpha2', 'beta2', 'home_advantage', 'pspread', 'pwin', 'success']\n    ))\n)\n\nprint(t.success.mean())\nprint(t.shape)\nt.tail()\n\n\nCode\n# use toggle\nss_res = ((t.spread - t.pspread)**2).sum()\nss_tot = ((t.spread - t.spread.mean())**2).sum()\nr_squared = 1 -  ss_res/ ss_tot\nsns.jointplot(\"spread\", \"pspread\", data=t, kind=\"hex\", space=0, color=\"b\", ratio=4)\ntitle = \"Actual spread vs. mean of  distribution. R squared= {:0.0%}\".format(r_squared)\nplt.suptitle(title, x=.45, y=1.01, fontsize=14)\n\n\nCode\n# use toggle\nplt.rcParams['figure.figsize'] = (15.0, 3.0)\nf, ax = plt.subplots(1, 3)\n\nsummary = t.groupby(['team1', 'season'], as_index=False).success.mean()\n\n# Histogram\nsns.distplot(summary.success, ax=ax[0], bins=np.linspace(0, 1, 11))\nax[0].axvline(0.5, color='k', linestyle=':')\nax[0].set_ylabel(\"frequency count\")\nax[0].set_title('Model accuracy (grouped by team, season)', fontsize=14)\n\n# Time series\nsns.lineplot(x=\"season\", y=\"success\", data=summary, ax=ax[1])\nax[1].set_ylabel(\"Model success rate\")\nax[1].set_title('Accuracy year over year (mean {:0.0%})'.format(t.success.mean()), fontsize=14)\n\n# ROC\nplot_roc(t.pwin, t.spread > 0, resolution=100, ax=ax[2])"
  },
  {
    "objectID": "posts/command_line/index.html",
    "href": "posts/command_line/index.html",
    "title": "Untitled",
    "section": "",
    "text": "create new venv\npython3 -m venv env\nactivate it\nsource env/bin/activate\ninstalling packages\npython3 -m pip install jupyter matplotlib pandas\ndeactivate\ndeactivate"
  },
  {
    "objectID": "posts/tidyverse_tricks_i/index.html",
    "href": "posts/tidyverse_tricks_i/index.html",
    "title": "Tidyverse Tips I",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\n\n\nThe data was originally from Kaggle 1.1 https://www.kaggle.com/crawford/80-cereals\n\n\n\n\n\nCode\ncereal <- read_delim(\n  'https://github.com/becausejustyn/blog/raw/main/content/post/2021-07-12-tidyverse-tips/cereal.txt',\n  delim = \"\\t\", \n  col_types = cols(\n    type = readr::col_factor(levels = NULL),\n    mfr = readr::col_factor(levels = NULL)\n    ),\n    escape_double = FALSE,\n    trim_ws = TRUE)\n\n\n\n\nCode\ncereal %>%\n    gt() %>% \n    tab_options(container.height = '500px') %>%\n    gt_theme_538() \n\n\n\n\n\n\n  \n  \n    \n      name\n      mfr\n      type\n      shelf\n      weight\n      cups\n      rating\n      calories\n      sugars\n      protein\n      fat\n      sodium\n      fiber\n    \n  \n  \n    100% Bran\nN\nC\n3\n1.00\n0.33\n68.40297\n70\n6\n4\n1\n130\n10.0\n    100% Natural Bran\nQ\nC\n3\n1.00\n1.00\n33.98368\n120\n8\n3\n5\n15\n2.0\n    All-Bran\nK\nC\n3\n1.00\n0.33\n59.42551\n70\n5\n4\n1\n260\n9.0\n    All-Bran with Extra Fiber\nK\nC\n3\n1.00\n0.50\n93.70491\n50\n0\n4\n0\n140\n14.0\n    Almond Delight\nR\nC\n3\n1.00\n0.75\n34.38484\n110\n8\n2\n2\n200\n1.0\n    Apple Cinnamon Cheerios\nG\nC\n1\n1.00\n0.75\n29.50954\n110\n10\n2\n2\n180\n1.5\n    Apple Jacks\nK\nC\n2\n1.00\n1.00\n33.17409\n110\n14\n2\n0\n125\n1.0\n    Basic 4\nG\nC\n3\n1.33\n0.75\n37.03856\n130\n8\n3\n2\n210\n2.0\n    Bran Chex\nR\nC\n1\n1.00\n0.67\n49.12025\n90\n6\n2\n1\n200\n4.0\n    Bran Flakes\nP\nC\n3\n1.00\n0.67\n53.31381\n90\n5\n3\n0\n210\n5.0\n    Cap'n'Crunch\nQ\nC\n2\n1.00\n0.75\n18.04285\n120\n12\n1\n2\n220\n0.0\n    Cheerios\nG\nC\n1\n1.00\n1.25\n50.76500\n110\n1\n6\n2\n290\n2.0\n    Cinnamon Toast Crunch\nG\nC\n2\n1.00\n0.75\n19.82357\n120\n9\n1\n3\n210\n0.0\n    Clusters\nG\nC\n3\n1.00\n0.50\n40.40021\n110\n7\n3\n2\n140\n2.0\n    Cocoa Puffs\nG\nC\n2\n1.00\n1.00\n22.73645\n110\n13\n1\n1\n180\n0.0\n    Corn Chex\nR\nC\n1\n1.00\n1.00\n41.44502\n110\n3\n2\n0\n280\n0.0\n    Corn Flakes\nK\nC\n1\n1.00\n1.00\n45.86332\n100\n2\n2\n0\n290\n1.0\n    Corn Pops\nK\nC\n2\n1.00\n1.00\n35.78279\n110\n12\n1\n0\n90\n1.0\n    Count Chocula\nG\nC\n2\n1.00\n1.00\n22.39651\n110\n13\n1\n1\n180\n0.0\n    Cracklin' Oat Bran\nK\nC\n3\n1.00\n0.50\n40.44877\n110\n7\n3\n3\n140\n4.0\n    Cream of Wheat (Quick)\nN\nH\n2\n1.00\n1.00\n64.53382\n100\n0\n3\n0\n80\n1.0\n    Crispix\nK\nC\n3\n1.00\n1.00\n46.89564\n110\n3\n2\n0\n220\n1.0\n    Crispy Wheat & Raisins\nG\nC\n3\n1.00\n0.75\n36.17620\n100\n10\n2\n1\n140\n2.0\n    Double Chex\nR\nC\n3\n1.00\n0.75\n44.33086\n100\n5\n2\n0\n190\n1.0\n    Froot Loops\nK\nC\n2\n1.00\n1.00\n32.20758\n110\n13\n2\n1\n125\n1.0\n    Frosted Flakes\nK\nC\n1\n1.00\n0.75\n31.43597\n110\n11\n1\n0\n200\n1.0\n    Frosted Mini-Wheats\nK\nC\n2\n1.00\n0.80\n58.34514\n100\n7\n3\n0\n0\n3.0\n    Fruit & Fibre Dates; Walnuts; and Oats\nP\nC\n3\n1.25\n0.67\n40.91705\n120\n10\n3\n2\n160\n5.0\n    Fruitful Bran\nK\nC\n3\n1.33\n0.67\n41.01549\n120\n12\n3\n0\n240\n5.0\n    Fruity Pebbles\nP\nC\n2\n1.00\n0.75\n28.02576\n110\n12\n1\n1\n135\n0.0\n    Golden Crisp\nP\nC\n1\n1.00\n0.88\n35.25244\n100\n15\n2\n0\n45\n0.0\n    Golden Grahams\nG\nC\n2\n1.00\n0.75\n23.80404\n110\n9\n1\n1\n280\n0.0\n    Grape Nuts Flakes\nP\nC\n3\n1.00\n0.88\n52.07690\n100\n5\n3\n1\n140\n3.0\n    Grape-Nuts\nP\nC\n3\n1.00\n0.25\n53.37101\n110\n3\n3\n0\n170\n3.0\n    Great Grains Pecan\nP\nC\n3\n1.00\n0.33\n45.81172\n120\n4\n3\n3\n75\n3.0\n    Honey Graham Ohs\nQ\nC\n2\n1.00\n1.00\n21.87129\n120\n11\n1\n2\n220\n1.0\n    Honey Nut Cheerios\nG\nC\n1\n1.00\n0.75\n31.07222\n110\n10\n3\n1\n250\n1.5\n    Honey-comb\nP\nC\n1\n1.00\n1.33\n28.74241\n110\n11\n1\n0\n180\n0.0\n    Just Right Crunchy  Nuggets\nK\nC\n3\n1.00\n1.00\n36.52368\n110\n6\n2\n1\n170\n1.0\n    Just Right Fruit & Nut\nK\nC\n3\n1.30\n0.75\n36.47151\n140\n9\n3\n1\n170\n2.0\n    Kix\nG\nC\n2\n1.00\n1.50\n39.24111\n110\n3\n2\n1\n260\n0.0\n    Life\nQ\nC\n2\n1.00\n0.67\n45.32807\n100\n6\n4\n2\n150\n2.0\n    Lucky Charms\nG\nC\n2\n1.00\n1.00\n26.73451\n110\n12\n2\n1\n180\n0.0\n    Muesli Raisins; Dates; & Almonds\nR\nC\n3\n1.00\n1.00\n37.13686\n150\n11\n4\n3\n95\n3.0\n    Muesli Raisins; Peaches; & Pecans\nR\nC\n3\n1.00\n1.00\n34.13976\n150\n11\n4\n3\n150\n3.0\n    Mueslix Crispy Blend\nK\nC\n3\n1.50\n0.67\n30.31335\n160\n13\n3\n2\n150\n3.0\n    Multi-Grain Cheerios\nG\nC\n1\n1.00\n1.00\n40.10596\n100\n6\n2\n1\n220\n2.0\n    Nut&Honey Crunch\nK\nC\n2\n1.00\n0.67\n29.92429\n120\n9\n2\n1\n190\n0.0\n    Nutri-Grain Almond-Raisin\nK\nC\n3\n1.33\n0.67\n40.69232\n140\n7\n3\n2\n220\n3.0\n    Nutri-grain Wheat\nK\nC\n3\n1.00\n1.00\n59.64284\n90\n2\n3\n0\n170\n3.0\n    Oatmeal Raisin Crisp\nG\nC\n3\n1.25\n0.50\n30.45084\n130\n10\n3\n2\n170\n1.5\n    Post Nat. Raisin Bran\nP\nC\n3\n1.33\n0.67\n37.84059\n120\n14\n3\n1\n200\n6.0\n    Product 19\nK\nC\n3\n1.00\n1.00\n41.50354\n100\n3\n3\n0\n320\n1.0\n    Puffed Rice\nQ\nC\n3\n0.50\n1.00\n60.75611\n50\n0\n1\n0\n0\n0.0\n    Puffed Wheat\nQ\nC\n3\n0.50\n1.00\n63.00565\n50\n0\n2\n0\n0\n1.0\n    Quaker Oat Squares\nQ\nC\n3\n1.00\n0.50\n49.51187\n100\n6\n4\n1\n135\n2.0\n    Raisin Bran\nK\nC\n2\n1.33\n0.75\n39.25920\n120\n12\n3\n1\n210\n5.0\n    Raisin Nut Bran\nG\nC\n3\n1.00\n0.50\n39.70340\n100\n8\n3\n2\n140\n2.5\n    Raisin Squares\nK\nC\n3\n1.00\n0.50\n55.33314\n90\n6\n2\n0\n0\n2.0\n    Rice Chex\nR\nC\n1\n1.00\n1.13\n41.99893\n110\n2\n1\n0\n240\n0.0\n    Rice Krispies\nK\nC\n1\n1.00\n1.00\n40.56016\n110\n3\n2\n0\n290\n0.0\n    Shredded Wheat\nN\nC\n1\n0.83\n1.00\n68.23588\n80\n0\n2\n0\n0\n3.0\n    Shredded Wheat 'n'Bran\nN\nC\n1\n1.00\n0.67\n74.47295\n90\n0\n3\n0\n0\n4.0\n    Shredded Wheat spoon size\nN\nC\n1\n1.00\n0.67\n72.80179\n90\n0\n3\n0\n0\n3.0\n    Smacks\nK\nC\n2\n1.00\n0.75\n31.23005\n110\n15\n2\n1\n70\n1.0\n    Special K\nK\nC\n1\n1.00\n1.00\n53.13132\n110\n3\n6\n0\n230\n1.0\n    Strawberry Fruit Wheats\nN\nC\n2\n1.00\n1.00\n59.36399\n90\n5\n2\n0\n15\n3.0\n    Total Corn Flakes\nG\nC\n3\n1.00\n1.00\n38.83975\n110\n3\n2\n1\n200\n0.0\n    Total Raisin Bran\nG\nC\n3\n1.50\n1.00\n28.59278\n140\n14\n3\n1\n190\n4.0\n    Total Whole Grain\nG\nC\n3\n1.00\n1.00\n46.65884\n100\n3\n3\n1\n200\n3.0\n    Triples\nG\nC\n3\n1.00\n0.75\n39.10617\n110\n3\n2\n1\n250\n0.0\n    Trix\nG\nC\n2\n1.00\n1.00\n27.75330\n110\n12\n1\n1\n140\n0.0\n    Wheat Chex\nR\nC\n1\n1.00\n0.67\n49.78744\n100\n3\n3\n1\n230\n3.0\n    Wheaties\nG\nC\n1\n1.00\n1.00\n51.59219\n100\n3\n3\n1\n200\n3.0\n    Wheaties Honey Gold\nG\nC\n1\n1.00\n0.75\n36.18756\n110\n8\n2\n1\n200\n1.0\n  \n  \n  \n\n\n\n\nAdding some missing values for later.\n\n\nCode\ncereal1 <- cereal %>%\n  #so sample iterates for each row\n  rowwise() %>% \n  mutate(\n    kids_rating = sample(c(\"A\", \"B\", \"C\", NA), 1, replace = TRUE),\n    adult_rating = sample(c(\n      \"A\", \"B\", \"C\", NA), 1, replace = TRUE, prob = c(0.1, 0.3, 0.5, 0.1))\n      )\n\n\n\nMissing Values\n\nacross() is used instead of _at(), _if() and _any()\nThe function inside the across() call is referred to as a selection helper, which make life easier once you are comfortable with them.\nYou can use everything() to pick all the columns.\n~sum() is similar notation to the purrr package.\n\n\n\nCode\ncereal1 %>%\n  summarise(across(everything(), ~sum(is.na(.)))) %>%\n  colSums() \n\n\n        name          mfr         type        shelf       weight         cups \n           0            0            0            0            0            0 \n      rating     calories       sugars      protein          fat       sodium \n           0            0            0            0            0            0 \n       fiber  kids_rating adult_rating \n           0           19            8 \n\n\nungroup() version of above changes the way it is displayed.\n\n\nCode\ncereal1 %>%\n  ungroup() %>%\n  summarise(across(everything(), ~sum(is.na(.))))\n\n\n# A tibble: 1 × 15\n   name   mfr  type shelf weight  cups rating calories sugars protein   fat\n  <int> <int> <int> <int>  <int> <int>  <int>    <int>  <int>   <int> <int>\n1     0     0     0     0      0     0      0        0      0       0     0\n# … with 4 more variables: sodium <int>, fiber <int>, kids_rating <int>,\n#   adult_rating <int>\n\n\nMissing values in the adult_rating column.\n\n\nCode\ncereal1 %>%\n  filter(if_any(adult_rating, is.na)) %>%\n  gt() %>% \n  tab_options(container.height = '500px') %>%\n  gt_theme_538() \n\n\n\n\n\n\n  \n  \n    \n      name\n      mfr\n      type\n      shelf\n      weight\n      cups\n      rating\n      calories\n      sugars\n      protein\n      fat\n      sodium\n      fiber\n      kids_rating\n      adult_rating\n    \n  \n  \n    100% Natural Bran\nQ\nC\n3\n1.00\n1.00\n33.98368\n120\n8\n3\n5\n15\n2\nNA\nNA\n    Crispy Wheat & Raisins\nG\nC\n3\n1.00\n0.75\n36.17620\n100\n10\n2\n1\n140\n2\nB\nNA\n    Just Right Fruit & Nut\nK\nC\n3\n1.30\n0.75\n36.47151\n140\n9\n3\n1\n170\n2\nB\nNA\n    Kix\nG\nC\n2\n1.00\n1.50\n39.24111\n110\n3\n2\n1\n260\n0\nB\nNA\n    Nutri-Grain Almond-Raisin\nK\nC\n3\n1.33\n0.67\n40.69232\n140\n7\n3\n2\n220\n3\nC\nNA\n    Rice Krispies\nK\nC\n1\n1.00\n1.00\n40.56016\n110\n3\n2\n0\n290\n0\nC\nNA\n    Shredded Wheat\nN\nC\n1\n0.83\n1.00\n68.23588\n80\n0\n2\n0\n0\n3\nB\nNA\n    Total Whole Grain\nG\nC\n3\n1.00\n1.00\n46.65884\n100\n3\n3\n1\n200\n3\nNA\nNA\n  \n  \n  \n\n\n\n\n\n\nSelection Helpers\n\nstarts_with()/ends_with()\ncontains()\nmatches()\neverything()\nall_of()\nany_of()\nwhere()\n\ncontains() example.\n\n\nCode\ncereal1 %>%\n  select(contains(\"_rating\")) \n\n\n# A tibble: 75 × 2\n# Rowwise: \n   kids_rating adult_rating\n   <chr>       <chr>       \n 1 B           B           \n 2 <NA>        <NA>        \n 3 B           C           \n 4 B           B           \n 5 A           B           \n 6 <NA>        B           \n 7 <NA>        B           \n 8 C           C           \n 9 B           B           \n10 A           C           \n# … with 65 more rows\n\n\n\n\nDistinct Column Values\nYou can select multiple columns by putting them in c() and then calling n_distinct\n\n\nCode\n(cereal3 <- cereal1 %>%\n  ungroup() %>%\n  summarise(across(c(mfr, type, kids_rating, adult_rating), n_distinct)))\n\n\n# A tibble: 1 × 4\n    mfr  type kids_rating adult_rating\n  <int> <int>       <int>        <int>\n1     6     2           4            4\n\n\nIf you just want to see the values for every column you can use everything()\n\n\nCode\ncereal3 %>%\n  summarise(across(everything(), n_distinct))\n\n\n# A tibble: 1 × 4\n    mfr  type kids_rating adult_rating\n  <int> <int>       <int>        <int>\n1     1     1           1            1\n\n\nLooking for column titles that have a certain string such as a prefix, word or letter you can use contains()\n\n\nCode\ncereal3 %>%\n  summarise(across(contains(\"e\"), n_distinct))\n\n\n# A tibble: 1 × 1\n   type\n  <int>\n1     1\n\n\nIf you only care about a particular data type, such as the numeric columns, you can use where() inside the across() call.\n\n\nCode\ncereal3 %>%\n  summarise(across(where(is.numeric), n_distinct))\n\n\n# A tibble: 1 × 4\n    mfr  type kids_rating adult_rating\n  <int> <int>       <int>        <int>\n1     1     1           1            1\n\n\n\n\nMutate\nTo create a new column, use mutate()\nThe case_when() function was used here to specify multiple conditions. .after and .before can be used to pick where you want the column located. The default is for the column to be on the far right.\n\n\nCode\ncereal %>%\n  mutate(\n    calorie_level = case_when(\n      calories < 90 ~ 'Low',\n      between(calories, 90, 110) ~ 'Moderate',\n      calories > 110 ~ 'High'), \n      .after = calories\n    ) %>%\n  gt() %>% \n    tab_options(container.height = '500px') %>%\n    gt_theme_538() \n\n\n\n\n\n\n  \n  \n    \n      name\n      mfr\n      type\n      shelf\n      weight\n      cups\n      rating\n      calories\n      calorie_level\n      sugars\n      protein\n      fat\n      sodium\n      fiber\n    \n  \n  \n    100% Bran\nN\nC\n3\n1.00\n0.33\n68.40297\n70\nLow\n6\n4\n1\n130\n10.0\n    100% Natural Bran\nQ\nC\n3\n1.00\n1.00\n33.98368\n120\nHigh\n8\n3\n5\n15\n2.0\n    All-Bran\nK\nC\n3\n1.00\n0.33\n59.42551\n70\nLow\n5\n4\n1\n260\n9.0\n    All-Bran with Extra Fiber\nK\nC\n3\n1.00\n0.50\n93.70491\n50\nLow\n0\n4\n0\n140\n14.0\n    Almond Delight\nR\nC\n3\n1.00\n0.75\n34.38484\n110\nModerate\n8\n2\n2\n200\n1.0\n    Apple Cinnamon Cheerios\nG\nC\n1\n1.00\n0.75\n29.50954\n110\nModerate\n10\n2\n2\n180\n1.5\n    Apple Jacks\nK\nC\n2\n1.00\n1.00\n33.17409\n110\nModerate\n14\n2\n0\n125\n1.0\n    Basic 4\nG\nC\n3\n1.33\n0.75\n37.03856\n130\nHigh\n8\n3\n2\n210\n2.0\n    Bran Chex\nR\nC\n1\n1.00\n0.67\n49.12025\n90\nModerate\n6\n2\n1\n200\n4.0\n    Bran Flakes\nP\nC\n3\n1.00\n0.67\n53.31381\n90\nModerate\n5\n3\n0\n210\n5.0\n    Cap'n'Crunch\nQ\nC\n2\n1.00\n0.75\n18.04285\n120\nHigh\n12\n1\n2\n220\n0.0\n    Cheerios\nG\nC\n1\n1.00\n1.25\n50.76500\n110\nModerate\n1\n6\n2\n290\n2.0\n    Cinnamon Toast Crunch\nG\nC\n2\n1.00\n0.75\n19.82357\n120\nHigh\n9\n1\n3\n210\n0.0\n    Clusters\nG\nC\n3\n1.00\n0.50\n40.40021\n110\nModerate\n7\n3\n2\n140\n2.0\n    Cocoa Puffs\nG\nC\n2\n1.00\n1.00\n22.73645\n110\nModerate\n13\n1\n1\n180\n0.0\n    Corn Chex\nR\nC\n1\n1.00\n1.00\n41.44502\n110\nModerate\n3\n2\n0\n280\n0.0\n    Corn Flakes\nK\nC\n1\n1.00\n1.00\n45.86332\n100\nModerate\n2\n2\n0\n290\n1.0\n    Corn Pops\nK\nC\n2\n1.00\n1.00\n35.78279\n110\nModerate\n12\n1\n0\n90\n1.0\n    Count Chocula\nG\nC\n2\n1.00\n1.00\n22.39651\n110\nModerate\n13\n1\n1\n180\n0.0\n    Cracklin' Oat Bran\nK\nC\n3\n1.00\n0.50\n40.44877\n110\nModerate\n7\n3\n3\n140\n4.0\n    Cream of Wheat (Quick)\nN\nH\n2\n1.00\n1.00\n64.53382\n100\nModerate\n0\n3\n0\n80\n1.0\n    Crispix\nK\nC\n3\n1.00\n1.00\n46.89564\n110\nModerate\n3\n2\n0\n220\n1.0\n    Crispy Wheat & Raisins\nG\nC\n3\n1.00\n0.75\n36.17620\n100\nModerate\n10\n2\n1\n140\n2.0\n    Double Chex\nR\nC\n3\n1.00\n0.75\n44.33086\n100\nModerate\n5\n2\n0\n190\n1.0\n    Froot Loops\nK\nC\n2\n1.00\n1.00\n32.20758\n110\nModerate\n13\n2\n1\n125\n1.0\n    Frosted Flakes\nK\nC\n1\n1.00\n0.75\n31.43597\n110\nModerate\n11\n1\n0\n200\n1.0\n    Frosted Mini-Wheats\nK\nC\n2\n1.00\n0.80\n58.34514\n100\nModerate\n7\n3\n0\n0\n3.0\n    Fruit & Fibre Dates; Walnuts; and Oats\nP\nC\n3\n1.25\n0.67\n40.91705\n120\nHigh\n10\n3\n2\n160\n5.0\n    Fruitful Bran\nK\nC\n3\n1.33\n0.67\n41.01549\n120\nHigh\n12\n3\n0\n240\n5.0\n    Fruity Pebbles\nP\nC\n2\n1.00\n0.75\n28.02576\n110\nModerate\n12\n1\n1\n135\n0.0\n    Golden Crisp\nP\nC\n1\n1.00\n0.88\n35.25244\n100\nModerate\n15\n2\n0\n45\n0.0\n    Golden Grahams\nG\nC\n2\n1.00\n0.75\n23.80404\n110\nModerate\n9\n1\n1\n280\n0.0\n    Grape Nuts Flakes\nP\nC\n3\n1.00\n0.88\n52.07690\n100\nModerate\n5\n3\n1\n140\n3.0\n    Grape-Nuts\nP\nC\n3\n1.00\n0.25\n53.37101\n110\nModerate\n3\n3\n0\n170\n3.0\n    Great Grains Pecan\nP\nC\n3\n1.00\n0.33\n45.81172\n120\nHigh\n4\n3\n3\n75\n3.0\n    Honey Graham Ohs\nQ\nC\n2\n1.00\n1.00\n21.87129\n120\nHigh\n11\n1\n2\n220\n1.0\n    Honey Nut Cheerios\nG\nC\n1\n1.00\n0.75\n31.07222\n110\nModerate\n10\n3\n1\n250\n1.5\n    Honey-comb\nP\nC\n1\n1.00\n1.33\n28.74241\n110\nModerate\n11\n1\n0\n180\n0.0\n    Just Right Crunchy  Nuggets\nK\nC\n3\n1.00\n1.00\n36.52368\n110\nModerate\n6\n2\n1\n170\n1.0\n    Just Right Fruit & Nut\nK\nC\n3\n1.30\n0.75\n36.47151\n140\nHigh\n9\n3\n1\n170\n2.0\n    Kix\nG\nC\n2\n1.00\n1.50\n39.24111\n110\nModerate\n3\n2\n1\n260\n0.0\n    Life\nQ\nC\n2\n1.00\n0.67\n45.32807\n100\nModerate\n6\n4\n2\n150\n2.0\n    Lucky Charms\nG\nC\n2\n1.00\n1.00\n26.73451\n110\nModerate\n12\n2\n1\n180\n0.0\n    Muesli Raisins; Dates; & Almonds\nR\nC\n3\n1.00\n1.00\n37.13686\n150\nHigh\n11\n4\n3\n95\n3.0\n    Muesli Raisins; Peaches; & Pecans\nR\nC\n3\n1.00\n1.00\n34.13976\n150\nHigh\n11\n4\n3\n150\n3.0\n    Mueslix Crispy Blend\nK\nC\n3\n1.50\n0.67\n30.31335\n160\nHigh\n13\n3\n2\n150\n3.0\n    Multi-Grain Cheerios\nG\nC\n1\n1.00\n1.00\n40.10596\n100\nModerate\n6\n2\n1\n220\n2.0\n    Nut&Honey Crunch\nK\nC\n2\n1.00\n0.67\n29.92429\n120\nHigh\n9\n2\n1\n190\n0.0\n    Nutri-Grain Almond-Raisin\nK\nC\n3\n1.33\n0.67\n40.69232\n140\nHigh\n7\n3\n2\n220\n3.0\n    Nutri-grain Wheat\nK\nC\n3\n1.00\n1.00\n59.64284\n90\nModerate\n2\n3\n0\n170\n3.0\n    Oatmeal Raisin Crisp\nG\nC\n3\n1.25\n0.50\n30.45084\n130\nHigh\n10\n3\n2\n170\n1.5\n    Post Nat. Raisin Bran\nP\nC\n3\n1.33\n0.67\n37.84059\n120\nHigh\n14\n3\n1\n200\n6.0\n    Product 19\nK\nC\n3\n1.00\n1.00\n41.50354\n100\nModerate\n3\n3\n0\n320\n1.0\n    Puffed Rice\nQ\nC\n3\n0.50\n1.00\n60.75611\n50\nLow\n0\n1\n0\n0\n0.0\n    Puffed Wheat\nQ\nC\n3\n0.50\n1.00\n63.00565\n50\nLow\n0\n2\n0\n0\n1.0\n    Quaker Oat Squares\nQ\nC\n3\n1.00\n0.50\n49.51187\n100\nModerate\n6\n4\n1\n135\n2.0\n    Raisin Bran\nK\nC\n2\n1.33\n0.75\n39.25920\n120\nHigh\n12\n3\n1\n210\n5.0\n    Raisin Nut Bran\nG\nC\n3\n1.00\n0.50\n39.70340\n100\nModerate\n8\n3\n2\n140\n2.5\n    Raisin Squares\nK\nC\n3\n1.00\n0.50\n55.33314\n90\nModerate\n6\n2\n0\n0\n2.0\n    Rice Chex\nR\nC\n1\n1.00\n1.13\n41.99893\n110\nModerate\n2\n1\n0\n240\n0.0\n    Rice Krispies\nK\nC\n1\n1.00\n1.00\n40.56016\n110\nModerate\n3\n2\n0\n290\n0.0\n    Shredded Wheat\nN\nC\n1\n0.83\n1.00\n68.23588\n80\nLow\n0\n2\n0\n0\n3.0\n    Shredded Wheat 'n'Bran\nN\nC\n1\n1.00\n0.67\n74.47295\n90\nModerate\n0\n3\n0\n0\n4.0\n    Shredded Wheat spoon size\nN\nC\n1\n1.00\n0.67\n72.80179\n90\nModerate\n0\n3\n0\n0\n3.0\n    Smacks\nK\nC\n2\n1.00\n0.75\n31.23005\n110\nModerate\n15\n2\n1\n70\n1.0\n    Special K\nK\nC\n1\n1.00\n1.00\n53.13132\n110\nModerate\n3\n6\n0\n230\n1.0\n    Strawberry Fruit Wheats\nN\nC\n2\n1.00\n1.00\n59.36399\n90\nModerate\n5\n2\n0\n15\n3.0\n    Total Corn Flakes\nG\nC\n3\n1.00\n1.00\n38.83975\n110\nModerate\n3\n2\n1\n200\n0.0\n    Total Raisin Bran\nG\nC\n3\n1.50\n1.00\n28.59278\n140\nHigh\n14\n3\n1\n190\n4.0\n    Total Whole Grain\nG\nC\n3\n1.00\n1.00\n46.65884\n100\nModerate\n3\n3\n1\n200\n3.0\n    Triples\nG\nC\n3\n1.00\n0.75\n39.10617\n110\nModerate\n3\n2\n1\n250\n0.0\n    Trix\nG\nC\n2\n1.00\n1.00\n27.75330\n110\nModerate\n12\n1\n1\n140\n0.0\n    Wheat Chex\nR\nC\n1\n1.00\n0.67\n49.78744\n100\nModerate\n3\n3\n1\n230\n3.0\n    Wheaties\nG\nC\n1\n1.00\n1.00\n51.59219\n100\nModerate\n3\n3\n1\n200\n3.0\n    Wheaties Honey Gold\nG\nC\n1\n1.00\n0.75\n36.18756\n110\nModerate\n8\n2\n1\n200\n1.0\n  \n  \n  \n\n\n\n\nWithin one mutate() call you can create multiple columns with a comma.\n\n\nCode\n#reproducibility \nset.seed(2017) \ncereal2 <- cereal %>%\n  #so sample iterates for each row\n  rowwise() %>% \n  mutate(\n    kids_rating = sample(c(\"A\", \"B\", \"C\", NA), 1, replace = TRUE),\n    adult_rating = sample(c(\n      \"A\", \"B\", \"C\", NA), 1, replace = TRUE, prob = c(0.1, 0.3, 0.5, 0.1))\n      )\n\ncereal2 %>%\n  gt() %>% \n  tab_options(container.height = '500px') %>%\n  gt_theme_538() \n\n\n\n\n\n\n  \n  \n    \n      name\n      mfr\n      type\n      shelf\n      weight\n      cups\n      rating\n      calories\n      sugars\n      protein\n      fat\n      sodium\n      fiber\n      kids_rating\n      adult_rating\n    \n  \n  \n    100% Bran\nN\nC\n3\n1.00\n0.33\n68.40297\n70\n6\n4\n1\n130\n10.0\nNA\nC\n    100% Natural Bran\nQ\nC\n3\n1.00\n1.00\n33.98368\n120\n8\n3\n5\n15\n2.0\nA\nC\n    All-Bran\nK\nC\n3\n1.00\n0.33\n59.42551\n70\n5\n4\n1\n260\n9.0\nB\nB\n    All-Bran with Extra Fiber\nK\nC\n3\n1.00\n0.50\n93.70491\n50\n0\n4\n0\n140\n14.0\nNA\nC\n    Almond Delight\nR\nC\n3\n1.00\n0.75\n34.38484\n110\n8\n2\n2\n200\n1.0\nA\nA\n    Apple Cinnamon Cheerios\nG\nC\n1\n1.00\n0.75\n29.50954\n110\n10\n2\n2\n180\n1.5\nA\nC\n    Apple Jacks\nK\nC\n2\n1.00\n1.00\n33.17409\n110\n14\n2\n0\n125\n1.0\nB\nNA\n    Basic 4\nG\nC\n3\n1.33\n0.75\n37.03856\n130\n8\n3\n2\n210\n2.0\nB\nB\n    Bran Chex\nR\nC\n1\n1.00\n0.67\n49.12025\n90\n6\n2\n1\n200\n4.0\nNA\nA\n    Bran Flakes\nP\nC\n3\n1.00\n0.67\n53.31381\n90\n5\n3\n0\n210\n5.0\nB\nC\n    Cap'n'Crunch\nQ\nC\n2\n1.00\n0.75\n18.04285\n120\n12\n1\n2\n220\n0.0\nA\nC\n    Cheerios\nG\nC\n1\n1.00\n1.25\n50.76500\n110\n1\n6\n2\n290\n2.0\nA\nC\n    Cinnamon Toast Crunch\nG\nC\n2\n1.00\n0.75\n19.82357\n120\n9\n1\n3\n210\n0.0\nA\nC\n    Clusters\nG\nC\n3\n1.00\n0.50\n40.40021\n110\n7\n3\n2\n140\n2.0\nA\nC\n    Cocoa Puffs\nG\nC\n2\n1.00\n1.00\n22.73645\n110\n13\n1\n1\n180\n0.0\nA\nC\n    Corn Chex\nR\nC\n1\n1.00\n1.00\n41.44502\n110\n3\n2\n0\n280\n0.0\nA\nNA\n    Corn Flakes\nK\nC\n1\n1.00\n1.00\n45.86332\n100\n2\n2\n0\n290\n1.0\nNA\nB\n    Corn Pops\nK\nC\n2\n1.00\n1.00\n35.78279\n110\n12\n1\n0\n90\n1.0\nA\nB\n    Count Chocula\nG\nC\n2\n1.00\n1.00\n22.39651\n110\n13\n1\n1\n180\n0.0\nB\nB\n    Cracklin' Oat Bran\nK\nC\n3\n1.00\n0.50\n40.44877\n110\n7\n3\n3\n140\n4.0\nB\nNA\n    Cream of Wheat (Quick)\nN\nH\n2\n1.00\n1.00\n64.53382\n100\n0\n3\n0\n80\n1.0\nB\nC\n    Crispix\nK\nC\n3\n1.00\n1.00\n46.89564\n110\n3\n2\n0\n220\n1.0\nC\nNA\n    Crispy Wheat & Raisins\nG\nC\n3\n1.00\n0.75\n36.17620\n100\n10\n2\n1\n140\n2.0\nNA\nC\n    Double Chex\nR\nC\n3\n1.00\n0.75\n44.33086\n100\n5\n2\n0\n190\n1.0\nNA\nNA\n    Froot Loops\nK\nC\n2\n1.00\n1.00\n32.20758\n110\n13\n2\n1\n125\n1.0\nA\nC\n    Frosted Flakes\nK\nC\n1\n1.00\n0.75\n31.43597\n110\n11\n1\n0\n200\n1.0\nNA\nA\n    Frosted Mini-Wheats\nK\nC\n2\n1.00\n0.80\n58.34514\n100\n7\n3\n0\n0\n3.0\nB\nA\n    Fruit & Fibre Dates; Walnuts; and Oats\nP\nC\n3\n1.25\n0.67\n40.91705\n120\n10\n3\n2\n160\n5.0\nA\nC\n    Fruitful Bran\nK\nC\n3\n1.33\n0.67\n41.01549\n120\n12\n3\n0\n240\n5.0\nNA\nC\n    Fruity Pebbles\nP\nC\n2\n1.00\n0.75\n28.02576\n110\n12\n1\n1\n135\n0.0\nC\nB\n    Golden Crisp\nP\nC\n1\n1.00\n0.88\n35.25244\n100\n15\n2\n0\n45\n0.0\nNA\nC\n    Golden Grahams\nG\nC\n2\n1.00\n0.75\n23.80404\n110\n9\n1\n1\n280\n0.0\nA\nC\n    Grape Nuts Flakes\nP\nC\n3\n1.00\n0.88\n52.07690\n100\n5\n3\n1\n140\n3.0\nNA\nB\n    Grape-Nuts\nP\nC\n3\n1.00\n0.25\n53.37101\n110\n3\n3\n0\n170\n3.0\nC\nC\n    Great Grains Pecan\nP\nC\n3\n1.00\n0.33\n45.81172\n120\n4\n3\n3\n75\n3.0\nC\nC\n    Honey Graham Ohs\nQ\nC\n2\n1.00\n1.00\n21.87129\n120\n11\n1\n2\n220\n1.0\nC\nC\n    Honey Nut Cheerios\nG\nC\n1\n1.00\n0.75\n31.07222\n110\n10\n3\n1\n250\n1.5\nA\nC\n    Honey-comb\nP\nC\n1\n1.00\n1.33\n28.74241\n110\n11\n1\n0\n180\n0.0\nNA\nA\n    Just Right Crunchy  Nuggets\nK\nC\n3\n1.00\n1.00\n36.52368\n110\n6\n2\n1\n170\n1.0\nA\nC\n    Just Right Fruit & Nut\nK\nC\n3\n1.30\n0.75\n36.47151\n140\n9\n3\n1\n170\n2.0\nC\nA\n    Kix\nG\nC\n2\n1.00\n1.50\n39.24111\n110\n3\n2\n1\n260\n0.0\nC\nB\n    Life\nQ\nC\n2\n1.00\n0.67\n45.32807\n100\n6\n4\n2\n150\n2.0\nNA\nC\n    Lucky Charms\nG\nC\n2\n1.00\n1.00\n26.73451\n110\n12\n2\n1\n180\n0.0\nB\nC\n    Muesli Raisins; Dates; & Almonds\nR\nC\n3\n1.00\n1.00\n37.13686\n150\n11\n4\n3\n95\n3.0\nA\nC\n    Muesli Raisins; Peaches; & Pecans\nR\nC\n3\n1.00\n1.00\n34.13976\n150\n11\n4\n3\n150\n3.0\nA\nC\n    Mueslix Crispy Blend\nK\nC\n3\n1.50\n0.67\n30.31335\n160\n13\n3\n2\n150\n3.0\nNA\nB\n    Multi-Grain Cheerios\nG\nC\n1\n1.00\n1.00\n40.10596\n100\n6\n2\n1\n220\n2.0\nC\nC\n    Nut&Honey Crunch\nK\nC\n2\n1.00\n0.67\n29.92429\n120\n9\n2\n1\n190\n0.0\nA\nB\n    Nutri-Grain Almond-Raisin\nK\nC\n3\n1.33\n0.67\n40.69232\n140\n7\n3\n2\n220\n3.0\nB\nC\n    Nutri-grain Wheat\nK\nC\n3\n1.00\n1.00\n59.64284\n90\n2\n3\n0\n170\n3.0\nA\nC\n    Oatmeal Raisin Crisp\nG\nC\n3\n1.25\n0.50\n30.45084\n130\n10\n3\n2\n170\n1.5\nNA\nC\n    Post Nat. Raisin Bran\nP\nC\n3\n1.33\n0.67\n37.84059\n120\n14\n3\n1\n200\n6.0\nNA\nB\n    Product 19\nK\nC\n3\n1.00\n1.00\n41.50354\n100\n3\n3\n0\n320\n1.0\nNA\nC\n    Puffed Rice\nQ\nC\n3\n0.50\n1.00\n60.75611\n50\n0\n1\n0\n0\n0.0\nA\nNA\n    Puffed Wheat\nQ\nC\n3\n0.50\n1.00\n63.00565\n50\n0\n2\n0\n0\n1.0\nC\nB\n    Quaker Oat Squares\nQ\nC\n3\n1.00\n0.50\n49.51187\n100\n6\n4\n1\n135\n2.0\nC\nB\n    Raisin Bran\nK\nC\n2\n1.33\n0.75\n39.25920\n120\n12\n3\n1\n210\n5.0\nA\nC\n    Raisin Nut Bran\nG\nC\n3\n1.00\n0.50\n39.70340\n100\n8\n3\n2\n140\n2.5\nA\nC\n    Raisin Squares\nK\nC\n3\n1.00\n0.50\n55.33314\n90\n6\n2\n0\n0\n2.0\nB\nC\n    Rice Chex\nR\nC\n1\n1.00\n1.13\n41.99893\n110\n2\n1\n0\n240\n0.0\nC\nB\n    Rice Krispies\nK\nC\n1\n1.00\n1.00\n40.56016\n110\n3\n2\n0\n290\n0.0\nNA\nC\n    Shredded Wheat\nN\nC\n1\n0.83\n1.00\n68.23588\n80\n0\n2\n0\n0\n3.0\nC\nB\n    Shredded Wheat 'n'Bran\nN\nC\n1\n1.00\n0.67\n74.47295\n90\n0\n3\n0\n0\n4.0\nC\nC\n    Shredded Wheat spoon size\nN\nC\n1\n1.00\n0.67\n72.80179\n90\n0\n3\n0\n0\n3.0\nB\nC\n    Smacks\nK\nC\n2\n1.00\n0.75\n31.23005\n110\n15\n2\n1\n70\n1.0\nNA\nC\n    Special K\nK\nC\n1\n1.00\n1.00\n53.13132\n110\n3\n6\n0\n230\n1.0\nB\nNA\n    Strawberry Fruit Wheats\nN\nC\n2\n1.00\n1.00\n59.36399\n90\n5\n2\n0\n15\n3.0\nNA\nNA\n    Total Corn Flakes\nG\nC\n3\n1.00\n1.00\n38.83975\n110\n3\n2\n1\n200\n0.0\nB\nC\n    Total Raisin Bran\nG\nC\n3\n1.50\n1.00\n28.59278\n140\n14\n3\n1\n190\n4.0\nNA\nC\n    Total Whole Grain\nG\nC\n3\n1.00\n1.00\n46.65884\n100\n3\n3\n1\n200\n3.0\nA\nC\n    Triples\nG\nC\n3\n1.00\n0.75\n39.10617\n110\n3\n2\n1\n250\n0.0\nA\nC\n    Trix\nG\nC\n2\n1.00\n1.00\n27.75330\n110\n12\n1\n1\n140\n0.0\nC\nC\n    Wheat Chex\nR\nC\n1\n1.00\n0.67\n49.78744\n100\n3\n3\n1\n230\n3.0\nNA\nA\n    Wheaties\nG\nC\n1\n1.00\n1.00\n51.59219\n100\n3\n3\n1\n200\n3.0\nNA\nA\n    Wheaties Honey Gold\nG\nC\n1\n1.00\n0.75\n36.18756\n110\n8\n2\n1\n200\n1.0\nC\nB\n  \n  \n  \n\n\n\n\nTo only show the rows without a condition such as missing values we use filter() with ! in front of the value, e.g. is.na()\n\n\nCode\ncereal2 %>%\n  filter(!is.na(adult_rating)) %>%\n  gt() %>% \n  tab_options(container.height = '500px') %>%\n  gt_theme_538() \n\n\n\n\n\n\n  \n  \n    \n      name\n      mfr\n      type\n      shelf\n      weight\n      cups\n      rating\n      calories\n      sugars\n      protein\n      fat\n      sodium\n      fiber\n      kids_rating\n      adult_rating\n    \n  \n  \n    100% Bran\nN\nC\n3\n1.00\n0.33\n68.40297\n70\n6\n4\n1\n130\n10.0\nNA\nC\n    100% Natural Bran\nQ\nC\n3\n1.00\n1.00\n33.98368\n120\n8\n3\n5\n15\n2.0\nA\nC\n    All-Bran\nK\nC\n3\n1.00\n0.33\n59.42551\n70\n5\n4\n1\n260\n9.0\nB\nB\n    All-Bran with Extra Fiber\nK\nC\n3\n1.00\n0.50\n93.70491\n50\n0\n4\n0\n140\n14.0\nNA\nC\n    Almond Delight\nR\nC\n3\n1.00\n0.75\n34.38484\n110\n8\n2\n2\n200\n1.0\nA\nA\n    Apple Cinnamon Cheerios\nG\nC\n1\n1.00\n0.75\n29.50954\n110\n10\n2\n2\n180\n1.5\nA\nC\n    Basic 4\nG\nC\n3\n1.33\n0.75\n37.03856\n130\n8\n3\n2\n210\n2.0\nB\nB\n    Bran Chex\nR\nC\n1\n1.00\n0.67\n49.12025\n90\n6\n2\n1\n200\n4.0\nNA\nA\n    Bran Flakes\nP\nC\n3\n1.00\n0.67\n53.31381\n90\n5\n3\n0\n210\n5.0\nB\nC\n    Cap'n'Crunch\nQ\nC\n2\n1.00\n0.75\n18.04285\n120\n12\n1\n2\n220\n0.0\nA\nC\n    Cheerios\nG\nC\n1\n1.00\n1.25\n50.76500\n110\n1\n6\n2\n290\n2.0\nA\nC\n    Cinnamon Toast Crunch\nG\nC\n2\n1.00\n0.75\n19.82357\n120\n9\n1\n3\n210\n0.0\nA\nC\n    Clusters\nG\nC\n3\n1.00\n0.50\n40.40021\n110\n7\n3\n2\n140\n2.0\nA\nC\n    Cocoa Puffs\nG\nC\n2\n1.00\n1.00\n22.73645\n110\n13\n1\n1\n180\n0.0\nA\nC\n    Corn Flakes\nK\nC\n1\n1.00\n1.00\n45.86332\n100\n2\n2\n0\n290\n1.0\nNA\nB\n    Corn Pops\nK\nC\n2\n1.00\n1.00\n35.78279\n110\n12\n1\n0\n90\n1.0\nA\nB\n    Count Chocula\nG\nC\n2\n1.00\n1.00\n22.39651\n110\n13\n1\n1\n180\n0.0\nB\nB\n    Cream of Wheat (Quick)\nN\nH\n2\n1.00\n1.00\n64.53382\n100\n0\n3\n0\n80\n1.0\nB\nC\n    Crispy Wheat & Raisins\nG\nC\n3\n1.00\n0.75\n36.17620\n100\n10\n2\n1\n140\n2.0\nNA\nC\n    Froot Loops\nK\nC\n2\n1.00\n1.00\n32.20758\n110\n13\n2\n1\n125\n1.0\nA\nC\n    Frosted Flakes\nK\nC\n1\n1.00\n0.75\n31.43597\n110\n11\n1\n0\n200\n1.0\nNA\nA\n    Frosted Mini-Wheats\nK\nC\n2\n1.00\n0.80\n58.34514\n100\n7\n3\n0\n0\n3.0\nB\nA\n    Fruit & Fibre Dates; Walnuts; and Oats\nP\nC\n3\n1.25\n0.67\n40.91705\n120\n10\n3\n2\n160\n5.0\nA\nC\n    Fruitful Bran\nK\nC\n3\n1.33\n0.67\n41.01549\n120\n12\n3\n0\n240\n5.0\nNA\nC\n    Fruity Pebbles\nP\nC\n2\n1.00\n0.75\n28.02576\n110\n12\n1\n1\n135\n0.0\nC\nB\n    Golden Crisp\nP\nC\n1\n1.00\n0.88\n35.25244\n100\n15\n2\n0\n45\n0.0\nNA\nC\n    Golden Grahams\nG\nC\n2\n1.00\n0.75\n23.80404\n110\n9\n1\n1\n280\n0.0\nA\nC\n    Grape Nuts Flakes\nP\nC\n3\n1.00\n0.88\n52.07690\n100\n5\n3\n1\n140\n3.0\nNA\nB\n    Grape-Nuts\nP\nC\n3\n1.00\n0.25\n53.37101\n110\n3\n3\n0\n170\n3.0\nC\nC\n    Great Grains Pecan\nP\nC\n3\n1.00\n0.33\n45.81172\n120\n4\n3\n3\n75\n3.0\nC\nC\n    Honey Graham Ohs\nQ\nC\n2\n1.00\n1.00\n21.87129\n120\n11\n1\n2\n220\n1.0\nC\nC\n    Honey Nut Cheerios\nG\nC\n1\n1.00\n0.75\n31.07222\n110\n10\n3\n1\n250\n1.5\nA\nC\n    Honey-comb\nP\nC\n1\n1.00\n1.33\n28.74241\n110\n11\n1\n0\n180\n0.0\nNA\nA\n    Just Right Crunchy  Nuggets\nK\nC\n3\n1.00\n1.00\n36.52368\n110\n6\n2\n1\n170\n1.0\nA\nC\n    Just Right Fruit & Nut\nK\nC\n3\n1.30\n0.75\n36.47151\n140\n9\n3\n1\n170\n2.0\nC\nA\n    Kix\nG\nC\n2\n1.00\n1.50\n39.24111\n110\n3\n2\n1\n260\n0.0\nC\nB\n    Life\nQ\nC\n2\n1.00\n0.67\n45.32807\n100\n6\n4\n2\n150\n2.0\nNA\nC\n    Lucky Charms\nG\nC\n2\n1.00\n1.00\n26.73451\n110\n12\n2\n1\n180\n0.0\nB\nC\n    Muesli Raisins; Dates; & Almonds\nR\nC\n3\n1.00\n1.00\n37.13686\n150\n11\n4\n3\n95\n3.0\nA\nC\n    Muesli Raisins; Peaches; & Pecans\nR\nC\n3\n1.00\n1.00\n34.13976\n150\n11\n4\n3\n150\n3.0\nA\nC\n    Mueslix Crispy Blend\nK\nC\n3\n1.50\n0.67\n30.31335\n160\n13\n3\n2\n150\n3.0\nNA\nB\n    Multi-Grain Cheerios\nG\nC\n1\n1.00\n1.00\n40.10596\n100\n6\n2\n1\n220\n2.0\nC\nC\n    Nut&Honey Crunch\nK\nC\n2\n1.00\n0.67\n29.92429\n120\n9\n2\n1\n190\n0.0\nA\nB\n    Nutri-Grain Almond-Raisin\nK\nC\n3\n1.33\n0.67\n40.69232\n140\n7\n3\n2\n220\n3.0\nB\nC\n    Nutri-grain Wheat\nK\nC\n3\n1.00\n1.00\n59.64284\n90\n2\n3\n0\n170\n3.0\nA\nC\n    Oatmeal Raisin Crisp\nG\nC\n3\n1.25\n0.50\n30.45084\n130\n10\n3\n2\n170\n1.5\nNA\nC\n    Post Nat. Raisin Bran\nP\nC\n3\n1.33\n0.67\n37.84059\n120\n14\n3\n1\n200\n6.0\nNA\nB\n    Product 19\nK\nC\n3\n1.00\n1.00\n41.50354\n100\n3\n3\n0\n320\n1.0\nNA\nC\n    Puffed Wheat\nQ\nC\n3\n0.50\n1.00\n63.00565\n50\n0\n2\n0\n0\n1.0\nC\nB\n    Quaker Oat Squares\nQ\nC\n3\n1.00\n0.50\n49.51187\n100\n6\n4\n1\n135\n2.0\nC\nB\n    Raisin Bran\nK\nC\n2\n1.33\n0.75\n39.25920\n120\n12\n3\n1\n210\n5.0\nA\nC\n    Raisin Nut Bran\nG\nC\n3\n1.00\n0.50\n39.70340\n100\n8\n3\n2\n140\n2.5\nA\nC\n    Raisin Squares\nK\nC\n3\n1.00\n0.50\n55.33314\n90\n6\n2\n0\n0\n2.0\nB\nC\n    Rice Chex\nR\nC\n1\n1.00\n1.13\n41.99893\n110\n2\n1\n0\n240\n0.0\nC\nB\n    Rice Krispies\nK\nC\n1\n1.00\n1.00\n40.56016\n110\n3\n2\n0\n290\n0.0\nNA\nC\n    Shredded Wheat\nN\nC\n1\n0.83\n1.00\n68.23588\n80\n0\n2\n0\n0\n3.0\nC\nB\n    Shredded Wheat 'n'Bran\nN\nC\n1\n1.00\n0.67\n74.47295\n90\n0\n3\n0\n0\n4.0\nC\nC\n    Shredded Wheat spoon size\nN\nC\n1\n1.00\n0.67\n72.80179\n90\n0\n3\n0\n0\n3.0\nB\nC\n    Smacks\nK\nC\n2\n1.00\n0.75\n31.23005\n110\n15\n2\n1\n70\n1.0\nNA\nC\n    Total Corn Flakes\nG\nC\n3\n1.00\n1.00\n38.83975\n110\n3\n2\n1\n200\n0.0\nB\nC\n    Total Raisin Bran\nG\nC\n3\n1.50\n1.00\n28.59278\n140\n14\n3\n1\n190\n4.0\nNA\nC\n    Total Whole Grain\nG\nC\n3\n1.00\n1.00\n46.65884\n100\n3\n3\n1\n200\n3.0\nA\nC\n    Triples\nG\nC\n3\n1.00\n0.75\n39.10617\n110\n3\n2\n1\n250\n0.0\nA\nC\n    Trix\nG\nC\n2\n1.00\n1.00\n27.75330\n110\n12\n1\n1\n140\n0.0\nC\nC\n    Wheat Chex\nR\nC\n1\n1.00\n0.67\n49.78744\n100\n3\n3\n1\n230\n3.0\nNA\nA\n    Wheaties\nG\nC\n1\n1.00\n1.00\n51.59219\n100\n3\n3\n1\n200\n3.0\nNA\nA\n    Wheaties Honey Gold\nG\nC\n1\n1.00\n0.75\n36.18756\n110\n8\n2\n1\n200\n1.0\nC\nB\n  \n  \n  \n\n\n\n\n\n\nNegative Values\n\n\nCode\ncereal2 %>%\n  rowwise() %>%\n  mutate(random_values = runif(1, min = -1, max = 1), .after = name) %>%\n  gt() %>% \n    tab_options(container.height = '500px') %>%\n    gt_theme_538() \n\n\n\n\n\n\n  \n  \n    \n      name\n      random_values\n      mfr\n      type\n      shelf\n      weight\n      cups\n      rating\n      calories\n      sugars\n      protein\n      fat\n      sodium\n      fiber\n      kids_rating\n      adult_rating\n    \n  \n  \n    100% Bran\n0.66611132\nN\nC\n3\n1.00\n0.33\n68.40297\n70\n6\n4\n1\n130\n10.0\nNA\nC\n    100% Natural Bran\n0.05625701\nQ\nC\n3\n1.00\n1.00\n33.98368\n120\n8\n3\n5\n15\n2.0\nA\nC\n    All-Bran\n0.51016114\nK\nC\n3\n1.00\n0.33\n59.42551\n70\n5\n4\n1\n260\n9.0\nB\nB\n    All-Bran with Extra Fiber\n-0.57461179\nK\nC\n3\n1.00\n0.50\n93.70491\n50\n0\n4\n0\n140\n14.0\nNA\nC\n    Almond Delight\n-0.26580914\nR\nC\n3\n1.00\n0.75\n34.38484\n110\n8\n2\n2\n200\n1.0\nA\nA\n    Apple Cinnamon Cheerios\n-0.61978115\nG\nC\n1\n1.00\n0.75\n29.50954\n110\n10\n2\n2\n180\n1.5\nA\nC\n    Apple Jacks\n-0.26443372\nK\nC\n2\n1.00\n1.00\n33.17409\n110\n14\n2\n0\n125\n1.0\nB\nNA\n    Basic 4\n0.72321201\nG\nC\n3\n1.33\n0.75\n37.03856\n130\n8\n3\n2\n210\n2.0\nB\nB\n    Bran Chex\n-0.25219902\nR\nC\n1\n1.00\n0.67\n49.12025\n90\n6\n2\n1\n200\n4.0\nNA\nA\n    Bran Flakes\n-0.58244538\nP\nC\n3\n1.00\n0.67\n53.31381\n90\n5\n3\n0\n210\n5.0\nB\nC\n    Cap'n'Crunch\n0.22039021\nQ\nC\n2\n1.00\n0.75\n18.04285\n120\n12\n1\n2\n220\n0.0\nA\nC\n    Cheerios\n-0.62316946\nG\nC\n1\n1.00\n1.25\n50.76500\n110\n1\n6\n2\n290\n2.0\nA\nC\n    Cinnamon Toast Crunch\n-0.95702376\nG\nC\n2\n1.00\n0.75\n19.82357\n120\n9\n1\n3\n210\n0.0\nA\nC\n    Clusters\n0.42103097\nG\nC\n3\n1.00\n0.50\n40.40021\n110\n7\n3\n2\n140\n2.0\nA\nC\n    Cocoa Puffs\n-0.67160492\nG\nC\n2\n1.00\n1.00\n22.73645\n110\n13\n1\n1\n180\n0.0\nA\nC\n    Corn Chex\n0.65231184\nR\nC\n1\n1.00\n1.00\n41.44502\n110\n3\n2\n0\n280\n0.0\nA\nNA\n    Corn Flakes\n-0.66665057\nK\nC\n1\n1.00\n1.00\n45.86332\n100\n2\n2\n0\n290\n1.0\nNA\nB\n    Corn Pops\n-0.16576370\nK\nC\n2\n1.00\n1.00\n35.78279\n110\n12\n1\n0\n90\n1.0\nA\nB\n    Count Chocula\n0.33479717\nG\nC\n2\n1.00\n1.00\n22.39651\n110\n13\n1\n1\n180\n0.0\nB\nB\n    Cracklin' Oat Bran\n0.02544158\nK\nC\n3\n1.00\n0.50\n40.44877\n110\n7\n3\n3\n140\n4.0\nB\nNA\n    Cream of Wheat (Quick)\n-0.26095974\nN\nH\n2\n1.00\n1.00\n64.53382\n100\n0\n3\n0\n80\n1.0\nB\nC\n    Crispix\n-0.28506238\nK\nC\n3\n1.00\n1.00\n46.89564\n110\n3\n2\n0\n220\n1.0\nC\nNA\n    Crispy Wheat & Raisins\n-0.81982770\nG\nC\n3\n1.00\n0.75\n36.17620\n100\n10\n2\n1\n140\n2.0\nNA\nC\n    Double Chex\n-0.68802564\nR\nC\n3\n1.00\n0.75\n44.33086\n100\n5\n2\n0\n190\n1.0\nNA\nNA\n    Froot Loops\n-0.60998161\nK\nC\n2\n1.00\n1.00\n32.20758\n110\n13\n2\n1\n125\n1.0\nA\nC\n    Frosted Flakes\n0.56613850\nK\nC\n1\n1.00\n0.75\n31.43597\n110\n11\n1\n0\n200\n1.0\nNA\nA\n    Frosted Mini-Wheats\n-0.62523864\nK\nC\n2\n1.00\n0.80\n58.34514\n100\n7\n3\n0\n0\n3.0\nB\nA\n    Fruit & Fibre Dates; Walnuts; and Oats\n-0.76541462\nP\nC\n3\n1.25\n0.67\n40.91705\n120\n10\n3\n2\n160\n5.0\nA\nC\n    Fruitful Bran\n0.01392629\nK\nC\n3\n1.33\n0.67\n41.01549\n120\n12\n3\n0\n240\n5.0\nNA\nC\n    Fruity Pebbles\n0.35404454\nP\nC\n2\n1.00\n0.75\n28.02576\n110\n12\n1\n1\n135\n0.0\nC\nB\n    Golden Crisp\n0.83145270\nP\nC\n1\n1.00\n0.88\n35.25244\n100\n15\n2\n0\n45\n0.0\nNA\nC\n    Golden Grahams\n0.67676455\nG\nC\n2\n1.00\n0.75\n23.80404\n110\n9\n1\n1\n280\n0.0\nA\nC\n    Grape Nuts Flakes\n-0.05476789\nP\nC\n3\n1.00\n0.88\n52.07690\n100\n5\n3\n1\n140\n3.0\nNA\nB\n    Grape-Nuts\n0.37816437\nP\nC\n3\n1.00\n0.25\n53.37101\n110\n3\n3\n0\n170\n3.0\nC\nC\n    Great Grains Pecan\n0.60016109\nP\nC\n3\n1.00\n0.33\n45.81172\n120\n4\n3\n3\n75\n3.0\nC\nC\n    Honey Graham Ohs\n-0.28393885\nQ\nC\n2\n1.00\n1.00\n21.87129\n120\n11\n1\n2\n220\n1.0\nC\nC\n    Honey Nut Cheerios\n-0.66607618\nG\nC\n1\n1.00\n0.75\n31.07222\n110\n10\n3\n1\n250\n1.5\nA\nC\n    Honey-comb\n-0.52276435\nP\nC\n1\n1.00\n1.33\n28.74241\n110\n11\n1\n0\n180\n0.0\nNA\nA\n    Just Right Crunchy  Nuggets\n-0.95112902\nK\nC\n3\n1.00\n1.00\n36.52368\n110\n6\n2\n1\n170\n1.0\nA\nC\n    Just Right Fruit & Nut\n-0.87211275\nK\nC\n3\n1.30\n0.75\n36.47151\n140\n9\n3\n1\n170\n2.0\nC\nA\n    Kix\n0.40027282\nG\nC\n2\n1.00\n1.50\n39.24111\n110\n3\n2\n1\n260\n0.0\nC\nB\n    Life\n-0.96275220\nQ\nC\n2\n1.00\n0.67\n45.32807\n100\n6\n4\n2\n150\n2.0\nNA\nC\n    Lucky Charms\n0.20587191\nG\nC\n2\n1.00\n1.00\n26.73451\n110\n12\n2\n1\n180\n0.0\nB\nC\n    Muesli Raisins; Dates; & Almonds\n0.36968891\nR\nC\n3\n1.00\n1.00\n37.13686\n150\n11\n4\n3\n95\n3.0\nA\nC\n    Muesli Raisins; Peaches; & Pecans\n0.68096552\nR\nC\n3\n1.00\n1.00\n34.13976\n150\n11\n4\n3\n150\n3.0\nA\nC\n    Mueslix Crispy Blend\n-0.51797488\nK\nC\n3\n1.50\n0.67\n30.31335\n160\n13\n3\n2\n150\n3.0\nNA\nB\n    Multi-Grain Cheerios\n-0.50014443\nG\nC\n1\n1.00\n1.00\n40.10596\n100\n6\n2\n1\n220\n2.0\nC\nC\n    Nut&Honey Crunch\n0.58813558\nK\nC\n2\n1.00\n0.67\n29.92429\n120\n9\n2\n1\n190\n0.0\nA\nB\n    Nutri-Grain Almond-Raisin\n-0.54095371\nK\nC\n3\n1.33\n0.67\n40.69232\n140\n7\n3\n2\n220\n3.0\nB\nC\n    Nutri-grain Wheat\n-0.70133721\nK\nC\n3\n1.00\n1.00\n59.64284\n90\n2\n3\n0\n170\n3.0\nA\nC\n    Oatmeal Raisin Crisp\n-0.23848839\nG\nC\n3\n1.25\n0.50\n30.45084\n130\n10\n3\n2\n170\n1.5\nNA\nC\n    Post Nat. Raisin Bran\n0.96898075\nP\nC\n3\n1.33\n0.67\n37.84059\n120\n14\n3\n1\n200\n6.0\nNA\nB\n    Product 19\n0.28978417\nK\nC\n3\n1.00\n1.00\n41.50354\n100\n3\n3\n0\n320\n1.0\nNA\nC\n    Puffed Rice\n0.89446662\nQ\nC\n3\n0.50\n1.00\n60.75611\n50\n0\n1\n0\n0\n0.0\nA\nNA\n    Puffed Wheat\n-0.45287473\nQ\nC\n3\n0.50\n1.00\n63.00565\n50\n0\n2\n0\n0\n1.0\nC\nB\n    Quaker Oat Squares\n0.41656567\nQ\nC\n3\n1.00\n0.50\n49.51187\n100\n6\n4\n1\n135\n2.0\nC\nB\n    Raisin Bran\n-0.45033439\nK\nC\n2\n1.33\n0.75\n39.25920\n120\n12\n3\n1\n210\n5.0\nA\nC\n    Raisin Nut Bran\n0.18717310\nG\nC\n3\n1.00\n0.50\n39.70340\n100\n8\n3\n2\n140\n2.5\nA\nC\n    Raisin Squares\n0.43847744\nK\nC\n3\n1.00\n0.50\n55.33314\n90\n6\n2\n0\n0\n2.0\nB\nC\n    Rice Chex\n0.59188250\nR\nC\n1\n1.00\n1.13\n41.99893\n110\n2\n1\n0\n240\n0.0\nC\nB\n    Rice Krispies\n0.97375824\nK\nC\n1\n1.00\n1.00\n40.56016\n110\n3\n2\n0\n290\n0.0\nNA\nC\n    Shredded Wheat\n-0.42771749\nN\nC\n1\n0.83\n1.00\n68.23588\n80\n0\n2\n0\n0\n3.0\nC\nB\n    Shredded Wheat 'n'Bran\n0.18744603\nN\nC\n1\n1.00\n0.67\n74.47295\n90\n0\n3\n0\n0\n4.0\nC\nC\n    Shredded Wheat spoon size\n-0.28871777\nN\nC\n1\n1.00\n0.67\n72.80179\n90\n0\n3\n0\n0\n3.0\nB\nC\n    Smacks\n0.51489170\nK\nC\n2\n1.00\n0.75\n31.23005\n110\n15\n2\n1\n70\n1.0\nNA\nC\n    Special K\n-0.98360035\nK\nC\n1\n1.00\n1.00\n53.13132\n110\n3\n6\n0\n230\n1.0\nB\nNA\n    Strawberry Fruit Wheats\n-0.55999867\nN\nC\n2\n1.00\n1.00\n59.36399\n90\n5\n2\n0\n15\n3.0\nNA\nNA\n    Total Corn Flakes\n-0.30296513\nG\nC\n3\n1.00\n1.00\n38.83975\n110\n3\n2\n1\n200\n0.0\nB\nC\n    Total Raisin Bran\n0.30374313\nG\nC\n3\n1.50\n1.00\n28.59278\n140\n14\n3\n1\n190\n4.0\nNA\nC\n    Total Whole Grain\n0.58866653\nG\nC\n3\n1.00\n1.00\n46.65884\n100\n3\n3\n1\n200\n3.0\nA\nC\n    Triples\n-0.19948074\nG\nC\n3\n1.00\n0.75\n39.10617\n110\n3\n2\n1\n250\n0.0\nA\nC\n    Trix\n0.12263145\nG\nC\n2\n1.00\n1.00\n27.75330\n110\n12\n1\n1\n140\n0.0\nC\nC\n    Wheat Chex\n-0.21903230\nR\nC\n1\n1.00\n0.67\n49.78744\n100\n3\n3\n1\n230\n3.0\nNA\nA\n    Wheaties\n0.45265594\nG\nC\n1\n1.00\n1.00\n51.59219\n100\n3\n3\n1\n200\n3.0\nNA\nA\n    Wheaties Honey Gold\n-0.48163389\nG\nC\n1\n1.00\n0.75\n36.18756\n110\n8\n2\n1\n200\n1.0\nC\nB\n  \n  \n  \n\n\n\n\n\n\nCode\ncereal2 %>%\n  rowwise() %>%\n  mutate(random_values = runif(1, min = -1, max = 1), .after = name) %>%\n  filter(if_any(everything(), ~ . < 0)) %>%\n  gt() %>% \n    tab_options(container.height = '500px') %>%\n    gt_theme_538() \n\n\n\n\n\n\n  \n  \n    \n      name\n      random_values\n      mfr\n      type\n      shelf\n      weight\n      cups\n      rating\n      calories\n      sugars\n      protein\n      fat\n      sodium\n      fiber\n      kids_rating\n      adult_rating\n    \n  \n  \n    100% Natural Bran\n-0.59099390\nQ\nC\n3\n1.00\n1.00\n33.98368\n120\n8\n3\n5\n15\n2.0\nA\nC\n    Apple Jacks\n-0.14953020\nK\nC\n2\n1.00\n1.00\n33.17409\n110\n14\n2\n0\n125\n1.0\nB\nNA\n    Basic 4\n-0.65192463\nG\nC\n3\n1.33\n0.75\n37.03856\n130\n8\n3\n2\n210\n2.0\nB\nB\n    Bran Flakes\n-0.53527415\nP\nC\n3\n1.00\n0.67\n53.31381\n90\n5\n3\n0\n210\n5.0\nB\nC\n    Cap'n'Crunch\n-0.95060099\nQ\nC\n2\n1.00\n0.75\n18.04285\n120\n12\n1\n2\n220\n0.0\nA\nC\n    Cheerios\n-0.14575755\nG\nC\n1\n1.00\n1.25\n50.76500\n110\n1\n6\n2\n290\n2.0\nA\nC\n    Corn Flakes\n-0.01404115\nK\nC\n1\n1.00\n1.00\n45.86332\n100\n2\n2\n0\n290\n1.0\nNA\nB\n    Corn Pops\n-0.53757006\nK\nC\n2\n1.00\n1.00\n35.78279\n110\n12\n1\n0\n90\n1.0\nA\nB\n    Count Chocula\n-0.81458583\nG\nC\n2\n1.00\n1.00\n22.39651\n110\n13\n1\n1\n180\n0.0\nB\nB\n    Cracklin' Oat Bran\n-0.19557691\nK\nC\n3\n1.00\n0.50\n40.44877\n110\n7\n3\n3\n140\n4.0\nB\nNA\n    Cream of Wheat (Quick)\n-0.12765056\nN\nH\n2\n1.00\n1.00\n64.53382\n100\n0\n3\n0\n80\n1.0\nB\nC\n    Crispix\n-0.42138345\nK\nC\n3\n1.00\n1.00\n46.89564\n110\n3\n2\n0\n220\n1.0\nC\nNA\n    Froot Loops\n-0.76033521\nK\nC\n2\n1.00\n1.00\n32.20758\n110\n13\n2\n1\n125\n1.0\nA\nC\n    Frosted Flakes\n-0.83097326\nK\nC\n1\n1.00\n0.75\n31.43597\n110\n11\n1\n0\n200\n1.0\nNA\nA\n    Frosted Mini-Wheats\n-0.63719565\nK\nC\n2\n1.00\n0.80\n58.34514\n100\n7\n3\n0\n0\n3.0\nB\nA\n    Fruity Pebbles\n-0.63628486\nP\nC\n2\n1.00\n0.75\n28.02576\n110\n12\n1\n1\n135\n0.0\nC\nB\n    Grape-Nuts\n-0.31589650\nP\nC\n3\n1.00\n0.25\n53.37101\n110\n3\n3\n0\n170\n3.0\nC\nC\n    Honey Graham Ohs\n-0.79255385\nQ\nC\n2\n1.00\n1.00\n21.87129\n120\n11\n1\n2\n220\n1.0\nC\nC\n    Honey Nut Cheerios\n-0.00408329\nG\nC\n1\n1.00\n0.75\n31.07222\n110\n10\n3\n1\n250\n1.5\nA\nC\n    Life\n-0.36261113\nQ\nC\n2\n1.00\n0.67\n45.32807\n100\n6\n4\n2\n150\n2.0\nNA\nC\n    Lucky Charms\n-0.26247427\nG\nC\n2\n1.00\n1.00\n26.73451\n110\n12\n2\n1\n180\n0.0\nB\nC\n    Muesli Raisins; Dates; & Almonds\n-0.19189932\nR\nC\n3\n1.00\n1.00\n37.13686\n150\n11\n4\n3\n95\n3.0\nA\nC\n    Muesli Raisins; Peaches; & Pecans\n-0.29492026\nR\nC\n3\n1.00\n1.00\n34.13976\n150\n11\n4\n3\n150\n3.0\nA\nC\n    Multi-Grain Cheerios\n-0.17173733\nG\nC\n1\n1.00\n1.00\n40.10596\n100\n6\n2\n1\n220\n2.0\nC\nC\n    Nut&Honey Crunch\n-0.62803106\nK\nC\n2\n1.00\n0.67\n29.92429\n120\n9\n2\n1\n190\n0.0\nA\nB\n    Nutri-Grain Almond-Raisin\n-0.61169738\nK\nC\n3\n1.33\n0.67\n40.69232\n140\n7\n3\n2\n220\n3.0\nB\nC\n    Post Nat. Raisin Bran\n-0.71665234\nP\nC\n3\n1.33\n0.67\n37.84059\n120\n14\n3\n1\n200\n6.0\nNA\nB\n    Quaker Oat Squares\n-0.70643022\nQ\nC\n3\n1.00\n0.50\n49.51187\n100\n6\n4\n1\n135\n2.0\nC\nB\n    Raisin Bran\n-0.86089752\nK\nC\n2\n1.33\n0.75\n39.25920\n120\n12\n3\n1\n210\n5.0\nA\nC\n    Raisin Squares\n-0.74979568\nK\nC\n3\n1.00\n0.50\n55.33314\n90\n6\n2\n0\n0\n2.0\nB\nC\n    Rice Krispies\n-0.56138145\nK\nC\n1\n1.00\n1.00\n40.56016\n110\n3\n2\n0\n290\n0.0\nNA\nC\n    Shredded Wheat\n-0.47634943\nN\nC\n1\n0.83\n1.00\n68.23588\n80\n0\n2\n0\n0\n3.0\nC\nB\n    Shredded Wheat 'n'Bran\n-0.58585422\nN\nC\n1\n1.00\n0.67\n74.47295\n90\n0\n3\n0\n0\n4.0\nC\nC\n    Shredded Wheat spoon size\n-0.64739482\nN\nC\n1\n1.00\n0.67\n72.80179\n90\n0\n3\n0\n0\n3.0\nB\nC\n    Smacks\n-0.78204560\nK\nC\n2\n1.00\n0.75\n31.23005\n110\n15\n2\n1\n70\n1.0\nNA\nC\n    Strawberry Fruit Wheats\n-0.90999803\nN\nC\n2\n1.00\n1.00\n59.36399\n90\n5\n2\n0\n15\n3.0\nNA\nNA\n    Total Corn Flakes\n-0.07846137\nG\nC\n3\n1.00\n1.00\n38.83975\n110\n3\n2\n1\n200\n0.0\nB\nC\n    Total Raisin Bran\n-0.18288058\nG\nC\n3\n1.50\n1.00\n28.59278\n140\n14\n3\n1\n190\n4.0\nNA\nC\n    Triples\n-0.16395244\nG\nC\n3\n1.00\n0.75\n39.10617\n110\n3\n2\n1\n250\n0.0\nA\nC\n    Wheaties\n-0.08900478\nG\nC\n1\n1.00\n1.00\n51.59219\n100\n3\n3\n1\n200\n3.0\nNA\nA\n    Wheaties Honey Gold\n-0.38797418\nG\nC\n1\n1.00\n0.75\n36.18756\n110\n8\n2\n1\n200\n1.0\nC\nB\n  \n  \n  \n\n\n\n\nHere we do the same thing except we only filter numeric columns for the formula call.\n\n\nCode\n#Find all rows where any numeric variable is less than zero\ncereal2 %>%\n  rowwise() %>%\n  mutate(random_values = runif(1, min = -1, max = 1), .after = name) %>%\n  filter(if_any(where(is.numeric), ~ .x < 0)) %>%\n  gt() %>% \n  tab_options(container.height = '500px') %>%\n  gt_theme_538() \n\n\n\n\n\n\n  \n  \n    \n      name\n      random_values\n      mfr\n      type\n      shelf\n      weight\n      cups\n      rating\n      calories\n      sugars\n      protein\n      fat\n      sodium\n      fiber\n      kids_rating\n      adult_rating\n    \n  \n  \n    100% Bran\n-0.29620140\nN\nC\n3\n1.00\n0.33\n68.40297\n70\n6\n4\n1\n130\n10.0\nNA\nC\n    All-Bran\n-0.02840512\nK\nC\n3\n1.00\n0.33\n59.42551\n70\n5\n4\n1\n260\n9.0\nB\nB\n    All-Bran with Extra Fiber\n-0.63079733\nK\nC\n3\n1.00\n0.50\n93.70491\n50\n0\n4\n0\n140\n14.0\nNA\nC\n    Almond Delight\n-0.46479905\nR\nC\n3\n1.00\n0.75\n34.38484\n110\n8\n2\n2\n200\n1.0\nA\nA\n    Bran Chex\n-0.64128219\nR\nC\n1\n1.00\n0.67\n49.12025\n90\n6\n2\n1\n200\n4.0\nNA\nA\n    Bran Flakes\n-0.11181362\nP\nC\n3\n1.00\n0.67\n53.31381\n90\n5\n3\n0\n210\n5.0\nB\nC\n    Cheerios\n-0.68393468\nG\nC\n1\n1.00\n1.25\n50.76500\n110\n1\n6\n2\n290\n2.0\nA\nC\n    Cocoa Puffs\n-0.71445799\nG\nC\n2\n1.00\n1.00\n22.73645\n110\n13\n1\n1\n180\n0.0\nA\nC\n    Corn Flakes\n-0.33299409\nK\nC\n1\n1.00\n1.00\n45.86332\n100\n2\n2\n0\n290\n1.0\nNA\nB\n    Count Chocula\n-0.83403533\nG\nC\n2\n1.00\n1.00\n22.39651\n110\n13\n1\n1\n180\n0.0\nB\nB\n    Crispix\n-0.14476432\nK\nC\n3\n1.00\n1.00\n46.89564\n110\n3\n2\n0\n220\n1.0\nC\nNA\n    Froot Loops\n-0.42169882\nK\nC\n2\n1.00\n1.00\n32.20758\n110\n13\n2\n1\n125\n1.0\nA\nC\n    Frosted Flakes\n-0.12083457\nK\nC\n1\n1.00\n0.75\n31.43597\n110\n11\n1\n0\n200\n1.0\nNA\nA\n    Frosted Mini-Wheats\n-0.87212455\nK\nC\n2\n1.00\n0.80\n58.34514\n100\n7\n3\n0\n0\n3.0\nB\nA\n    Fruitful Bran\n-0.09404340\nK\nC\n3\n1.33\n0.67\n41.01549\n120\n12\n3\n0\n240\n5.0\nNA\nC\n    Golden Crisp\n-0.69373157\nP\nC\n1\n1.00\n0.88\n35.25244\n100\n15\n2\n0\n45\n0.0\nNA\nC\n    Grape-Nuts\n-0.02355772\nP\nC\n3\n1.00\n0.25\n53.37101\n110\n3\n3\n0\n170\n3.0\nC\nC\n    Great Grains Pecan\n-0.55600242\nP\nC\n3\n1.00\n0.33\n45.81172\n120\n4\n3\n3\n75\n3.0\nC\nC\n    Honey Graham Ohs\n-0.01212090\nQ\nC\n2\n1.00\n1.00\n21.87129\n120\n11\n1\n2\n220\n1.0\nC\nC\n    Honey Nut Cheerios\n-0.27934787\nG\nC\n1\n1.00\n0.75\n31.07222\n110\n10\n3\n1\n250\n1.5\nA\nC\n    Just Right Fruit & Nut\n-0.24567649\nK\nC\n3\n1.30\n0.75\n36.47151\n140\n9\n3\n1\n170\n2.0\nC\nA\n    Kix\n-0.24108681\nG\nC\n2\n1.00\n1.50\n39.24111\n110\n3\n2\n1\n260\n0.0\nC\nB\n    Muesli Raisins; Peaches; & Pecans\n-0.52066977\nR\nC\n3\n1.00\n1.00\n34.13976\n150\n11\n4\n3\n150\n3.0\nA\nC\n    Nut&Honey Crunch\n-0.58527933\nK\nC\n2\n1.00\n0.67\n29.92429\n120\n9\n2\n1\n190\n0.0\nA\nB\n    Puffed Rice\n-0.15900634\nQ\nC\n3\n0.50\n1.00\n60.75611\n50\n0\n1\n0\n0\n0.0\nA\nNA\n    Raisin Nut Bran\n-0.43607801\nG\nC\n3\n1.00\n0.50\n39.70340\n100\n8\n3\n2\n140\n2.5\nA\nC\n    Raisin Squares\n-0.49022627\nK\nC\n3\n1.00\n0.50\n55.33314\n90\n6\n2\n0\n0\n2.0\nB\nC\n    Rice Krispies\n-0.36849460\nK\nC\n1\n1.00\n1.00\n40.56016\n110\n3\n2\n0\n290\n0.0\nNA\nC\n    Shredded Wheat spoon size\n-0.26527593\nN\nC\n1\n1.00\n0.67\n72.80179\n90\n0\n3\n0\n0\n3.0\nB\nC\n    Smacks\n-0.23583223\nK\nC\n2\n1.00\n0.75\n31.23005\n110\n15\n2\n1\n70\n1.0\nNA\nC\n    Strawberry Fruit Wheats\n-0.44869987\nN\nC\n2\n1.00\n1.00\n59.36399\n90\n5\n2\n0\n15\n3.0\nNA\nNA\n    Trix\n-0.74190320\nG\nC\n2\n1.00\n1.00\n27.75330\n110\n12\n1\n1\n140\n0.0\nC\nC\n    Wheat Chex\n-0.84209743\nR\nC\n1\n1.00\n0.67\n49.78744\n100\n3\n3\n1\n230\n3.0\nNA\nA\n  \n  \n  \n\n\n\n\n\n\n%in% operator\n\n\nCode\ncereal2 %>%\n  ungroup() %>%\n  filter(protein %in% c(3, 4)) %>%\n  gt() %>% \n    tab_options(container.height = '500px') %>%\n    gt_theme_538() \n\n\n\n\n\n\n  \n  \n    \n      name\n      mfr\n      type\n      shelf\n      weight\n      cups\n      rating\n      calories\n      sugars\n      protein\n      fat\n      sodium\n      fiber\n      kids_rating\n      adult_rating\n    \n  \n  \n    100% Bran\nN\nC\n3\n1.00\n0.33\n68.40297\n70\n6\n4\n1\n130\n10.0\nNA\nC\n    100% Natural Bran\nQ\nC\n3\n1.00\n1.00\n33.98368\n120\n8\n3\n5\n15\n2.0\nA\nC\n    All-Bran\nK\nC\n3\n1.00\n0.33\n59.42551\n70\n5\n4\n1\n260\n9.0\nB\nB\n    All-Bran with Extra Fiber\nK\nC\n3\n1.00\n0.50\n93.70491\n50\n0\n4\n0\n140\n14.0\nNA\nC\n    Basic 4\nG\nC\n3\n1.33\n0.75\n37.03856\n130\n8\n3\n2\n210\n2.0\nB\nB\n    Bran Flakes\nP\nC\n3\n1.00\n0.67\n53.31381\n90\n5\n3\n0\n210\n5.0\nB\nC\n    Clusters\nG\nC\n3\n1.00\n0.50\n40.40021\n110\n7\n3\n2\n140\n2.0\nA\nC\n    Cracklin' Oat Bran\nK\nC\n3\n1.00\n0.50\n40.44877\n110\n7\n3\n3\n140\n4.0\nB\nNA\n    Cream of Wheat (Quick)\nN\nH\n2\n1.00\n1.00\n64.53382\n100\n0\n3\n0\n80\n1.0\nB\nC\n    Frosted Mini-Wheats\nK\nC\n2\n1.00\n0.80\n58.34514\n100\n7\n3\n0\n0\n3.0\nB\nA\n    Fruit & Fibre Dates; Walnuts; and Oats\nP\nC\n3\n1.25\n0.67\n40.91705\n120\n10\n3\n2\n160\n5.0\nA\nC\n    Fruitful Bran\nK\nC\n3\n1.33\n0.67\n41.01549\n120\n12\n3\n0\n240\n5.0\nNA\nC\n    Grape Nuts Flakes\nP\nC\n3\n1.00\n0.88\n52.07690\n100\n5\n3\n1\n140\n3.0\nNA\nB\n    Grape-Nuts\nP\nC\n3\n1.00\n0.25\n53.37101\n110\n3\n3\n0\n170\n3.0\nC\nC\n    Great Grains Pecan\nP\nC\n3\n1.00\n0.33\n45.81172\n120\n4\n3\n3\n75\n3.0\nC\nC\n    Honey Nut Cheerios\nG\nC\n1\n1.00\n0.75\n31.07222\n110\n10\n3\n1\n250\n1.5\nA\nC\n    Just Right Fruit & Nut\nK\nC\n3\n1.30\n0.75\n36.47151\n140\n9\n3\n1\n170\n2.0\nC\nA\n    Life\nQ\nC\n2\n1.00\n0.67\n45.32807\n100\n6\n4\n2\n150\n2.0\nNA\nC\n    Muesli Raisins; Dates; & Almonds\nR\nC\n3\n1.00\n1.00\n37.13686\n150\n11\n4\n3\n95\n3.0\nA\nC\n    Muesli Raisins; Peaches; & Pecans\nR\nC\n3\n1.00\n1.00\n34.13976\n150\n11\n4\n3\n150\n3.0\nA\nC\n    Mueslix Crispy Blend\nK\nC\n3\n1.50\n0.67\n30.31335\n160\n13\n3\n2\n150\n3.0\nNA\nB\n    Nutri-Grain Almond-Raisin\nK\nC\n3\n1.33\n0.67\n40.69232\n140\n7\n3\n2\n220\n3.0\nB\nC\n    Nutri-grain Wheat\nK\nC\n3\n1.00\n1.00\n59.64284\n90\n2\n3\n0\n170\n3.0\nA\nC\n    Oatmeal Raisin Crisp\nG\nC\n3\n1.25\n0.50\n30.45084\n130\n10\n3\n2\n170\n1.5\nNA\nC\n    Post Nat. Raisin Bran\nP\nC\n3\n1.33\n0.67\n37.84059\n120\n14\n3\n1\n200\n6.0\nNA\nB\n    Product 19\nK\nC\n3\n1.00\n1.00\n41.50354\n100\n3\n3\n0\n320\n1.0\nNA\nC\n    Quaker Oat Squares\nQ\nC\n3\n1.00\n0.50\n49.51187\n100\n6\n4\n1\n135\n2.0\nC\nB\n    Raisin Bran\nK\nC\n2\n1.33\n0.75\n39.25920\n120\n12\n3\n1\n210\n5.0\nA\nC\n    Raisin Nut Bran\nG\nC\n3\n1.00\n0.50\n39.70340\n100\n8\n3\n2\n140\n2.5\nA\nC\n    Shredded Wheat 'n'Bran\nN\nC\n1\n1.00\n0.67\n74.47295\n90\n0\n3\n0\n0\n4.0\nC\nC\n    Shredded Wheat spoon size\nN\nC\n1\n1.00\n0.67\n72.80179\n90\n0\n3\n0\n0\n3.0\nB\nC\n    Total Raisin Bran\nG\nC\n3\n1.50\n1.00\n28.59278\n140\n14\n3\n1\n190\n4.0\nNA\nC\n    Total Whole Grain\nG\nC\n3\n1.00\n1.00\n46.65884\n100\n3\n3\n1\n200\n3.0\nA\nC\n    Wheat Chex\nR\nC\n1\n1.00\n0.67\n49.78744\n100\n3\n3\n1\n230\n3.0\nNA\nA\n    Wheaties\nG\nC\n1\n1.00\n1.00\n51.59219\n100\n3\n3\n1\n200\n3.0\nNA\nA\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\n\nCode\nsessionInfo()\n\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] gtExtras_0.2.3  gt_0.3.1        forcats_0.5.1   stringr_1.4.0  \n [5] dplyr_1.0.9     purrr_0.3.4     readr_2.1.2     tidyr_1.2.0    \n [9] tibble_3.1.7    ggplot2_3.3.6   tidyverse_1.3.1\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3           lubridate_1.8.0        assertthat_0.2.1      \n [4] digest_0.6.29          utf8_1.2.2             R6_2.5.1              \n [7] cellranger_1.1.0       backports_1.4.1        reprex_2.0.1          \n[10] evaluate_0.15          httr_1.4.3             pillar_1.7.0          \n[13] rlang_1.0.2            curl_4.3.2             readxl_1.3.1          \n[16] rstudioapi_0.13.0-9000 fontawesome_0.2.2.9000 checkmate_2.0.0       \n[19] rmarkdown_2.14         webshot_0.5.2          htmlwidgets_1.5.4     \n[22] bit_4.0.4              munsell_0.5.0          broom_0.8.0           \n[25] compiler_4.1.2         modelr_0.1.8           xfun_0.31.2           \n[28] pkgconfig_2.0.3        htmltools_0.5.2        tidyselect_1.1.2      \n[31] fansi_1.0.3            crayon_1.5.1           tzdb_0.2.0            \n[34] dbplyr_2.1.1           withr_2.5.0            grid_4.1.2            \n[37] jsonlite_1.8.0         gtable_0.3.0           lifecycle_1.0.1       \n[40] DBI_1.1.2              magrittr_2.0.3         scales_1.2.0          \n[43] vroom_1.5.7            cli_3.3.0              stringi_1.7.6         \n[46] renv_0.15.5            fs_1.5.2               xml2_1.3.3            \n[49] paletteer_1.4.0        ellipsis_0.3.2         generics_0.1.2        \n[52] vctrs_0.4.1            rematch2_2.1.2         tools_4.1.2           \n[55] bit64_4.0.5            glue_1.6.2             hms_1.1.1             \n[58] parallel_4.1.2         fastmap_1.1.0          yaml_2.3.5            \n[61] colorspace_2.0-3       rvest_1.0.2            knitr_1.39            \n[64] haven_2.4.3            sass_0.4.1"
  },
  {
    "objectID": "posts/tidyverse_tricks_ii/index.html",
    "href": "posts/tidyverse_tricks_ii/index.html",
    "title": "Tidyverse Tips II",
    "section": "",
    "text": "Selecting Columns\n\n\nCode\nmidwest %>%\n  select(county, starts_with(\"A\")) %>%\n  head()\n\n\n# A tibble: 6 × 2\n  county     area\n  <chr>     <dbl>\n1 ADAMS     0.052\n2 ALEXANDER 0.014\n3 BOND      0.022\n4 BOONE     0.017\n5 BROWN     0.018\n6 BUREAU    0.05 \n\n\n\n\nSelecting Columns (Regex)\nmatches() here results all columns that have the letter p followed by more than one letter that also contain the letter n.\n\n\nCode\nmidwest %>%\n  select(matches(\"p.+n\")) %>%\n  head()\n\n\n# A tibble: 6 × 7\n  popdensity popamerindian popasian percamerindan percasian poppovertyknown\n       <dbl>         <int>    <int>         <dbl>     <dbl>           <int>\n1      1271.            98      249         0.148    0.377            63628\n2       759             19       48         0.179    0.452            10529\n3       681.            35       16         0.233    0.107            14235\n4      1812.            46      150         0.149    0.487            30337\n5       324.            14        5         0.240    0.0857            4815\n6       714.            65      195         0.182    0.546            35107\n# … with 1 more variable: percpovertyknown <dbl>\n\n\n\n\nReformatting All Column Names\n\n\nCode\n#could also use tolower()\nmidwest %>%\n  select_all(toupper) \n\n\n# A tibble: 437 × 28\n     PID COUNTY  STATE  AREA POPTOTAL POPDENSITY POPWHITE POPBLACK POPAMERINDIAN\n   <int> <chr>   <chr> <dbl>    <int>      <dbl>    <int>    <int>         <int>\n 1   561 ADAMS   IL    0.052    66090      1271.    63917     1702            98\n 2   562 ALEXAN… IL    0.014    10626       759      7054     3496            19\n 3   563 BOND    IL    0.022    14991       681.    14477      429            35\n 4   564 BOONE   IL    0.017    30806      1812.    29344      127            46\n 5   565 BROWN   IL    0.018     5836       324.     5264      547            14\n 6   566 BUREAU  IL    0.05     35688       714.    35157       50            65\n 7   567 CALHOUN IL    0.017     5322       313.     5298        1             8\n 8   568 CARROLL IL    0.027    16805       622.    16519      111            30\n 9   569 CASS    IL    0.024    13437       560.    13384       16             8\n10   570 CHAMPA… IL    0.058   173025      2983.   146506    16559           331\n# … with 427 more rows, and 19 more variables: POPASIAN <int>, POPOTHER <int>,\n#   PERCWHITE <dbl>, PERCBLACK <dbl>, PERCAMERINDAN <dbl>, PERCASIAN <dbl>,\n#   PERCOTHER <dbl>, POPADULTS <int>, PERCHSD <dbl>, PERCOLLEGE <dbl>,\n#   PERCPROF <dbl>, POPPOVERTYKNOWN <int>, PERCPOVERTYKNOWN <dbl>,\n#   PERCBELOWPOVERTY <dbl>, PERCCHILDBELOWPOVERT <dbl>, PERCADULTPOVERTY <dbl>,\n#   PERCELDERLYPOVERTY <dbl>, INMETRO <int>, CATEGORY <chr>\n\n\n\n\nCleaning Column Names\n\n\nCode\n#making an unclean database:\nmidwest2 <- midwest %>%\n  select(poptotal, popother, county, percbelowpoverty, category)\n\ncolnames(midwest2) <- c(\"pop total\", \"pop other\", \n                        \"county\", \"perc below poverty\", \"category\")\n\nmidwest2 %>%\n  select_all(~str_replace(., \" \", \"_\"))\n\n\n# A tibble: 437 × 5\n   pop_total pop_other county    `perc_below poverty` category\n       <int>     <int> <chr>                    <dbl> <chr>   \n 1     66090       124 ADAMS                    13.2  AAR     \n 2     10626         9 ALEXANDER                32.2  LHR     \n 3     14991        34 BOND                     12.1  AAR     \n 4     30806      1139 BOONE                     7.21 ALU     \n 5      5836         6 BROWN                    13.5  AAR     \n 6     35688       221 BUREAU                   10.4  AAR     \n 7      5322         0 CALHOUN                  15.1  LAR     \n 8     16805        84 CARROLL                  11.7  AAR     \n 9     13437         6 CASS                     13.9  AAR     \n10    173025      1596 CHAMPAIGN                15.6  HAU     \n# … with 427 more rows\n\n\n\n\nFiltering\nYou can use near() with the tolerance argument to find values close to a certain number.\n\n\nCode\nmidwest %>%\n  select(county, poptotal) %>%\n  filter(near(\n    poptotal, 96000, tol = 15000\n    ))\n\n\n# A tibble: 26 × 2\n   county    poptotal\n   <chr>        <int>\n 1 KANKAKEE     96255\n 2 LA SALLE    106913\n 3 VERMILION    88257\n 4 CLARK        87777\n 5 HAMILTON    108936\n 6 JOHNSON      88109\n 7 LA PORTE    107066\n 8 MONROE      108978\n 9 VIGO        106107\n10 ALLEGAN      90509\n# … with 16 more rows\n\n\nIf you want to filter your rows on multiple values, you can use the %in% operator.\n\n\nCode\nmidwest %>%\n  select(poptotal, popother, county, percbelowpoverty, category) %>%\n  filter(county %in% c(\"IOWA\", \"JERSEY\", \"PARKE\", \"FULTON\"))\n\n\n# A tibble: 6 × 5\n  poptotal popother county percbelowpoverty category\n     <int>    <int> <chr>             <dbl> <chr>   \n1    38080      107 FULTON            15.5  AAR     \n2    20539       22 JERSEY             9.74 AAU     \n3    18840       51 FULTON            10.3  AAR     \n4    15410       12 PARKE             12.2  AAR     \n5    38498     1109 FULTON             6.23 ALU     \n6    20150       10 IOWA               9.99 HAR     \n\n\nTo return all rows where only one condition is met you can use xor() from base.\n\n\nCode\nmidwest %>%\n  filter(xor(popother < 80, percbelowpoverty > 15))\n\n\n# A tibble: 189 × 28\n     PID county  state  area poptotal popdensity popwhite popblack popamerindian\n   <int> <chr>   <chr> <dbl>    <int>      <dbl>    <int>    <int>         <int>\n 1   563 BOND    IL    0.022    14991       681.    14477      429            35\n 2   565 BROWN   IL    0.018     5836       324.     5264      547            14\n 3   569 CASS    IL    0.024    13437       560.    13384       16             8\n 4   570 CHAMPA… IL    0.058   173025      2983.   146506    16559           331\n 5   571 CHRIST… IL    0.042    34418       819.    34176       82            51\n 6   572 CLARK   IL    0.03     15921       531.    15842       10            26\n 7   575 COLES   IL    0.03     51644      1721.    50177      925            92\n 8   577 CRAWFO… IL    0.026    19464       749.    19300       63            34\n 9   578 CUMBER… IL    0.02     10670       534.    10627        5             6\n10   580 DE WITT IL    0.023    16516       718.    16387       25            37\n# … with 179 more rows, and 19 more variables: popasian <int>, popother <int>,\n#   percwhite <dbl>, percblack <dbl>, percamerindan <dbl>, percasian <dbl>,\n#   percother <dbl>, popadults <int>, perchsd <dbl>, percollege <dbl>,\n#   percprof <dbl>, poppovertyknown <int>, percpovertyknown <dbl>,\n#   percbelowpoverty <dbl>, percchildbelowpovert <dbl>, percadultpoverty <dbl>,\n#   percelderlypoverty <dbl>, inmetro <int>, category <chr>\n\n\nReturn all rows where popother is less than \\(80\\), but percbelowpoverty does not exceed \\(15%\\).\n\n\nCode\nmidwest %>%\n  filter(popother < 80, !percbelowpoverty > 15)\n\n\n# A tibble: 153 × 28\n     PID county  state  area poptotal popdensity popwhite popblack popamerindian\n   <int> <chr>   <chr> <dbl>    <int>      <dbl>    <int>    <int>         <int>\n 1   563 BOND    IL    0.022    14991       681.    14477      429            35\n 2   565 BROWN   IL    0.018     5836       324.     5264      547            14\n 3   569 CASS    IL    0.024    13437       560.    13384       16             8\n 4   571 CHRIST… IL    0.042    34418       819.    34176       82            51\n 5   572 CLARK   IL    0.03     15921       531.    15842       10            26\n 6   577 CRAWFO… IL    0.026    19464       749.    19300       63            34\n 7   578 CUMBER… IL    0.02     10670       534.    10627        5             6\n 8   580 DE WITT IL    0.023    16516       718.    16387       25            37\n 9   584 EDWARDS IL    0.014     7440       531.     7401        6             8\n10   585 EFFING… IL    0.028    31704      1132.    31523       12            45\n# … with 143 more rows, and 19 more variables: popasian <int>, popother <int>,\n#   percwhite <dbl>, percblack <dbl>, percamerindan <dbl>, percasian <dbl>,\n#   percother <dbl>, popadults <int>, perchsd <dbl>, percollege <dbl>,\n#   percprof <dbl>, poppovertyknown <int>, percpovertyknown <dbl>,\n#   percbelowpoverty <dbl>, percchildbelowpovert <dbl>, percadultpoverty <dbl>,\n#   percelderlypoverty <dbl>, inmetro <int>, category <chr>\n\n\n\n\nCounting\nThe count() function has a sort argument.\n\n\nCode\nmidwest %>%\n  count(category, sort = TRUE)\n\n\n# A tibble: 16 × 2\n   category     n\n   <chr>    <int>\n 1 AAR        193\n 2 AAU         77\n 3 LAR         30\n 4 LHR         28\n 5 HLU         26\n 6 ALU         20\n 7 HAU         20\n 8 AHR         16\n 9 ALR         11\n10 HAR          6\n11 LAU          3\n12 HLR          2\n13 LHU          2\n14 AHU          1\n15 HHR          1\n16 HHU          1\n\n\nThis was just a quick one. I think better examples are when you do a project, so I will display more complex situations later on.\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\n\nCode\nsessionInfo()\n\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] gtExtras_0.2.3  gt_0.3.1        forcats_0.5.1   stringr_1.4.0  \n [5] dplyr_1.0.9     purrr_0.3.4     readr_2.1.2     tidyr_1.2.0    \n [9] tibble_3.1.7    ggplot2_3.3.6   tidyverse_1.3.1\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8.3           lubridate_1.8.0        assertthat_0.2.1      \n [4] digest_0.6.29          utf8_1.2.2             R6_2.5.1              \n [7] cellranger_1.1.0       backports_1.4.1        reprex_2.0.1          \n[10] evaluate_0.15          httr_1.4.3             pillar_1.7.0          \n[13] rlang_1.0.2            readxl_1.3.1           rstudioapi_0.13.0-9000\n[16] fontawesome_0.2.2.9000 rmarkdown_2.14         webshot_0.5.2         \n[19] htmlwidgets_1.5.4      munsell_0.5.0          broom_0.8.0           \n[22] compiler_4.1.2         modelr_0.1.8           xfun_0.31.2           \n[25] pkgconfig_2.0.3        htmltools_0.5.2        tidyselect_1.1.2      \n[28] fansi_1.0.3            crayon_1.5.1           tzdb_0.2.0            \n[31] dbplyr_2.1.1           withr_2.5.0            grid_4.1.2            \n[34] jsonlite_1.8.0         gtable_0.3.0           lifecycle_1.0.1       \n[37] DBI_1.1.2              magrittr_2.0.3         scales_1.2.0          \n[40] cli_3.3.0              stringi_1.7.6          renv_0.15.5           \n[43] fs_1.5.2               xml2_1.3.3             paletteer_1.4.0       \n[46] ellipsis_0.3.2         generics_0.1.2         vctrs_0.4.1           \n[49] rematch2_2.1.2         tools_4.1.2            glue_1.6.2            \n[52] hms_1.1.1              fastmap_1.1.0          yaml_2.3.5            \n[55] colorspace_2.0-3       rvest_1.0.2            knitr_1.39            \n[58] haven_2.4.3"
  },
  {
    "objectID": "posts/combinations_vs_permutation/index.html",
    "href": "posts/combinations_vs_permutation/index.html",
    "title": "Untitled",
    "section": "",
    "text": "When the order doesn’t matter, it is a combination;\nWhen the order does matter it is a permutation."
  },
  {
    "objectID": "posts/combinations_vs_permutation/index.html#repetition",
    "href": "posts/combinations_vs_permutation/index.html#repetition",
    "title": "Untitled",
    "section": "Repetition",
    "text": "Repetition\n\nRepetition could be allowed in a situation where you wanted to know the character in a password (you can have multiple letters or numbers);\nNo repetition is when there is no replacement in the values such as lotto numbers or selecting a seat (you cannot sit where somebody is already sitting)."
  },
  {
    "objectID": "posts/combinations_vs_permutation/index.html#example",
    "href": "posts/combinations_vs_permutation/index.html#example",
    "title": "Untitled",
    "section": "Example",
    "text": "Example\nI had a gift card where two of the numbers were scratched off as I peeled back the scratchy thing. I did not want to contact the store if I could avoid that.\nFor example,\nA3E# - 3JH# - 45HL\nwhere the hashtags are the two values I can not see clearly. There are 36 possible values for each (26 letters and 10 numbers), so figuring this out via bruteforce will take a long time. But how many possible numbers could it be?\nWhere\n\\[\n\\text{ Characters to choose from} = n\n\\]\n\\[\nn = 36\n\\]\n\\[\n\\text{ Characters selected} = k\n\\]\n\\[\nk = 2\n\\]"
  },
  {
    "objectID": "posts/combinations_vs_permutation/index.html#combinations",
    "href": "posts/combinations_vs_permutation/index.html#combinations",
    "title": "Untitled",
    "section": "Combinations",
    "text": "Combinations\nPretending that order does not matter for our code or password, we could calculate the combinations by\n\\[\nC_{k}^{\\prime}(n) = \\binom{n + k  - 1}{k}\n\\]\n\\[\nC_{2}^{\\prime}(36) = C_{2}(36 +2 - 1)\n\\]\n\\[\nC_{2}(37) = \\binom{37}{2} = \\frac{37!}{2!(37 - 2)!} = \\frac{37 \\cdot 36}{2 \\cdot 1} = 666\n\\]\nIf order did not matter and we could not have repetition then we would have\n\\[\nC_{k}(n) = \\binom{n}{k} = \\frac{n!}{k!(n - k)!}\n\\]\n\\[\nC_{2}(36) = \\binom{36}{2} = \\frac{36!}{2!(36 - 2)!} = \\frac{36 \\cdot 35}{2 \\cdot 1} = 630\n\\]"
  },
  {
    "objectID": "posts/combinations_vs_permutation/index.html#permutation",
    "href": "posts/combinations_vs_permutation/index.html#permutation",
    "title": "Untitled",
    "section": "Permutation",
    "text": "Permutation\nThis problem will be a permutation problem since the order is important since A and 5 is not the same as 5 and A due to both values being independent from each other. For example,\nA3EA - 3JH5 - 45HL \\(!=\\) A3E5 - 3JHA - 45HL\nThis is a simple solution since repetition is allowed. For example, both values could be the same. As such\n\\[\n\\text{Permutation} = P\n\\]\n\\[\nP_{k}^{\\prime} = n^{k}\n\\]\n\\[\nP_{2}^{\\prime} = 36^{2} = 1296\n\\]\nHowever, lets just say in this situation repetition is not allowed. This would mean if the first character is A then the second character cannot be A. While this may seem like an obscure example in this example, imagine if the code was not entirely scratched off and you were certain that the two values were different from each other.\n\\[\nP_{k}(n) = \\frac{n!}{(n - k)}!\n\\]\n\\[\nP_{2}(36) = \\frac{36!}{(36-2)} = \\frac{36!}{34!} = 36 \\cdot 35 = 1260\n\\]\nThis should be fairly intuitive since 1296 - 1260 = 36."
  },
  {
    "objectID": "posts/combinations_vs_permutation/index.html#passwords",
    "href": "posts/combinations_vs_permutation/index.html#passwords",
    "title": "Untitled",
    "section": "Passwords",
    "text": "Passwords\nTo finish here are the amount of unique combinations for a password of 6 characters\n\n\n\n\n\n\n\n\nCharacters Used\nPossible Characters\nUnique Permutations\n\n\n\n\na - z\n26\n308,915,776\n\n\na - z & 0 - 9\n36\n2,176,782,336\n\n\na - z & A - Z\n52\n19,770,609,664\n\n\na - z & A - Z & 0 - 9\n62\n56,800,235,584\n\n\na - z & A - Z & 0 - 9 & !@#$%^&*()+-\n74\n164,206,490,176\n\n\n\nand 8 characters\n\n\n\n\n\n\n\n\nCharacters Used\nPossible Characters\nUnique Permutations\n\n\n\n\na - z\n26\n208,827,064,576\n\n\na - z & 0 - 9\n36\n2,821,109,907,456\n\n\na - z & A - Z\n52\n53,459,728,531,456\n\n\na - z & A - Z & 0 - 9\n62\n218,340,105,584,896\n\n\na - z & A - Z & 0 - 9 & !@#$%^&*()+-\n74\n899,194,740,203,776\n\n\n\nIf you want to calculate these values in R you can simply do it via\n\n\nCode\nlibrary(tidyverse)\nlibrary(gt)\n\ntibble(\n  # this will have each of the n characters repeat by 5\n  # before the next one starts\n  # total possible characters\n  'Character n' = rep(c(26, 36, 52, 62, 74), each = 5),\n  # whereas this approach will repeat the vector in that order\n  # password length\n  Length = rep(c(6, 7, 8, 10, 12), 5), \n  'Possible Values' = `Character n` ^ Length\n) %>%\n  gt() %>%\n  tab_options(container.height = '500px') %>%\n  fmt_number(columns = 3, decimals = 0)\n\n\n\n\n\n\n  \n  \n    \n      Character n\n      Length\n      Possible Values\n    \n  \n  \n    26\n6\n308,915,776\n    26\n7\n8,031,810,176\n    26\n8\n208,827,064,576\n    26\n10\n141,167,095,653,376\n    26\n12\n95,428,956,661,682,176\n    36\n6\n2,176,782,336\n    36\n7\n78,364,164,096\n    36\n8\n2,821,109,907,456\n    36\n10\n3,656,158,440,062,976\n    36\n12\n4,738,381,338,321,616,896\n    52\n6\n19,770,609,664\n    52\n7\n1,028,071,702,528\n    52\n8\n53,459,728,531,456\n    52\n10\n144,555,105,949,057,024\n    52\n12\n390,877,006,486,250,192,896\n    62\n6\n56,800,235,584\n    62\n7\n3,521,614,606,208\n    62\n8\n218,340,105,584,896\n    62\n10\n839,299,365,868,340,224\n    62\n12\n3,226,266,762,397,899,816,960\n    74\n6\n164,206,490,176\n    74\n7\n12,151,280,273,024\n    74\n8\n899,194,740,203,776\n    74\n10\n4,923,990,397,355,877,376\n    74\n12\n26,963,771,415,920,782,606,336"
  },
  {
    "objectID": "posts/python_in_rstudio/index.html",
    "href": "posts/python_in_rstudio/index.html",
    "title": "Untitled",
    "section": "",
    "text": "GT Table with Game Excitment\n\n\nCode\nplay_by_play_2021 <- read_rds(\"Documents/nfl/data/pbp/play_by_play_2021.rds\")\n\n\n\nGame Excitement Index\n\\[\n\\frac{2400}{\\text{Length of Game}} \\sum_{i = 2}^{\\text{n plays}} \\mid \\text{WinProb(i) - WinProb(i - 1)} \\mid\n\\]\n\nthe formula sums the absolute value of the win probability change from each play\n\nhttps://sports.sites.yale.edu/game-excitement-index-part-ii\n\n\nCode\n#Calculate average length of 2019 games for use in our function\navg_length = data.groupby(by = ['game_id'])['epa'].count().mean()\n\ndef calc_gei(game_id):\n  game = data[(data['game_id']==game_id)]\n  #Length of game\n  length = len(game)\n  #Adjusting for game length\n  normalize = avg_length / length\n  #Get win probability differences for each play\n  win_prob_change = game['home_wp'].diff().abs()\n  #Normalization\n  gei = normalize * win_prob_change.sum()\n  return gei\n\n\n\n\nCode\ncalc_gei <- function(data, this_game_id){\n  game = filter(data, game_id == this_game_id)\n  length = length(game)\n  normalise = avg_length / length\n  #Get win probability differences for each play\n  win_prob_change = select(pbp, home_wp) %>% lag()\n  #Normalization\n  gei = normalise * win_prob_change %>% sum()\n  return(gei)\n}\n\npbp %>%\n  calc_gei('2021_20_BUF_KC')\n\nplay_by_play_2021 %>%\n  filter(game_id == '2021_20_BUF_KC') %>%\n  summarise(\n    normalise = avg_length / length(game_id),\n    win_prob_change = diff(home_wp) %>% abs(),\n    gei = normalise * win_prob_change %>% sum(na.rm = TRUE)\n  )\n\ncalc_gei <- function(data, this_game_id){\n  game = filter(data, game_id == this_game_id)\n  length = length(game)\n  normalise = avg_length / length\n  #Get win probability differences for each play\n  win_prob_change = select(pbp, home_wp) %>% lag()\n  #Normalization\n  gei = normalise * win_prob_change %>% sum()\n  return(gei)\n}\n\n\n\n\nCode\n#Calculate average length of 2019 games for use in our function\n#avg_length = data.groupby(by = ['game_id'])['epa'].count().mean()\n\navg_length <- play_by_play_2021 %>%\n  group_by(game_id) %>%\n  count() %>%\n  ungroup() %>%\n  summarise(mean(n)) %>%\n  pull()\n\n\n\n\nCode\ngei_games <- play_by_play_2021 %>%\n  group_by(game_id) %>%\n  summarise(\n    normalise = avg_length / length(game_id),\n    win_prob_change = diff(home_wp) %>% abs(),\n    gei = normalise * win_prob_change %>% sum(na.rm = TRUE)\n  ) %>%\n  distinct(game_id, .keep_all = TRUE) %>%\n  select(game_id, gei)\n\n\n\n\nCode\ngei_joined <- play_by_play_2021 %>%\n  select(game_id, home_team, away_team, week) %>%\n  distinct(game_id, .keep_all = TRUE) %>%\n  left_join(gei_games)\n\nlead_changes <- play_by_play_2021 %>%\n  group_by(game_id) %>%\n  mutate(\n    team_leading = case_when(\n      total_home_score - total_away_score > 0 ~ 'home',\n      total_home_score - total_away_score < 0 ~ 'away',\n      TRUE ~ 'tie'),\n    lead_change = if_else(team_leading == lag(team_leading), 0, 1)\n  ) %>%\n  select(game_id, lead_change) %>%\n  summarise(\n    lead_changes = sum(lead_change, na.rm = TRUE)\n  )\n\ngei_joined <- gei_joined %>%\n  left_join(lead_changes)\n\n\n\n\nCode\n#fig.height=5\ngei_joined %>%\n  select(-game_id) %>%\n  mutate(\n    week = case_when(\n      week == 19 ~ 'Wild Card',\n      week == 20 ~ 'Divisional',\n      TRUE ~ as.character(week)\n    ),\n    gei = round(gei, 2)\n    ) %>% \n  slice_max(gei, n = 15) %>%\n  arrange(-gei) %>%\n  gt() %>%\n  tab_spanner(\n    label = \"TEAMS\",\n    columns = c(home_team, away_team)\n  ) %>%\n  data_color(\n    columns = c(gei),\n    colors = scales::col_numeric(\n      palette = c(\"#ffffcc\", \"#2c7fb8\"), #\"white\", #3fc1c9\n      domain = NULL\n    )\n  ) %>% \n  cols_label(\n    home_team = \"HOME TEAM\",\n    away_team = \"AWAY TEAM\",\n    week = \"WEEK\",\n    gei = \"GEI\"\n  ) %>% \n  tab_source_note(\n    source_note = md(\"Data: nflfastR<br>TABLE: @becausejustyn\")\n  ) %>% \n  gt_theme_538_v2(table.width = px(550))\n\n\nPhilly vs LAC was the only game with 10 lead changes that was not in the top 15 for GEI, likewise, NYG vs WAS was not in the top 15 despite 9 lead changes, however, that is excused\n\n\nCode\ngei_joined %>%\n  select(-game_id) %>%\n  mutate(\n    week_var = case_when(\n      week == 19 ~ 'Wild Card',\n      week == 20 ~ 'Divisional',\n      TRUE ~ as.numeric(week)\n    )\n  ) %>%\n  arrange(-gei) %>%\n  gt() %>%\n  tab_spanner(\n    label = \"TEAMS\",\n    columns = c(home_team, away_team)\n  ) %>%\n  data_color(\n    columns = c(gei),\n    colors = scales::col_numeric(\n      palette = c(\"white\", \"#3fc1c9\"),\n      domain = NULL\n    )\n  ) %>% \n  cols_label(\n    home_team = \"HOME TEAM\",\n    away_team = \"AWAY TEAM\",\n    week = \"WEEK\",\n    gei = \"GEI\"\n  ) %>% \n  tab_source_note(\n    source_note = md(\"Data: nflfastR<br>TABLE: @becausejustyn\")\n  ) %>% \n  gt_theme_538(table.width = px(550))\n\n\n\n\nCode\ngei_joined <- play_by_play_2021 %>%\n  select(game_id, home_team, away_team, week, home_score, away_score) %>%\n  distinct(game_id, .keep_all = TRUE) %>%\n  left_join(gei_games)\n\n\n\n\nCode\n# https://www.kaylinpavlik.com/how-to-watch-as-little-football-as-possible/\n# get number of lead changes\n\n\n\n\nCode\nplay_by_play_2021 %>%\n  filter(between(wp, .3, .7)) %>%\n  group_by(game_id) %>%\n  mutate(fun = length(game_id)) %>%\n  select(game_id, fun) %>%\n  distinct(game_id, .keep_all = TRUE) %>%\n  arrange(-fun)"
  }
]