{
  "hash": "fb5b60ced0d3c310f15e989f05c324f0",
  "result": {
    "markdown": "---\ntitle: \"Untitled\"\nexecute: \n  freeze: true\n  eval: false\nformat:\n  html:\n    highlight: true\n    code-fold: show\n    code-tools: true\n    highlight-style: github\n---\n\n## Autocorrelation in Elo ratings\n\nFiveThirtyEight uses the following formula for their NFL Elo ratings:\n\n\n$$\nR_{Team_{A}}^{k+1} = R_{Team_{A}}^{k} + K \\cdot M(mov) \\cdot A(x) \\cdot (S_{Team_{A} \\; Team_{B}} - \\sigma(x))\n$$\n\n\nwhere $mov$ is the game's margin of victory, $x = R_{Team_{A}}^{k} - R_{Team_{B}}^{k}$, and\n\n\n$$\n\\Large \n\\begin{align*}\nM(mov) &= \\ln (|mov|+1) \\\\\nA(x) &= \\frac{2.2}{2.2-0.001(-1)^{S_{Team_{A} \\; Team_{B}}} x} \\\\\n& = \\frac{1}{1-(-1)^{S_{Team_{A} \\; Team_{B}}}\\frac{x}{2200}} \\\\\nS_{Team_{A} \\; Team_{B}} &= \\begin{cases} 1 & Team_{A} \\text{ wins} \\\\ 0.5 & Team_{A} \\text{ ties} \\\\ 0 & Team_{A} \\text{ loses}\\end{cases} \\\\\n\\sigma(x) &= \\frac{1}{1+10^{-x/400}}\n\\end{align*}\n$$\n\n\nMy question is the specific justification for the $$A(x)$$ term: \n\n- **why this form**, and \n- **why those numbers.**  \n \nI'm looking for more than a layman's explanation, which 538 already offers and is the typical answer [elsewhere](https://stats.stackexchange.com/questions/168047/accounting-for-autocorrelation-in-margin-based-elo-ratings).  (Although [this post](https://andr3w321.com/a-note-on-autocorrelation/) goes a bit deeper.)\n\n\n## Quick intuition\n\nSo first you should buy off on the idea that given Team $$i$$'s current rating is $$R_i$$, we should *expect* its rating after the current game to still be $$R_i$$.  For example, we shouldn't ever *expect* Team $$i$$'s rating to increase, because if we did, [\"we should have rated them higher to begin with\"](https://fivethirtyeight.com/features/introducing-nfl-elo-ratings)!\n\nPut in statistical language, this is the statement that we want $$\\mathbb{E}[R_i^{k+1}\\vert\\text{all prev ratings}] = R_i^k$$, but more on that in a second.\n\n\n## The typical explanation\n\nSo $A(x)$, which again is\n\n\n$$\n\\begin{equation}\nA(x) = \\frac{1}{1-(-1)^{S_{ij}}\\frac{x}{2200}}\n\\label{eq:autocorr}\n\\end{equation}\n$$ \n\n\nis intended to correct for over- or under-inflation of ratings in the model.  We see the function is designed so that\n\n- If Team $$i$$ is the favorite ($$x>0$$), a loss is upweighted ($$A(x)>0$$) and a win is downweighted ($$A(x)<0$$).\n- If Team $$i$$ is the underdog ($$x<0$$), the opposite.\n\nSo this *seems* like it would correct for over-inflation of rating due to a heavy favorite, and vice-versa for a big underdog.\n\nHowever, we are left with the questions: why achieve it *in this way*? And why use the denominator $$d=2200$$?\n\n\n\n## Gettin stats-y\n\nWe may interpret Elo ratings as a time series where each new rating depends only on the previous rating, plus some \"noise.\"  More specifically, under this interpretation Elo assumes each team has some true rating, its mean, about which it is constantly fluctuating.  This is called an autoregressive model, in our case AR(1).  We are *also* assuming the team's ratings are **stationary**, meaning (loosely) the mean stays the same over time.\n\n(By the way, this is a different interpretation than the [connection to SGD](https://stmorse.github.io/journal/Elo.html) I wrote about before, but SGD and AR(1) are, in some sense, the same thing.)\n\n[Skipping some details](https://www.gilgamath.com/elo-distribution), (I think) this all amounts to needing the next observation in the time series to, in expectation, equal our current observation.  That is, $$\\mathbb{E}[R_i^{k+1}\\vert\\text{prev ratings}] = R_i^k$$.\n\nThe fact that we're fretting about a correction term at all arises because of the margin-of-victory $$M(z)$$ term we are including.  This isn't part of the \"classical\" Elo rating scheme, and it's messing everything up!  Without it, we have\n\n\n$$\n\\begin{align*}\n\\mathbb{E}[R_k^{k+1}] &= \\mathbb{E}[R_i^k] + \\mathbb{E}[k(S_{ij}-\\sigma(x))] \\\\\n&= R_i^k + k(\\mathbb{E}[S_{ij}] - \\sigma(x)) \\\\\n&= R_i^k \n\\end{align*}\n$$\n\n\nwhich is just fine.    \n\nNow, including a margin-of-victory term $$M(z)$$, we need\n\n\n$$\n\\mathbb{E}[M(z)\\cdot A(x) \\cdot (S_{ij} - \\sigma(x))] = 0\n$$\n\n\nwhich, computing expectation over all possible game outcomes as encoded in $$z$$, given $$R_i^k$$ and $$R_j^k$$, implies\n\n\n$$\n\\int_{-\\infty}^0 M(z) A(x) (-\\sigma(x)) \\text{Pr}(z) \\ dz + \\int_0^{\\infty} M(z) A(x) (1-\\sigma(x)) \\text{Pr}(z) \\ dz = 0\n$$\n\n\nover some distribution for the margin $$z$$.  Rearranging, we get\n\n\n$$\n\\begin{equation}\n\\frac{A(x; i \\text{ win})}{A(x; i \\text{ lose})} = \\frac{\\sigma(x)}{1-\\sigma(x)} \\cdot \\frac{\\mathbb{E}[M(z)|i \\text{ lose}]}{\\mathbb{E}[M(z)|i \\text{ wins}]}\n\\label{eq:goal}\n\\end{equation}\n$$\n\n\nand we want some $$A(x)$$ so this holds for any Elo delta $$x$$.\n\nWe should be able to interpolate some function for the expected $$M(z)$$'s, and then if we are satisfied with our functional form for $$A(x)$$, solve for the denominator $$d$$.\n\nLet's try it.\n\n\n## In search of d\n\nWe need to (1) work out the empirical conditional expectations for $$M(z)$$, then (2) approximate them with functions, and finally (3) solve for $$d$$ in terms of those functions.  \n\nWe can easily* pull boxscores, Elo ratings, and plot the mean $$M(z)$$'s.  (*really not that easy, but [see code](#code) at the bottom of this post.)\n\nHere's the expected $$M(z)$$ given various Elo rating deltas, based on the past 18 NFL seasons.\n\nNice!  So the empirical (conditional) expectations we're after are both reasonably approximated by linear functions:\n\n\n$$\n\\begin{align*}\n\\mathbb{E}[M(z)|i \\text{ win}] &\\approx \\frac{x}{1000} + 2.2 \\\\\n\\mathbb{E}[M(z)|i \\text{ lose}] &\\approx -\\frac{x}{1000} + 2.2\n\\end{align*}\n$$\n\n\n(Let the reader note: the slope and intercept here are what you might call \"eyeball\" estimates, although they are quite close to an OLS estimate.)\n\nReturning to Eq. \\eqref{eq:goal} and using our satisfying functional form for $$A(x)$$ from Eq. \\eqref{eq:autocorr}, we (almost) have\n\n\n$$\n\\begin{align*}\n\\frac{A(x; i \\text{ wins})}{A(x; i \\text{ lose})} &= \n\\frac{\\mathbb{E}[M(z)|i \\text{ wins}]}{\\mathbb{E}[M(z)|i \\text{ lose}]} \\\\\n\\frac{1-x/d}{1+x/d} &\\approx \\frac{-x/1000 + 2.2}{x/1000 + 2.2}\n\\end{align*}\n$$\n\n\nwhich gives $$d=2200$$.  Voila!\n\n**To do.** To get here, we had to ignore the $$\\sigma/(1-\\sigma)=10^{x/400}$$ term in Eq. \\eqref{eq:goal}.  There might be a way to rewrite the expected $$M(z)$$'s in a way they still fit the data and cancel this other term out, but if not, I'm not sure.\n\n<hr>\n\n## Code\n\nFortunately we can make heavy use of 538's public NFL data.\n\nFirst some imports to get us running in a Jupyter notebook:\n\n``` {.python .cell-code}\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n```\n\n\nThen we load the data, extract just the last few seasons, and run the desired stats:\n\n``` {.python .cell-code}\nbox = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/nfl-elo-game/master/data/nfl_games.csv')\n\n# convert date to datetime\nbox['date'] = pd.to_datetime(box['date'])\n\n# grab since ____ season\nbox = (box[box['date'] > '2003-08-01']\n       .reset_index()\n       .drop(['index'], axis=1)\n       .copy())\n\nn = box.shape[0]\nelodiffs = np.zeros(n)\npdiffs   = np.zeros(n)\nfor i, row in box.iterrows():\n    elodiffs[i] = row['elo1'] - row['elo2'] + (0 if row['neutral']==1 else 65)\n    pdiffs[i]   = row['score1'] - row['score2']\n```\n\n\nWe could do something like a `pdiffs` vs. `elodiffs` plot with Seaborn at this point, perhaps a `jointplot` like this ...\n\n``` {.python .cell-code}\nimport seaborn as sns\nsns.jointplot(elodiffs / 25, pdiffs, ylim=(-50,50), kind='hex')\n```\n\n\nwhich is quite exciting.  But what we're really interested in is the mean MOV function distributions, which is the much less pretty:\n\n``` {.python .cell-code}\nxs = np.linspace(-300,400,10)\n\ndef g1(x):\n    return x/1000) + 2.2\ndef g2(x):\n    return -x/1000 + 2.2\n\nfig, ax = plt.subplots(1,1, figsize=(8,6))\n\nax.plot(ed, mw, 'ko-', label='Win')\nax.plot(xs, g1(xs), 'k--')\n\nax.plot(ed, ml, 'co-', label='Loss')\nax.plot(xs, g2(xs), 'c--')\n\nax.set_xlabel(r'$R_i - R_j$', fontsize=16)\nax.set_ylabel(r'Mean $M(z)$', fontsize=16)\nax.legend()\n\nplt.show()\n```\n\n\n",
    "supporting": [
      "elo_autocorr_files"
    ],
    "filters": [],
    "includes": {}
  }
}